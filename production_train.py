"""
–ü—Ä–æ–¥–∞–∫—à–Ω —Å–∫—Ä–∏–ø—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –≤—ã—Å–æ–∫–æ—Ç–æ—á–Ω–æ–π –º–æ–¥–µ–ª–∏ –∞–Ω–∞–ª–∏–∑–∞ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–π
"""
import os
import sys
import pandas as pd
import numpy as np
import joblib
from pathlib import Path
from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import json

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ –º–æ–¥—É–ª—è–º
sys.path.append('src')

from config import (
    MODEL_CONFIG, MODEL_PATHS, TOKENIZER_CONFIG, VALIDATION_CONFIG,
    PROCESSED_DATA_DIR, DATA_SOURCES
)
from data_loader import EnhancedDataLoader
from model import SentimentModelEnsemble, AdvancedSentimentTrainer


class ProductionTrainer:
    """–ü—Ä–æ–¥–∞–∫—à–Ω —Ç—Ä–µ–Ω–µ—Ä –¥–ª—è –≤—ã—Å–æ–∫–æ—Ç–æ—á–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–π"""
    
    def __init__(self):
        self.data_loader = EnhancedDataLoader()
        self.advanced_trainer = AdvancedSentimentTrainer()
        self.tokenizer = None
        self.label_encoder = None
        self.final_results = {}
        
    def setup_environment(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ–∫—Ä—É–∂–µ–Ω–∏—è –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"""
        print("üîß –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ–∫—Ä—É–∂–µ–Ω–∏—è...")
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π
        for path in MODEL_PATHS.values():
            if isinstance(path, Path):
                path.parent.mkdir(parents=True, exist_ok=True)
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ TensorFlow
        import tensorflow as tf
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ GPU (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–æ)
        gpus = tf.config.experimental.list_physical_devices('GPU')
        if gpus:
            try:
                for gpu in gpus:
                    tf.config.experimental.set_memory_growth(gpu, True)
                print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(gpus)} GPU —É—Å—Ç—Ä–æ–π—Å—Ç–≤")
            except RuntimeError as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ GPU: {e}")
        else:
            print("‚ÑπÔ∏è –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è CPU")
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏
        tf.random.set_seed(42)
        np.random.seed(42)
        
        print("‚úÖ –û–∫—Ä—É–∂–µ–Ω–∏–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–æ")
    
    def load_and_prepare_data(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö"""
        print("\n" + "="*60)
        print("üìä –ó–ê–ì–†–£–ó–ö–ê –ò –ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–ù–ù–´–•")
        print("="*60)
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏—è –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        train_path = PROCESSED_DATA_DIR / "train_data.csv"
        test_path = PROCESSED_DATA_DIR / "test_data.csv"
        
        if train_path.exists() and test_path.exists():
            print("üìÇ –ó–∞–≥—Ä—É–∑–∫–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")
            train_df = pd.read_csv(train_path)
            test_df = pd.read_csv(test_path)
            
            # –ó–∞–≥—Ä—É–∑–∫–∞ label encoder
            label_encoder_path = PROCESSED_DATA_DIR / "label_encoder.pkl"
            if label_encoder_path.exists():
                self.label_encoder = joblib.load(label_encoder_path)
            else:
                raise FileNotFoundError("Label encoder –Ω–µ –Ω–∞–π–¥–µ–Ω")
        else:
            print("üîÑ –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")
            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–≥–æ –∑–∞–≥—Ä—É–∑—á–∏–∫–∞
            train_df, test_df = self.data_loader.prepare_final_dataset()
            self.label_encoder = self.data_loader.label_encoder
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
            self.data_loader.save_processed_data(train_df, test_df)
        
        print(f"\nüìà –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–∞–Ω–Ω—ã—Ö:")
        print(f"   –û–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞: {len(train_df):,} –ø—Ä–∏–º–µ—Ä–æ–≤")
        print(f"   –¢–µ—Å—Ç–æ–≤–∞—è –≤—ã–±–æ—Ä–∫–∞: {len(test_df):,} –ø—Ä–∏–º–µ—Ä–æ–≤")
        print(f"   –í—Å–µ–≥–æ: {len(train_df) + len(test_df):,} –ø—Ä–∏–º–µ—Ä–æ–≤")
        
        # –ê–Ω–∞–ª–∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∫–ª–∞—Å—Å–æ–≤
        print(f"\nüìä –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤ –≤ –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–µ:")
        label_counts = train_df['label'].value_counts()
        for label, count in label_counts.items():
            percentage = count / len(train_df) * 100
            print(f"   {label}: {count:,} ({percentage:.1f}%)")
        
        return train_df, test_df
    
    def prepare_tokenizer_and_sequences(self, train_df, test_df):
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –∏ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π"""
        print("\n" + "="*60)
        print("üî§ –¢–û–ö–ï–ù–ò–ó–ê–¶–ò–Ø –ò –ü–û–î–ì–û–¢–û–í–ö–ê –ü–û–°–õ–ï–î–û–í–ê–¢–ï–õ–¨–ù–û–°–¢–ï–ô")
        print("="*60)
        
        # –°–æ–∑–¥–∞–Ω–∏–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞
        self.tokenizer = Tokenizer(**TOKENIZER_CONFIG)
        
        # –û–±—É—á–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞
        print("üîÑ –û–±—É—á–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞...")
        all_texts = list(train_df['processed_text']) + list(test_df['processed_text'])
        self.tokenizer.fit_on_texts(all_texts)
        
        vocab_size = len(self.tokenizer.word_index) + 1
        print(f"   –†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è: {vocab_size:,} —Å–ª–æ–≤")
        print(f"   –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π —Ä–∞–∑–º–µ—Ä: {min(vocab_size, MODEL_CONFIG['max_features']):,}")
        
        # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        print("üîÑ –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–æ–≤ –≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏...")
        X_train_seq = self.tokenizer.texts_to_sequences(train_df['processed_text'])
        X_test_seq = self.tokenizer.texts_to_sequences(test_df['processed_text'])
        
        # –ü–∞–¥–¥–∏–Ω–≥ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π
        X_train = pad_sequences(
            X_train_seq,
            maxlen=MODEL_CONFIG["max_sequence_length"],
            padding='post',
            truncating='post'
        )
        X_test = pad_sequences(
            X_test_seq,
            maxlen=MODEL_CONFIG["max_sequence_length"],
            padding='post',
            truncating='post'
        )
        
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –º–µ—Ç–æ–∫
        y_train = train_df['label_encoded'].values
        y_test = test_df['label_encoded'].values
        
        # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏ –Ω–∞ train/validation
        X_train, X_val, y_train, y_val = train_test_split(
            X_train, y_train,
            test_size=VALIDATION_CONFIG["validation_size"],
            random_state=VALIDATION_CONFIG["random_state"],
            stratify=y_train
        )
        
        print(f"\nüìä –§–∏–Ω–∞–ª—å–Ω—ã–µ —Ä–∞–∑–º–µ—Ä—ã –¥–∞–Ω–Ω—ã—Ö:")
        print(f"   –û–±—É—á–µ–Ω–∏–µ: {X_train.shape}")
        print(f"   –í–∞–ª–∏–¥–∞—Ü–∏—è: {X_val.shape}")
        print(f"   –¢–µ—Å—Ç: {X_test.shape}")
        
        # –ê–Ω–∞–ª–∏–∑ –¥–ª–∏–Ω –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π
        train_lengths = [len([w for w in seq if w != 0]) for seq in X_train]
        print(f"\nüìè –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–ª–∏–Ω –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π:")
        print(f"   –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞: {np.mean(train_lengths):.1f}")
        print(f"   –ú–µ–¥–∏–∞–Ω–Ω–∞—è –¥–ª–∏–Ω–∞: {np.median(train_lengths):.1f}")
        print(f"   95-–π –ø–µ—Ä—Ü–µ–Ω—Ç–∏–ª—å: {np.percentile(train_lengths, 95):.1f}")
        
        return X_train, X_val, X_test, y_train, y_val, y_test
    
    def train_production_models(self, X_train, X_val, X_test, y_train, y_val, y_test):
        """–û–±—É—á–µ–Ω–∏–µ –ø—Ä–æ–¥–∞–∫—à–Ω –º–æ–¥–µ–ª–µ–π"""
        print("\n" + "="*60)
        print("üöÄ –û–ë–£–ß–ï–ù–ò–ï –ü–†–û–î–ê–ö–®–ù –ú–û–î–ï–õ–ï–ô")
        print("="*60)
        
        # –°–æ–∑–¥–∞–Ω–∏–µ —Ç—Ä–µ–Ω–µ—Ä–∞
        from model import SentimentModelEnsemble
        vocab_size = len(self.tokenizer.word_index) + 1
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –∞–Ω—Å–∞–º–±–ª—è –º–æ–¥–µ–ª–µ–π
        ensemble = SentimentModelEnsemble(vocab_size=vocab_size)
        ensemble.build_all_models()
        
        # –û–±—É—á–µ–Ω–∏–µ –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π
        print("\nüéØ –û–±—É—á–µ–Ω–∏–µ –∞–Ω—Å–∞–º–±–ª—è –º–æ–¥–µ–ª–µ–π...")
        histories = ensemble.train_ensemble(X_train, y_train, X_val, y_val)
        
        # –û—Ü–µ–Ω–∫–∞ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        print("\nüìä –û—Ü–µ–Ω–∫–∞ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")
        test_results = ensemble.evaluate_ensemble(X_test, y_test)
        
        # –í–∞–ª–∏–¥–∞—Ü–∏—è –∫–∞—á–µ—Å—Ç–≤–∞
        validation_results = {}
        for model_name, model in ensemble.models.items():
            predictions = model.predict(X_test, verbose=0)
            predicted_classes = np.argmax(predictions, axis=1)
            
            from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score
            
            accuracy = accuracy_score(y_test, predicted_classes)
            f1 = f1_score(y_test, predicted_classes, average='weighted')
            precision = precision_score(y_test, predicted_classes, average='weighted')
            recall = recall_score(y_test, predicted_classes, average='weighted')
            
            validation_results[model_name] = {
                'accuracy': accuracy,
                'f1_score': f1,
                'precision': precision,
                'recall': recall,
                'quality_passed': accuracy >= VALIDATION_CONFIG["target_accuracy"]
            }
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π
        ensemble.save_all_models()
        
        self.final_results = {
            'training_history': histories,
            'test_results': test_results,
            'validation_results': validation_results,
            'training_duration': str(datetime.now() - datetime.now()),
            'best_model': self._find_best_model(validation_results)
        }
        
        return self.final_results
    
    def _find_best_model(self, validation_results):
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏"""
        if not validation_results:
            return None
        
        best_model = max(
            validation_results.items(),
            key=lambda x: x[1]['accuracy'] if isinstance(x[1], dict) else 0
        )
        
        return {
            'name': best_model[0],
            'accuracy': best_model[1]['accuracy'],
            'f1_score': best_model[1]['f1_score']
        }
    
    def save_production_artifacts(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤ –¥–ª—è –ø—Ä–æ–¥–∞–∫—à–Ω–∞"""
        print("\n" + "="*60)
        print("üíæ –°–û–•–†–ê–ù–ï–ù–ò–ï –ü–†–û–î–ê–ö–®–ù –ê–†–¢–ï–§–ê–ö–¢–û–í")
        print("="*60)
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞
        tokenizer_path = MODEL_PATHS["tokenizer"]
        joblib.dump(self.tokenizer, tokenizer_path)
        print(f"‚úÖ –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {tokenizer_path}")
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ label encoder
        label_encoder_path = MODEL_PATHS["label_encoder"]
        joblib.dump(self.label_encoder, label_encoder_path)
        print(f"‚úÖ Label encoder —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {label_encoder_path}")
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        config_path = MODEL_PATHS["main_model"].parent / "model_config.json"
        config_data = {
            'model_config': MODEL_CONFIG,
            'tokenizer_config': TOKENIZER_CONFIG,
            'vocab_size': len(self.tokenizer.word_index) + 1,
            'class_names': MODEL_CONFIG["class_names"],
            'training_timestamp': datetime.now().isoformat(),
            'final_results': self.final_results
        }
        
        with open(config_path, 'w', encoding='utf-8') as f:
            # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è numpy —Ç–∏–ø–æ–≤
            def convert_numpy(obj):
                if isinstance(obj, np.integer):
                    return int(obj)
                elif isinstance(obj, np.floating):
                    return float(obj)
                elif isinstance(obj, np.ndarray):
                    return obj.tolist()
                return obj
            
            json.dump(config_data, f, ensure_ascii=False, indent=2, default=convert_numpy)
        print(f"‚úÖ –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {config_path}")
        
        print("\nüìÑ –ê—Ä—Ç–µ—Ñ–∞–∫—Ç—ã –≥–æ—Ç–æ–≤—ã –¥–ª—è –ø—Ä–æ–¥–∞–∫—à–Ω–∞!")
    
    def create_visualizations(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
        print("\n" + "="*60)
        print("üìä –°–û–ó–î–ê–ù–ò–ï –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–ô")
        print("="*60)
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –¥–ª—è –≥—Ä–∞—Ñ–∏–∫–æ–≤
        plots_dir = MODEL_PATHS["main_model"].parent / "plots"
        plots_dir.mkdir(exist_ok=True)
        
        # –ì—Ä–∞—Ñ–∏–∫ –∏—Å—Ç–æ—Ä–∏–∏ –æ–±—É—á–µ–Ω–∏—è
        if self.final_results.get('training_history'):
            self._plot_training_history(plots_dir)
        
        # –ì—Ä–∞—Ñ–∏–∫ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π
        if self.final_results.get('validation_results'):
            self._plot_model_comparison(plots_dir)
        
        # –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫ –¥–ª—è –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏
        self._plot_confusion_matrix(plots_dir)
        
        print(f"‚úÖ –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {plots_dir}")
    
    def _plot_training_history(self, plots_dir):
        """–ì—Ä–∞—Ñ–∏–∫ –∏—Å—Ç–æ—Ä–∏–∏ –æ–±—É—á–µ–Ω–∏—è"""
        history = self.final_results['training_history']
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        fig.suptitle('–ò—Å—Ç–æ—Ä–∏—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π', fontsize=16)
        
        models = ['lstm', 'cnn_lstm', 'transformer']
        colors = ['blue', 'green', 'red']
        
        for i, (model_name, color) in enumerate(zip(models, colors)):
            if model_name in history:
                hist = history[model_name].history
                
                # –¢–æ—á–Ω–æ—Å—Ç—å
                axes[0, 0].plot(hist['accuracy'], label=f'{model_name} train', color=color)
                axes[0, 0].plot(hist['val_accuracy'], label=f'{model_name} val', color=color, linestyle='--')
                
                # –ü–æ—Ç–µ—Ä–∏
                axes[0, 1].plot(hist['loss'], label=f'{model_name} train', color=color)
                axes[0, 1].plot(hist['val_loss'], label=f'{model_name} val', color=color, linestyle='--')
        
        axes[0, 0].set_title('–¢–æ—á–Ω–æ—Å—Ç—å')
        axes[0, 0].set_xlabel('–≠–ø–æ—Ö–∞')
        axes[0, 0].set_ylabel('–¢–æ—á–Ω–æ—Å—Ç—å')
        axes[0, 0].legend()
        axes[0, 0].grid(True)
        
        axes[0, 1].set_title('–ü–æ—Ç–µ—Ä–∏')
        axes[0, 1].set_xlabel('–≠–ø–æ—Ö–∞')
        axes[0, 1].set_ylabel('–ü–æ—Ç–µ—Ä–∏')
        axes[0, 1].legend()
        axes[0, 1].grid(True)
        
        plt.tight_layout()
        plt.savefig(plots_dir / 'training_history.png', dpi=300, bbox_inches='tight')
        plt.close()
    
    def _plot_model_comparison(self, plots_dir):
        """–ì—Ä–∞—Ñ–∏–∫ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π"""
        validation_results = self.final_results['validation_results']
        
        models = list(validation_results.keys())
        metrics = ['accuracy', 'f1_score', 'precision', 'recall']
        
        fig, ax = plt.subplots(figsize=(12, 8))
        
        x = np.arange(len(models))
        width = 0.2
        
        for i, metric in enumerate(metrics):
            values = [validation_results[model][metric] for model in models]
            ax.bar(x + i * width, values, width, label=metric)
        
        ax.set_xlabel('–ú–æ–¥–µ–ª–∏')
        ax.set_ylabel('–ó–Ω–∞—á–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫–∏')
        ax.set_title('–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ –º–æ–¥–µ–ª–µ–π')
        ax.set_xticks(x + width * 1.5)
        ax.set_xticklabels(models)
        ax.legend()
        ax.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(plots_dir / 'model_comparison.png', dpi=300, bbox_inches='tight')
        plt.close()
    
    def _plot_confusion_matrix(self, plots_dir):
        """–ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫ –¥–ª—è –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏"""
        # –ó–¥–µ—Å—å –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –ª–æ–≥–∏–∫–∞ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –º–∞—Ç—Ä–∏—Ü—ã –æ—à–∏–±–æ–∫
        # –ù–∞ –¥–∞–Ω–Ω—ã–π –º–æ–º–µ–Ω—Ç —Å–æ–∑–¥–∞–µ–º –∑–∞–≥–ª—É—à–∫—É
        plt.figure(figsize=(8, 6))
        plt.text(0.5, 0.5, 'Confusion Matrix\n(Implementation needed)', 
                ha='center', va='center', fontsize=16)
        plt.title('–ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏')
        plt.savefig(plots_dir / 'confusion_matrix.png', dpi=300, bbox_inches='tight')
        plt.close()
    
    def run_production_training(self):
        """–ü–æ–ª–Ω—ã–π —Ü–∏–∫–ª –ø—Ä–æ–¥–∞–∫—à–Ω –æ–±—É—á–µ–Ω–∏—è"""
        print("üéØ –ó–ê–ü–£–°–ö –ü–†–û–î–ê–ö–®–ù –û–ë–£–ß–ï–ù–ò–Ø –ú–û–î–ï–õ–ò –ê–ù–ê–õ–ò–ó–ê –ù–ê–°–¢–†–û–ï–ù–ò–ô")
        print("="*80)
        
        start_time = datetime.now()
        
        try:
            # 1. –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ–∫—Ä—É–∂–µ–Ω–∏—è
            self.setup_environment()
            
            # 2. –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
            train_df, test_df = self.load_and_prepare_data()
            
            # 3. –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
            X_train, X_val, X_test, y_train, y_val, y_test = self.prepare_tokenizer_and_sequences(
                train_df, test_df
            )
            
            # 4. –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π
            results = self.train_production_models(
                X_train, X_val, X_test, y_train, y_val, y_test
            )
            
            # 5. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤
            self.save_production_artifacts()
            
            # 6. –°–æ–∑–¥–∞–Ω–∏–µ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–π
            self.create_visualizations()
            
            # 7. –§–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç
            self.print_final_report()
            
            end_time = datetime.now()
            total_duration = end_time - start_time
            
            print(f"\nüéâ –û–ë–£–ß–ï–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û –£–°–ü–ï–®–ù–û!")
            print(f"‚è±Ô∏è –û–±—â–µ–µ –≤—Ä–µ–º—è: {total_duration}")
            print(f"üèÜ –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å: {self.final_results['best_model']['name']}")
            print(f"üìä –õ—É—á—à–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å: {self.final_results['best_model']['accuracy']:.4f}")
            
            return True
            
        except Exception as e:
            print(f"\n‚ùå –û–®–ò–ë–ö–ê –ü–†–ò –û–ë–£–ß–ï–ù–ò–ò: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def print_final_report(self):
        """–ü–µ—á–∞—Ç—å —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞"""
        print("\n" + "="*60)
        print("üìã –§–ò–ù–ê–õ–¨–ù–´–ô –û–¢–ß–ï–¢")
        print("="*60)
        
        if self.final_results.get('validation_results'):
            print("\nüèÜ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –º–æ–¥–µ–ª–µ–π:")
            for model_name, metrics in self.final_results['validation_results'].items():
                if isinstance(metrics, dict):
                    quality_status = "‚úÖ –ü–†–û–ô–î–ï–ù–û" if metrics.get('quality_passed', False) else "‚ùå –ù–ï –ü–†–û–ô–î–ï–ù–û"
                    print(f"\n{model_name.upper()}:")
                    print(f"  üìä –¢–æ—á–Ω–æ—Å—Ç—å: {metrics['accuracy']:.4f}")
                    print(f"  üìä F1-score: {metrics['f1_score']:.4f}")
                    print(f"  üìä Precision: {metrics['precision']:.4f}")
                    print(f"  üìä Recall: {metrics['recall']:.4f}")
                    print(f"  üéØ –ö–∞—á–µ—Å—Ç–≤–æ: {quality_status}")
        
        if self.final_results.get('best_model'):
            best = self.final_results['best_model']
            print(f"\nü•á –õ–£–ß–®–ê–Ø –ú–û–î–ï–õ–¨: {best['name'].upper()}")
            print(f"   üìä –¢–æ—á–Ω–æ—Å—Ç—å: {best['accuracy']:.4f}")
            print(f"   üìä F1-score: {best['f1_score']:.4f}")
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        print(f"\nüí° –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò:")
        if self.final_results.get('best_model'):
            best_acc = self.final_results['best_model']['accuracy']
            if best_acc >= 0.92:
                print("   üéØ –û—Ç–ª–∏—á–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ! –ú–æ–¥–µ–ª—å –≥–æ—Ç–æ–≤–∞ –¥–ª—è –ø—Ä–æ–¥–∞–∫—à–Ω–∞.")
            elif best_acc >= 0.88:
                print("   ‚úÖ –•–æ—Ä–æ—à–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ. –ú–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≤ –ø—Ä–æ–¥–∞–∫—à–Ω–µ.")
            elif best_acc >= 0.85:
                print("   ‚ö†Ô∏è –ü—Ä–∏–µ–º–ª–µ–º–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ. –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞.")
            else:
                print("   ‚ùå –ù–∏–∑–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ. –¢—Ä–µ–±—É–µ—Ç—Å—è —É–ª—É—á—à–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∏–ª–∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã.")
        
        print(f"\nüìÅ –§–∞–π–ª—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {MODEL_PATHS['main_model'].parent}")
        print(f"ü§ñ –î–ª—è –∑–∞–ø—É—Å–∫–∞ –±–æ—Ç–∞: python src/bot.py")


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    trainer = ProductionTrainer()
    success = trainer.run_production_training()
    
    if success:
        print("\nüöÄ –ì–æ—Ç–æ–≤–æ! –¢–µ–ø–µ—Ä—å –º–æ–∂–µ—Ç–µ –∑–∞–ø—É—Å—Ç–∏—Ç—å –±–æ—Ç–∞.")
    else:
        print("\n‚ùå –û–±—É—á–µ–Ω–∏–µ –Ω–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –æ—à–∏–±–∫–∏ –≤—ã—à–µ.")
    
    return success


if __name__ == "__main__":
    main()