"""
–ü—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–µ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
"""
import sys
sys.path.append('src')

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.utils import class_weight
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional, Conv1D, GlobalMaxPooling1D
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau
from tensorflow.keras.optimizers import Adam
import joblib
import re
from pathlib import Path
import random

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
MAX_WORDS = 50000  # –ë–æ–ª—å—à–µ —Å–ª–æ–≤ –¥–ª—è –ª—É—á—à–µ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è
MAX_LENGTH = 200   # –î–ª–∏–Ω–Ω–µ–µ —Ç–µ–∫—Å—Ç—ã
EMBEDDING_DIM = 300  # –ë–æ–ª—å—à–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å
LSTM_UNITS = 128
EPOCHS = 50  # –ë–æ–ª—å—à–µ —ç–ø–æ—Ö
BATCH_SIZE = 32  # –ú–µ–Ω—å—à–µ –±–∞—Ç—á –¥–ª—è –ª—É—á—à–µ–≥–æ –æ–±—É—á–µ–Ω–∏—è

# –ü—É—Ç–∏
MODELS_DIR = Path("models")
MODELS_DIR.mkdir(exist_ok=True)

def create_quality_dataset(num_samples=50000):
    """–°–æ–∑–¥–∞–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞"""
    print("–°–æ–∑–¥–∞–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞...")
    
    data = []
    
    # –ü–û–ó–ò–¢–ò–í–ù–´–ï –ü–†–ò–ú–ï–†–´ (—Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç—ã)
    positive_contexts = {
        '–ø–æ–∫—É–ø–∫–∏': [
            "–ó–∞–∫–∞–∑–∞–ª {item}, –ø—Ä–∏—à–ª–æ –±—ã—Å—Ç—Ä–æ, –∫–∞—á–µ—Å—Ç–≤–æ {quality}! {emotion}",
            "–ö—É–ø–∏–ª {item} –∏ –æ—Å—Ç–∞–ª—Å—è –æ—á–µ–Ω—å –¥–æ–≤–æ–ª–µ–Ω. {quality} —Ç–æ–≤–∞—Ä, {service} —Å–µ—Ä–≤–∏—Å!",
            "{item} –ø—Ä–µ–≤–∑–æ—à–µ–ª –≤—Å–µ –æ–∂–∏–¥–∞–Ω–∏—è! {quality} –∫–∞—á–µ—Å—Ç–≤–æ, {emotion}",
            "–ù–∞–∫–æ–Ω–µ—Ü-—Ç–æ –Ω–∞—à–µ–ª –∏–¥–µ–∞–ª—å–Ω—ã–π {item}! {service} –æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏–µ, –≤—Å–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥—É—é!",
            "–ü–æ–∫—É–ø–∫–æ–π {item} –æ—á–µ–Ω—å –¥–æ–≤–æ–ª–µ–Ω, {quality} –∏—Å–ø–æ–ª–Ω–µ–Ω–∏–µ, {emotion}"
        ],
        '—ç–º–æ—Ü–∏–∏': [
            "–°–µ–≥–æ–¥–Ω—è —Ç–∞–∫–æ–π {day} –¥–µ–Ω—å! –ù–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ {mood}, –≤—Å–µ {result}!",
            "–Ø —Ç–∞–∫ {emotion}! –ù–∞–∫–æ–Ω–µ—Ü-—Ç–æ {event}, —ç—Ç–æ {feeling}!",
            "–ö–∞–∫–æ–µ {feeling} —É—Ç—Ä–æ! –ü—Ä–æ—Å–Ω—É–ª—Å—è —Å {mood} –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ–º!",
            "–≠—Ç–æ –ø—Ä–æ—Å—Ç–æ {amazing}! –Ø –≤ –ø–æ–ª–Ω–æ–º {emotion}!",
            "{event} –ø—Ä–æ—à–ª–æ {result}! –ß—É–≤—Å—Ç–≤—É—é —Å–µ–±—è {mood}!"
        ],
        '–¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è': [
            "–£—Ä–∞! –Ø {achievement}! –≠—Ç–æ –±—ã–ª–æ {difficulty}, –Ω–æ —è {result}!",
            "–ù–∞–∫–æ–Ω–µ—Ü-—Ç–æ {achievement}! –°—Ç–æ–ª—å–∫–æ {effort}, –Ω–æ –æ–Ω–æ —Ç–æ–≥–æ {worth}!",
            "–ì–æ—Ä–∂—É—Å—å —Å–æ–±–æ–π - {achievement}! {emotion} –±–µ–∑–≥—Ä–∞–Ω–∏—á–Ω–∞!",
            "–°–¥–µ–ª–∞–ª —ç—Ç–æ! {achievement} - –º–æ—è –Ω–æ–≤–∞—è {milestone}!",
            "–ù–µ–≤–µ—Ä–æ—è—Ç–Ω–æ! –Ø —Å–º–æ–≥ {achievement}! {emotion}!"
        ],
        '–æ—Ç–Ω–æ—à–µ–Ω–∏—è': [
            "–í—Å—Ç—Ä–µ—Ç–∏–ª {person} —á–µ–ª–æ–≤–µ–∫–∞, –º—ã —Ç–∞–∫ {connection}!",
            "–° {person} —Ç–∞–∫ {feeling} –ø—Ä–æ–≤–æ–¥–∏—Ç—å –≤—Ä–µ–º—è! {emotion}!",
            "–õ—é–±–ª—é —Å–≤–æ—é {family}, –æ–Ω–∏ –¥–µ–ª–∞—é—Ç –º–µ–Ω—è {mood}!",
            "–î—Ä—É–∑—å—è - —ç—Ç–æ {treasure}! –° –Ω–∏–º–∏ –≤—Å–µ–≥–¥–∞ {feeling}!",
            "–ë–ª–∞–≥–æ–¥–∞—Ä–µ–Ω {person} –∑–∞ {action}, —ç—Ç–æ —Ç–∞–∫ {emotion}!"
        ]
    }
    
    positive_items = ["—Ç–µ–ª–µ—Ñ–æ–Ω", "–Ω–æ—É—Ç–±—É–∫", "–Ω–∞—É—à–Ω–∏–∫–∏", "—á–∞—Å—ã", "–∫—Ä–æ—Å—Å–æ–≤–∫–∏", "—Ä—é–∫–∑–∞–∫", "–∫–Ω–∏–≥—É", "–ø–æ–¥–∞—Ä–æ–∫"]
    positive_quality = ["–æ—Ç–ª–∏—á–Ω–æ–µ", "–ø—Ä–µ–≤–æ—Å—Ö–æ–¥–Ω–æ–µ", "–∑–∞–º–µ—á–∞—Ç–µ–ª—å–Ω–æ–µ", "—à–∏–∫–∞—Ä–Ω–æ–µ", "–≤—ã—Å–æ–∫–æ–∫–ª–∞—Å—Å–Ω–æ–µ", "–ø—Ä–µ–º–∏–∞–ª—å–Ω–æ–µ"]
    positive_service = ["–±—ã—Å—Ç—Ä—ã–π", "–≤–µ–∂–ª–∏–≤—ã–π", "–ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π", "–≤–Ω–∏–º–∞—Ç–µ–ª—å–Ω—ã–π", "–æ—Ç–∑—ã–≤—á–∏–≤—ã–π"]
    positive_emotion = ["–°—É–ø–µ—Ä", "–í–æ—Å—Ç–æ—Ä–≥", "–°—á–∞—Å—Ç–ª–∏–≤", "–†–∞–¥", "–î–æ–≤–æ–ª–µ–Ω", "–í –≤–æ—Å—Ö–∏—â–µ–Ω–∏–∏", "–†–∞–¥–æ—Å—Ç—å"]
    positive_day = ["–ø—Ä–µ–∫—Ä–∞—Å–Ω—ã–π", "—á—É–¥–µ—Å–Ω—ã–π", "—Å–æ–ª–Ω–µ—á–Ω—ã–π", "—É–¥–∞—á–Ω—ã–π", "—Å—á–∞—Å—Ç–ª–∏–≤—ã–π", "–≤–æ–ª—à–µ–±–Ω—ã–π"]
    positive_mood = ["–æ—Ç–ª–∏—á–Ω–æ–µ", "–ø—Ä–µ–∫—Ä–∞—Å–Ω–æ–µ", "—Ä–∞–¥–æ—Å—Ç–Ω–æ–µ", "–ø—Ä–∏–ø–æ–¥–Ω—è—Ç–æ–µ", "–≤–æ—Å—Ç–æ—Ä–∂–µ–Ω–Ω–æ–µ"]
    positive_result = ["–ø–æ–ª—É—á–∞–µ—Ç—Å—è", "—É–¥–∞–µ—Ç—Å—è", "—Å–∫–ª–∞–¥—ã–≤–∞–µ—Ç—Å—è", "–≤—ã—Ö–æ–¥–∏—Ç", "–ø–æ–ª—É—á–∏–ª–æ—Å—å"]
    positive_feeling = ["—Å—á–∞—Å—Ç—å–µ", "—Ä–∞–¥–æ—Å—Ç—å", "–≤–æ—Å—Ç–æ—Ä–≥", "—É–¥–æ–≤–æ–ª—å—Å—Ç–≤–∏–µ", "–±–ª–∞–∂–µ–Ω—Å—Ç–≤–æ"]
    positive_amazing = ["–ø–æ—Ç—Ä—è—Å–∞—é—â–µ", "–Ω–µ–≤–µ—Ä–æ—è—Ç–Ω–æ", "—Ñ–∞–Ω—Ç–∞—Å—Ç–∏–∫–∞", "—á—É–¥–µ—Å–Ω–æ", "–≤–µ–ª–∏–∫–æ–ª–µ–ø–Ω–æ"]
    positive_achievement = ["—Å–¥–∞–ª —ç–∫–∑–∞–º–µ–Ω", "–ø–æ–ª—É—á–∏–ª —Ä–∞–±–æ—Ç—É", "–∑–∞–∫–æ–Ω—á–∏–ª –ø—Ä–æ–µ–∫—Ç", "–≤—ã–∏–≥—Ä–∞–ª –∫–æ–Ω–∫—É—Ä—Å", "–¥–æ—Å—Ç–∏–≥ —Ü–µ–ª–∏"]
    positive_difficulty = ["—Å–ª–æ–∂–Ω–æ", "—Ç—Ä—É–¥–Ω–æ", "–Ω–µ–ø—Ä–æ—Å—Ç–æ", "—Ç—è–∂–µ–ª–æ", "–Ω–µ–ª–µ–≥–∫–æ"]
    positive_effort = ["—É—Å–∏–ª–∏–π", "—Ç—Ä—É–¥–∞", "—Å—Ç–∞—Ä–∞–Ω–∏–π", "–ø–æ–ø—ã—Ç–æ–∫", "—Ä–∞–±–æ—Ç—ã"]
    positive_worth = ["—Å—Ç–æ–∏–ª–æ", "—Å—Ç–æ–∏—Ç", "–æ–ø—Ä–∞–≤–¥–∞–Ω–æ", "–∑–∞—Å–ª—É–∂–µ–Ω–æ", "–æ–ø—Ä–∞–≤–¥–∞–ª–æ"]
    positive_milestone = ["–ø–æ–±–µ–¥–∞", "–¥–æ—Å—Ç–∏–∂–µ–Ω–∏–µ", "–≤–µ—Ä—à–∏–Ω–∞", "—Ä–µ–∫–æ—Ä–¥", "—É—Å–ø–µ—Ö"]
    positive_person = ["–∑–∞–º–µ—á–∞—Ç–µ–ª—å–Ω–æ–≥–æ", "–ø—Ä–µ–∫—Ä–∞—Å–Ω–æ–≥–æ", "—É–¥–∏–≤–∏—Ç–µ–ª—å–Ω–æ–≥–æ", "—á—É–¥–µ—Å–Ω–æ–≥–æ", "–∫–ª–∞—Å—Å–Ω–æ–≥–æ"]
    positive_connection = ["—Å–æ–≤–ø–∞–ª–∏", "–ø–æ–¥—Ä—É–∂–∏–ª–∏—Å—å", "—Å–æ—à–ª–∏—Å—å", "–ø–æ–Ω–∏–º–∞–µ–º –¥—Ä—É–≥ –¥—Ä—É–≥–∞", "–Ω–∞ –æ–¥–Ω–æ–π –≤–æ–ª–Ω–µ"]
    positive_family = ["—Å–µ–º—å—é", "—Ä–æ–¥–Ω—ã—Ö", "–±–ª–∏–∑–∫–∏—Ö", "–¥—Ä—É–∑–µ–π", "–∫–æ–º–∞–Ω–¥—É"]
    positive_treasure = ["—Å–æ–∫—Ä–æ–≤–∏—â–µ", "–±–æ–≥–∞—Ç—Å—Ç–≤–æ", "—Å—á–∞—Å—Ç—å–µ", "–ø–æ–¥–∞—Ä–æ–∫", "–±–ª–∞–≥–æ"]
    positive_action = ["–ø–æ–¥–¥–µ—Ä–∂–∫—É", "–ø–æ–º–æ—â—å", "—Å–æ–≤–µ—Ç", "–≤–Ω–∏–º–∞–Ω–∏–µ", "–∑–∞–±–æ—Ç—É"]
    
    # –ù–ï–ì–ê–¢–ò–í–ù–´–ï –ü–†–ò–ú–ï–†–´
    negative_contexts = {
        '–ø–æ–∫—É–ø–∫–∏': [
            "–ö—É–ø–∏–ª {item}, –ø–æ–ª–Ω–æ–µ {disappointment}. {quality} –∫–∞—á–µ—Å—Ç–≤–æ, {emotion}",
            "–ó–∞–∫–∞–∑—ã–≤–∞–ª {item}, –ø—Ä–∏—à–µ–ª {defect}. {service} —Å–µ—Ä–≤–∏—Å, {emotion}!",
            "{item} –æ–∫–∞–∑–∞–ª—Å—è {quality}, –¥–µ–Ω—å–≥–∏ {waste}. {emotion}",
            "–†–∞–∑–æ—á–∞—Ä–æ–≤–∞–Ω –ø–æ–∫—É–ø–∫–æ–π {item}. {defect}, {service} –ø–æ–¥–¥–µ—Ä–∂–∫–∞",
            "–ù–µ –ø–æ–∫—É–ø–∞–π—Ç–µ {item}! {quality} —Ç–æ–≤–∞—Ä, {emotion}"
        ],
        '—ç–º–æ—Ü–∏–∏': [
            "–ß—Ç–æ –∑–∞ {day} –¥–µ–Ω—å... –ù–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ {mood}, –≤—Å–µ {result}",
            "–Ø —Ç–∞–∫ {emotion}... –û–ø—è—Ç—å {event}, —ç—Ç–æ {feeling}",
            "{feeling} —É—Ç—Ä–æ, –ø—Ä–æ—Å–Ω—É–ª—Å—è —Å {mood} –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ–º",
            "–í—Å–µ {result}, —è –≤ {emotion}. –ö–æ–≥–¥–∞ —ç—Ç–æ {end}?",
            "–°–Ω–æ–≤–∞ {event}. –ß—É–≤—Å—Ç–≤—É—é —Å–µ–±—è {mood}"
        ],
        '–ø—Ä–æ–±–ª–µ–º—ã': [
            "{problem} –æ–ø—è—Ç—å {occurred}. –£–∂–µ {tired} –æ—Ç —ç—Ç–æ–≥–æ",
            "–ù–µ –º–æ–≥—É –±–æ–ª—å—à–µ! {problem} –º–µ–Ω—è {effect}",
            "–ö–æ–≥–¥–∞ –∂–µ {problem} –∑–∞–∫–æ–Ω—á–∏—Ç—Å—è? {emotion}",
            "–ò–∑-–∑–∞ {problem} –≤—Å–µ {result}. {feeling}",
            "{problem} —Ä–∞–∑—Ä—É—à–∞–µ—Ç –º–æ—é {life}. {emotion}"
        ],
        '—Ä–∞–∑–æ—á–∞—Ä–æ–≤–∞–Ω–∏—è': [
            "–û–∂–∏–¥–∞–ª {expected}, –ø–æ–ª—É—á–∏–ª {reality}. {emotion}",
            "–î—É–º–∞–ª –±—É–¥–µ—Ç {expected}, –∞ –≤—ã—à–ª–æ {reality}. {disappointment}",
            "–ù–∞–¥–µ—è–ª—Å—è –Ω–∞ {expected}, –Ω–æ {reality}. {feeling}",
            "–í–º–µ—Å—Ç–æ {expected} –ø–æ–ª—É—á–∏–ª {reality}. {emotion}",
            "–û–±–µ—â–∞–ª–∏ {expected}, –¥–∞–ª–∏ {reality}. {disappointment}"
        ]
    }
    
    negative_item = ["—Ç–µ–ª–µ—Ñ–æ–Ω", "–Ω–æ—É—Ç–±—É–∫", "—Ç–æ–≤–∞—Ä", "–∑–∞–∫–∞–∑", "–ø—Ä–æ–¥—É–∫—Ç", "–¥–µ–≤–∞–π—Å"]
    negative_disappointment = ["—Ä–∞–∑–æ—á–∞—Ä–æ–≤–∞–Ω–∏–µ", "–∫–æ—à–º–∞—Ä", "—É–∂–∞—Å", "–ø—Ä–æ–≤–∞–ª", "—Ñ–∏–∞—Å–∫–æ"]
    negative_quality = ["—É–∂–∞—Å–Ω–æ–µ", "–æ—Ç–≤—Ä–∞—Ç–∏—Ç–µ–ª—å–Ω–æ–µ", "–Ω–∏–∫—É–¥—ã—à–Ω–æ–µ", "–ø–ª–æ—Ö–æ–µ", "–Ω–∏–∑–∫–æ–µ"]
    negative_emotion = ["–ó–æ–ª", "–†–∞—Å—Å—Ç—Ä–æ–µ–Ω", "–†–∞–∑–æ—á–∞—Ä–æ–≤–∞–Ω", "–û–±–∏–∂–µ–Ω", "–†–∞–∑–¥—Ä–∞–∂–µ–Ω", "–ó–ª—é—Å—å"]
    negative_defect = ["–±—Ä–∞–∫", "–¥–µ—Ñ–µ–∫—Ç", "—Å–ª–æ–º–∞–Ω–Ω—ã–π —Ç–æ–≤–∞—Ä", "–Ω–µ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π", "–∏—Å–ø–æ—Ä—á–µ–Ω–Ω—ã–π"]
    negative_service = ["—É–∂–∞—Å–Ω—ã–π", "–≥—Ä—É–±—ã–π", "–Ω–µ–∫–æ–º–ø–µ—Ç–µ–Ω—Ç–Ω—ã–π", "–º–µ–¥–ª–µ–Ω–Ω—ã–π", "–æ—Ç–≤—Ä–∞—Ç–∏—Ç–µ–ª—å–Ω—ã–π"]
    negative_waste = ["–Ω–∞ –≤–µ—Ç–µ—Ä", "–≤—ã–±—Ä–æ—à–µ–Ω—ã", "–ø–æ—Ç—Ä–∞—á–µ–Ω—ã –∑—Ä—è", "–ø—Ä–æ–ø–∞–ª–∏", "–ø–æ—Ç–µ—Ä—è–Ω—ã"]
    negative_day = ["—É–∂–∞—Å–Ω—ã–π", "–∫–æ—à–º–∞—Ä–Ω—ã–π", "–ø–∞—Ä—à–∏–≤—ã–π", "–æ—Ç–≤—Ä–∞—Ç–∏—Ç–µ–ª—å–Ω—ã–π", "–ø—Ä–æ–∫–ª—è—Ç—ã–π"]
    negative_mood = ["–ø–ª–æ—Ö–æ–µ", "—É–∂–∞—Å–Ω–æ–µ", "–ø–∞—Ä—à–∏–≤–æ–µ", "–º–µ—Ä–∑–∫–æ–µ", "–æ—Ç–≤—Ä–∞—Ç–∏—Ç–µ–ª—å–Ω–æ–µ"]
    negative_result = ["–ø–ª–æ—Ö–æ", "—Ä–∞–∑–≤–∞–ª–∏–≤–∞–µ—Ç—Å—è", "—Ä—É—à–∏—Ç—Å—è", "–∏–¥–µ—Ç –Ω–µ —Ç–∞–∫", "–ø—Ä–æ–≤–∞–ª–∏–ª–æ—Å—å"]
    negative_event = ["–Ω–µ –ø–æ–ª—É—á–∏–ª–æ—Å—å", "–ø—Ä–æ–≤–∞–ª–∏–ª–æ—Å—å", "—Å–æ—Ä–≤–∞–ª–æ—Å—å", "–ø–æ—à–ª–æ –Ω–µ —Ç–∞–∫", "—Å–ª—É—á–∏–ª–æ—Å—å"]
    negative_feeling = ["—É–∂–∞—Å", "–∫–æ—à–º–∞—Ä", "–æ—Ç—á–∞—è–Ω–∏–µ", "–≥—Ä—É—Å—Ç—å", "–ø–µ—á–∞–ª—å"]
    negative_end = ["–∫–æ–Ω—á–∏—Ç—Å—è", "–∑–∞–∫–æ–Ω—á–∏—Ç—Å—è", "–ø—Ä–µ–∫—Ä–∞—Ç–∏—Ç—Å—è", "–æ—Å—Ç–∞–Ω–æ–≤–∏—Ç—Å—è", "–ø—Ä–æ–π–¥–µ—Ç"]
    negative_problem = ["–ü—Ä–æ–±–ª–µ–º–∞", "–ù–µ–ø—Ä–∏—è—Ç–Ω–æ—Å—Ç—å", "–ë–µ–¥–∞", "–ö—Ä–∏–∑–∏—Å", "–ö–∞—Ç–∞—Å—Ç—Ä–æ—Ñ–∞"]
    negative_occurred = ["—Å–ª—É—á–∏–ª–∞—Å—å", "–ø—Ä–æ–∏–∑–æ—à–ª–∞", "–ø–æ–≤—Ç–æ—Ä–∏–ª–∞—Å—å", "–≤–µ—Ä–Ω—É–ª–∞—Å—å", "–Ω–∞—á–∞–ª–∞—Å—å"]
    negative_tired = ["—É—Å—Ç–∞–ª", "–∏–∑–º—É—á–µ–Ω", "–≤—ã–º–æ—Ç–∞–Ω", "–∏–∑–Ω—É—Ä–µ–Ω", "–∑–∞–º—É—á–µ–Ω"]
    negative_effect = ["–¥–æ—Å—Ç–∞–ª–∞", "–∏–∑–º—É—á–∏–ª–∞", "–¥–æ–≤–µ–ª–∞", "—Ä–∞–∑—Ä—É—à–∞–µ—Ç", "—É–±–∏–≤–∞–µ—Ç"]
    negative_life = ["–∂–∏–∑–Ω—å", "–ø–ª–∞–Ω—ã", "–Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ", "–∑–¥–æ—Ä–æ–≤—å–µ", "—Ä–∞–±–æ—Ç—É"]
    negative_expected = ["–ª—É—á—à–µ–µ", "–∫–∞—á–µ—Å—Ç–≤–æ", "—Ö–æ—Ä–æ—à–µ–µ", "–Ω–æ—Ä–º–∞–ª—å–Ω–æ–µ", "–¥–æ—Å—Ç–æ–π–Ω–æ–µ"]
    negative_reality = ["—Ö—É–¥—à–µ–µ", "–º—É—Å–æ—Ä", "–µ—Ä—É–Ω–¥—É", "–∫–æ—à–º–∞—Ä", "—É–∂–∞—Å"]
    
    # –ù–ï–ô–¢–†–ê–õ–¨–ù–´–ï –ü–†–ò–ú–ï–†–´
    neutral_contexts = {
        '–æ–ø–∏—Å–∞–Ω–∏—è': [
            "{item} –∏–º–µ–µ—Ç {feature}. –†–∞–±–æ—Ç–∞–µ—Ç {performance}",
            "–ò—Å–ø–æ–ª—å–∑—É—é {item} —É–∂–µ {time}. {observation}",
            "{item} —Å—Ç–æ–∏—Ç {price}. –§—É–Ω–∫—Ü–∏–∏ {standard}",
            "–í {item} –µ—Å—Ç—å {pros}, –Ω–æ —Ç–∞–∫–∂–µ {cons}",
            "{item} –ø–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è {purpose}. {summary}"
        ],
        '—Ñ–∞–∫—Ç—ã': [
            "–°–µ–≥–æ–¥–Ω—è {day}. –ü–æ–≥–æ–¥–∞ {weather}, –ø–ª–∞–Ω—ã {plans}",
            "–ù–∞ —Ä–∞–±–æ—Ç–µ {routine}. –í—Å–µ {status}",
            "–°–¥–µ–ª–∞–ª {task}. –¢–µ–ø–µ—Ä—å –Ω—É–∂–Ω–æ {next}",
            "{event} –ø—Ä–æ—à–ª–æ {normally}. –†–µ–∑—É–ª—å—Ç–∞—Ç {expected}",
            "–î–µ–Ω—å –∫–∞–∫ –¥–µ–Ω—å. {routine}, –Ω–∏—á–µ–≥–æ {special}"
        ],
        '–Ω–∞–±–ª—é–¥–µ–Ω–∏—è': [
            "–ó–∞–º–µ—Ç–∏–ª, —á—Ç–æ {observation}. {conclusion}",
            "–õ—é–¥–∏ {behavior}. –≠—Ç–æ {normal}",
            "–í –≥–æ—Ä–æ–¥–µ {happening}. –ñ–∏–∑–Ω—å {continues}",
            "–ß–∏—Ç–∞–ª –ø—Ä–æ {topic}. {interesting}",
            "–í–∏–¥–µ–ª {event}. {reaction}"
        ]
    }
    
    neutral_item = ["—Ç–æ–≤–∞—Ä", "–ø—Ä–æ–¥—É–∫—Ç", "—É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ", "–ø—Ä–µ–¥–º–µ—Ç", "–≤–µ—â—å"]
    neutral_feature = ["—Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏", "–æ–±—ã—á–Ω—ã–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏", "–±–∞–∑–æ–≤—ã–µ –æ–ø—Ü–∏–∏", "—Å—Ä–µ–¥–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã"]
    neutral_performance = ["–Ω–æ—Ä–º–∞–ª—å–Ω–æ", "—Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ", "–∫–∞–∫ –æ–∂–∏–¥–∞–ª–æ—Å—å", "–±–µ–∑ —Å—é—Ä–ø—Ä–∏–∑–æ–≤", "–æ–±—ã—á–Ω–æ"]
    neutral_time = ["–Ω–µ–¥–µ–ª—é", "–º–µ—Å—è—Ü", "–Ω–µ—Å–∫–æ–ª—å–∫–æ –¥–Ω–µ–π", "–ø–∞—Ä—É –Ω–µ–¥–µ–ª—å", "–Ω–µ–∫–æ—Ç–æ—Ä–æ–µ –≤—Ä–µ–º—è"]
    neutral_observation = ["–†–∞–±–æ—Ç–∞–µ—Ç —Å—Ç–∞–±–∏–ª—å–Ω–æ", "–ë–µ–∑ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–µ–π", "–í—Å–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ", "–ù–∏—á–µ–≥–æ –Ω–µ–æ–±—ã—á–Ω–æ–≥–æ"]
    neutral_price = ["—Å—Ä–µ–¥–Ω—é—é —Ü–µ–Ω—É", "–æ–±—ã—á–Ω—ã–µ –¥–µ–Ω—å–≥–∏", "—Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—É—é —Å—Ç–æ–∏–º–æ—Å—Ç—å", "—Ä—ã–Ω–æ—á–Ω—É—é —Ü–µ–Ω—É"]
    neutral_standard = ["—Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ", "–æ–±—ã—á–Ω—ã–µ", "–±–∞–∑–æ–≤—ã–µ", "—Ç–∏–ø–∏—á–Ω—ã–µ", "—Å—Ä–µ–¥–Ω–∏–µ"]
    neutral_pros = ["–ø–ª—é—Å—ã", "–¥–æ—Å—Ç–æ–∏–Ω—Å—Ç–≤–∞", "–ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞", "–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ —Å—Ç–æ—Ä–æ–Ω—ã"]
    neutral_cons = ["–º–∏–Ω—É—Å—ã", "–Ω–µ–¥–æ—Å—Ç–∞—Ç–∫–∏", "–Ω—é–∞–Ω—Å—ã", "–æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏"]
    neutral_purpose = ["–ø–æ–≤—Å–µ–¥–Ω–µ–≤–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è", "–æ–±—ã—á–Ω—ã—Ö –∑–∞–¥–∞—á", "—Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö —Ü–µ–ª–µ–π", "–±–∞–∑–æ–≤—ã—Ö –Ω—É–∂–¥"]
    neutral_summary = ["–í —Ü–µ–ª–æ–º –Ω–æ—Ä–º–∞–ª—å–Ω–æ", "–°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç —Ü–µ–Ω–µ", "–î–ª—è —Å–≤–æ–∏—Ö –∑–∞–¥–∞—á –ø–æ–¥—Ö–æ–¥–∏—Ç", "–û–±—ã—á–Ω—ã–π –≤–∞—Ä–∏–∞–Ω—Ç"]
    neutral_day = ["–ø–æ–Ω–µ–¥–µ–ª—å–Ω–∏–∫", "–≤—Ç–æ—Ä–Ω–∏–∫", "—Å—Ä–µ–¥–∞", "—á–µ—Ç–≤–µ—Ä–≥", "–ø—è—Ç–Ω–∏—Ü–∞"]
    neutral_weather = ["–æ–±—ã—á–Ω–∞—è", "–ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è", "–ø–æ —Å–µ–∑–æ–Ω—É", "—Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è", "—Ç–∏–ø–∏—á–Ω–∞—è"]
    neutral_plans = ["–æ–±—ã—á–Ω—ã–µ", "—Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ", "—Ä–∞–±–æ—á–∏–µ", "–ø–æ–≤—Å–µ–¥–Ω–µ–≤–Ω—ã–µ", "—Ç–∏–ø–∏—á–Ω—ã–µ"]
    neutral_routine = ["–æ–±—ã—á–Ω–∞—è —Ä–∞–±–æ—Ç–∞", "—Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –∑–∞–¥–∞—á–∏", "—Ä—É—Ç–∏–Ω–∞", "—Ç–µ–∫—É—á–∫–∞", "–æ–±—ã—á–Ω—ã–µ –¥–µ–ª–∞"]
    neutral_status = ["–∫–∞–∫ –æ–±—ã—á–Ω–æ", "–ø–æ –ø–ª–∞–Ω—É", "–≤ –Ω–æ—Ä–º–µ", "—Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ", "–±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π"]
    neutral_task = ["—Ä–∞–±–æ—Ç—É", "–∑–∞–¥–∞–Ω–∏–µ", "–¥–µ–ª–æ", "–ø–æ—Ä—É—á–µ–Ω–∏–µ", "–∑–∞–¥–∞—á—É"]
    neutral_next = ["–¥—Ä—É–≥–æ–µ", "—Å–ª–µ–¥—É—é—â–µ–µ", "–æ—Å—Ç–∞–ª—å–Ω–æ–µ", "–ø—Ä–æ–¥–æ–ª–∂–∏—Ç—å", "–∑–∞–∫–æ–Ω—á–∏—Ç—å"]
    neutral_event = ["–°–æ–≤–µ—â–∞–Ω–∏–µ", "–í—Å—Ç—Ä–µ—á–∞", "–ú–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–µ", "–°–æ–±—Ä–∞–Ω–∏–µ", "–°–æ–±—ã—Ç–∏—è"]
    neutral_normally = ["–ø–æ –ø–ª–∞–Ω—É", "–∫–∞–∫ –æ–±—ã—á–Ω–æ", "—Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ", "–±–µ–∑ —ç–∫—Å—Ü–µ—Å—Å–æ–≤", "–Ω–æ—Ä–º–∞–ª—å–Ω–æ"]
    neutral_expected = ["–æ–∂–∏–¥–∞–µ–º—ã–π", "–ø—Ä–µ–¥—Å–∫–∞–∑—É–µ–º—ã–π", "—Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π", "–æ–±—ã—á–Ω—ã–π", "—Ç–∏–ø–∏—á–Ω—ã–π"]
    neutral_special = ["–æ—Å–æ–±–µ–Ω–Ω–æ–≥–æ", "–Ω–µ–æ–±—ã—á–Ω–æ–≥–æ", "–≤—ã–¥–∞—é—â–µ–≥–æ—Å—è", "–ø—Ä–∏–º–µ—á–∞—Ç–µ–ª—å–Ω–æ–≥–æ", "–∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ–≥–æ"]
    neutral_behavior = ["–≤–µ–¥—É—Ç —Å–µ–±—è –æ–±—ã—á–Ω–æ", "–¥–µ–ª–∞—é—Ç —Å–≤–æ–µ –¥–µ–ª–æ", "–∂–∏–≤—É—Ç —Å–≤–æ–µ–π –∂–∏–∑–Ω—å—é", "–∑–∞–Ω—è—Ç—ã –¥–µ–ª–∞–º–∏"]
    neutral_normal = ["–Ω–æ—Ä–º–∞–ª—å–Ω–æ", "–æ–±—ã—á–Ω–æ", "–µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ", "—Ç–∏–ø–∏—á–Ω–æ", "—Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ"]
    neutral_happening = ["–∏–¥–µ—Ç —Å—Ç—Ä–æ–π–∫–∞", "—Ä–µ–º–æ–Ω—Ç –¥–æ—Ä–æ–≥", "–æ–±—ã—á–Ω–∞—è –∂–∏–∑–Ω—å", "–ø–æ–≤—Å–µ–¥–Ω–µ–≤–Ω–æ—Å—Ç—å", "–±—É–¥–Ω–∏"]
    neutral_continues = ["–ø—Ä–æ–¥–æ–ª–∂–∞–µ—Ç—Å—è", "–∏–¥–µ—Ç —Å–≤–æ–∏–º —á–µ—Ä–µ–¥–æ–º", "—Ç–µ—á–µ—Ç", "–¥–≤–∏–∂–µ—Ç—Å—è", "–Ω–µ –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç—Å—è"]
    neutral_topic = ["–Ω–æ–≤–æ—Å—Ç–∏", "—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏", "—Å–æ–±—ã—Ç–∏—è", "—Ç–µ–Ω–¥–µ–Ω—Ü–∏–∏", "–∏–∑–º–µ–Ω–µ–Ω–∏—è"]
    neutral_interesting = ["–õ—é–±–æ–ø—ã—Ç–Ω–æ", "–ü–æ–∑–Ω–∞–≤–∞—Ç–µ–ª—å–Ω–æ", "–ò–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω–æ", "–ü–æ–ª–µ–∑–Ω–æ –∑–Ω–∞—Ç—å", "–ò–Ω—Ç–µ—Ä–µ—Å–Ω—ã–π —Ñ–∞–∫—Ç"]
    neutral_reaction = ["–û–±—ã—á–Ω–æ–µ –¥–µ–ª–æ", "–ù–∏—á–µ–≥–æ –æ—Å–æ–±–µ–Ω–Ω–æ–≥–æ", "–ë—ã–≤–∞–µ—Ç", "–ù–æ—Ä–º–∞–ª—å–Ω–æ", "–ñ–∏–∑–Ω—å"]
    
    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–æ–∑–∏—Ç–∏–≤–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤
    for _ in range(num_samples // 3):
        context_type = random.choice(list(positive_contexts.keys()))
        template = random.choice(positive_contexts[context_type])
        
        # –ó–∞–ø–æ–ª–Ω—è–µ–º —à–∞–±–ª–æ–Ω
        text = template
        replacements = {
            '{item}': random.choice(positive_items),
            '{quality}': random.choice(positive_quality),
            '{service}': random.choice(positive_service),
            '{emotion}': random.choice(positive_emotion),
            '{day}': random.choice(positive_day),
            '{mood}': random.choice(positive_mood),
            '{result}': random.choice(positive_result),
            '{event}': random.choice(positive_achievement),
            '{feeling}': random.choice(positive_feeling),
            '{amazing}': random.choice(positive_amazing),
            '{achievement}': random.choice(positive_achievement),
            '{difficulty}': random.choice(positive_difficulty),
            '{effort}': random.choice(positive_effort),
            '{worth}': random.choice(positive_worth),
            '{milestone}': random.choice(positive_milestone),
            '{person}': random.choice(positive_person),
            '{connection}': random.choice(positive_connection),
            '{family}': random.choice(positive_family),
            '{treasure}': random.choice(positive_treasure),
            '{action}': random.choice(positive_action)
        }
        
        for key, value in replacements.items():
            text = text.replace(key, value)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –≤–∞—Ä–∏–∞—Ç–∏–≤–Ω–æ—Å—Ç—å
        if random.random() > 0.5:
            text += " " + random.choice(["üòä", "üòÑ", "üéâ", "‚ù§Ô∏è", "üëç", "‚ú®", "üôå", ""])
        
        data.append({"text": text, "label": "positive"})
    
    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤
    for _ in range(num_samples // 3):
        context_type = random.choice(list(negative_contexts.keys()))
        template = random.choice(negative_contexts[context_type])
        
        text = template
        replacements = {
            '{item}': random.choice(negative_item),
            '{disappointment}': random.choice(negative_disappointment),
            '{quality}': random.choice(negative_quality),
            '{emotion}': random.choice(negative_emotion),
            '{defect}': random.choice(negative_defect),
            '{service}': random.choice(negative_service),
            '{waste}': random.choice(negative_waste),
            '{day}': random.choice(negative_day),
            '{mood}': random.choice(negative_mood),
            '{result}': random.choice(negative_result),
            '{event}': random.choice(negative_event),
            '{feeling}': random.choice(negative_feeling),
            '{end}': random.choice(negative_end),
            '{problem}': random.choice(negative_problem),
            '{occurred}': random.choice(negative_occurred),
            '{tired}': random.choice(negative_tired),
            '{effect}': random.choice(negative_effect),
            '{life}': random.choice(negative_life),
            '{expected}': random.choice(negative_expected),
            '{reality}': random.choice(negative_reality)
        }
        
        for key, value in replacements.items():
            text = text.replace(key, value)
        
        if random.random() > 0.5:
            text += " " + random.choice(["üòî", "üò°", "üòû", "üíî", "üëé", "üò¢", "ü§¨", ""])
        
        data.append({"text": text, "label": "negative"})
    
    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –Ω–µ–π—Ç—Ä–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤
    for _ in range(num_samples - 2 * (num_samples // 3)):
        context_type = random.choice(list(neutral_contexts.keys()))
        template = random.choice(neutral_contexts[context_type])
        
        text = template
        replacements = {
            '{item}': random.choice(neutral_item),
            '{feature}': random.choice(neutral_feature),
            '{performance}': random.choice(neutral_performance),
            '{time}': random.choice(neutral_time),
            '{observation}': random.choice(neutral_observation),
            '{price}': random.choice(neutral_price),
            '{standard}': random.choice(neutral_standard),
            '{pros}': random.choice(neutral_pros),
            '{cons}': random.choice(neutral_cons),
            '{purpose}': random.choice(neutral_purpose),
            '{summary}': random.choice(neutral_summary),
            '{day}': random.choice(neutral_day),
            '{weather}': random.choice(neutral_weather),
            '{plans}': random.choice(neutral_plans),
            '{routine}': random.choice(neutral_routine),
            '{status}': random.choice(neutral_status),
            '{task}': random.choice(neutral_task),
            '{next}': random.choice(neutral_next),
            '{event}': random.choice(neutral_event),
            '{normally}': random.choice(neutral_normally),
            '{expected}': random.choice(neutral_expected),
            '{special}': random.choice(neutral_special),
            '{behavior}': random.choice(neutral_behavior),
            '{normal}': random.choice(neutral_normal),
            '{happening}': random.choice(neutral_happening),
            '{continues}': random.choice(neutral_continues),
            '{topic}': random.choice(neutral_topic),
            '{interesting}': random.choice(neutral_interesting),
            '{reaction}': random.choice(neutral_reaction)
        }
        
        for key, value in replacements.items():
            text = text.replace(key, value)
        
        data.append({"text": text, "label": "neutral"})
    
    df = pd.DataFrame(data)
    return df.sample(frac=1).reset_index(drop=True)

def preprocess_text(text):
    """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞"""
    if pd.isna(text):
        return ""
    
    text = str(text)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —ç–º–æ–¥–∑–∏ –∏ –∑–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è (–æ–Ω–∏ –≤–∞–∂–Ω—ã –¥–ª—è —ç–º–æ—Ü–∏–π!)
    # –¢–æ–ª—å–∫–æ –±–∞–∑–æ–≤–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
    text = re.sub(r'http\S+|www\S+', '', text)  # –£–¥–∞–ª—è–µ–º URL
    text = re.sub(r'\s+', ' ', text)  # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –ø—Ä–æ–±–µ–ª—ã
    
    # –ù–ï –ø—Ä–∏–≤–æ–¥–∏–º –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É - –∫–∞–ø—Å —Ç–æ–∂–µ –ø–µ—Ä–µ–¥–∞–µ—Ç —ç–º–æ—Ü–∏–∏
    # –ù–ï —É–¥–∞–ª—è–µ–º –∑–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è - –æ–Ω–∏ –≤–∞–∂–Ω—ã
    
    return text.strip()

def create_advanced_model(vocab_size, max_length):
    """–°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–π –º–æ–¥–µ–ª–∏"""
    model = Sequential([
        # –≠–º–±–µ–¥–¥–∏–Ω–≥–∏
        Embedding(vocab_size, EMBEDDING_DIM, input_length=max_length),
        
        # –°–≤–µ—Ä—Ç–æ—á–Ω—ã–π —Å–ª–æ–π –¥–ª—è –ª–æ–∫–∞–ª—å–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
        Conv1D(128, 5, activation='relu'),
        GlobalMaxPooling1D(),
        
        # –ü–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã–µ —Å–ª–æ–∏
        Dense(128, activation='relu'),
        Dropout(0.5),
        Dense(64, activation='relu'),
        Dropout(0.3),
        
        # –í—ã—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π
        Dense(3, activation='softmax')
    ])
    
    # –ö–æ–º–ø–∏–ª—è—Ü–∏—è —Å –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–º–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏
    optimizer = Adam(learning_rate=0.001)
    model.compile(
        optimizer=optimizer,
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy']
    )
    
    return model

def main():
    print("=" * 50)
    print("–ü–†–û–î–í–ò–ù–£–¢–û–ï –û–ë–£–ß–ï–ù–ò–ï –ú–û–î–ï–õ–ò")
    print("=" * 50)
    
    # 1. –°–æ–∑–¥–∞–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞
    print("\n1. –°–æ–∑–¥–∞–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞...")
    df = create_quality_dataset(50000)  # –ë–æ–ª—å—à–æ–π –¥–∞—Ç–∞—Å–µ—Ç
    print(f"   –°–æ–∑–¥–∞–Ω–æ {len(df)} —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤")
    
    # 2. –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞
    print("\n2. –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤...")
    df['processed_text'] = df['text'].apply(preprocess_text)
    
    # 3. –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –º–µ—Ç–æ–∫
    print("\n3. –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –º–µ—Ç–æ–∫...")
    label_encoder = LabelEncoder()
    df['label_encoded'] = label_encoder.fit_transform(df['label'])
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –±–∞–ª–∞–Ω—Å–∞ –∫–ª–∞—Å—Å–æ–≤
    print("\n   –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤:")
    for label in df['label'].unique():
        count = len(df[df['label'] == label])
        print(f"   {label}: {count} ({count/len(df)*100:.1f}%)")
    
    # 4. –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
    print("\n4. –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ –æ–±—É—á–∞—é—â—É—é –∏ —Ç–µ—Å—Ç–æ–≤—É—é –≤—ã–±–æ—Ä–∫–∏...")
    X_train, X_test, y_train, y_test = train_test_split(
        df['processed_text'], 
        df['label_encoded'],
        test_size=0.2,
        random_state=42,
        stratify=df['label_encoded']
    )
    
    print(f"   –û–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞: {len(X_train)}")
    print(f"   –¢–µ—Å—Ç–æ–≤–∞—è –≤—ã–±–æ—Ä–∫–∞: {len(X_test)}")
    
    # 5. –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
    print("\n5. –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è...")
    tokenizer = Tokenizer(num_words=MAX_WORDS, oov_token='<OOV>')
    tokenizer.fit_on_texts(X_train)
    
    X_train_seq = tokenizer.texts_to_sequences(X_train)
    X_test_seq = tokenizer.texts_to_sequences(X_test)
    
    X_train_pad = pad_sequences(X_train_seq, maxlen=MAX_LENGTH, padding='post', truncating='post')
    X_test_pad = pad_sequences(X_test_seq, maxlen=MAX_LENGTH, padding='post', truncating='post')
    
    vocab_size = min(len(tokenizer.word_index) + 1, MAX_WORDS)
    print(f"   –†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è: {vocab_size}")
    print(f"   –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤: {len(tokenizer.word_index)}")
    
    # 6. –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
    print("\n6. –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–π –º–æ–¥–µ–ª–∏...")
    model = create_advanced_model(vocab_size, MAX_LENGTH)
    model.summary()
    
    # 7. –í–µ—Å–∞ –∫–ª–∞—Å—Å–æ–≤ –¥–ª—è –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏
    class_weights = class_weight.compute_class_weight(
        'balanced',
        classes=np.unique(y_train),
        y=y_train
    )
    class_weight_dict = dict(enumerate(class_weights))
    
    # 8. Callbacks
    callbacks = [
        EarlyStopping(
            monitor='val_loss',
            patience=5,
            restore_best_weights=True,
            verbose=1
        ),
        ModelCheckpoint(
            str(MODELS_DIR / 'advanced_model.keras'),
            save_best_only=True,
            monitor='val_accuracy',
            mode='max',
            verbose=1
        ),
        ReduceLROnPlateau(
            monitor='val_loss',
            factor=0.5,
            patience=3,
            min_lr=0.00001,
            verbose=1
        )
    ]
    
    # 9. –û–±—É—á–µ–Ω–∏–µ
    print(f"\n7. –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ ({EPOCHS} —ç–ø–æ—Ö)...")
    history = model.fit(
        X_train_pad, y_train,
        batch_size=BATCH_SIZE,
        epochs=EPOCHS,
        validation_split=0.15,
        callbacks=callbacks,
        class_weight=class_weight_dict,
        verbose=1
    )
    
    # 10. –û—Ü–µ–Ω–∫–∞
    print("\n8. –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏...")
    test_loss, test_accuracy = model.evaluate(X_test_pad, y_test, verbose=0)
    print(f"   –¢–æ—á–Ω–æ—Å—Ç—å –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ: {test_accuracy:.2%}")
    
    # 11. –î–µ—Ç–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞
    from sklearn.metrics import classification_report, confusion_matrix
    y_pred = model.predict(X_test_pad)
    y_pred_classes = np.argmax(y_pred, axis=1)
    
    print("\n   –î–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç:")
    print(classification_report(y_test, y_pred_classes, 
                              target_names=['negative', 'neutral', 'positive']))
    
    # 12. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
    print("\n9. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∏ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤...")
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∫ –æ—Å–Ω–æ–≤–Ω—É—é –º–æ–¥–µ–ª—å
    model.save(MODELS_DIR / 'sentiment_model.keras')
    model.save(MODELS_DIR / 'sentiment_model.h5')
    
    joblib.dump(tokenizer, MODELS_DIR / 'tokenizer.pkl')
    joblib.dump(label_encoder, MODELS_DIR / 'label_encoder.pkl')
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
    config = {
        'max_words': MAX_WORDS,
        'max_length': MAX_LENGTH,
        'vocab_size': vocab_size,
        'test_accuracy': float(test_accuracy),
        'training_samples': len(X_train),
        'model_type': 'advanced_cnn'
    }
    joblib.dump(config, MODELS_DIR / 'model_config.pkl')
    
    # 13. –¢–µ—Å—Ç–æ–≤—ã–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
    print("\n10. –ü—Ä–∏–º–µ—Ä—ã –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π:")
    test_texts = [
        "–û—Ç–ª–∏—á–Ω—ã–π —Ç–æ–≤–∞—Ä, –≤—Å–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥—É—é! –ö–∞—á–µ—Å—Ç–≤–æ —Å—É–ø–µ—Ä!",
        "–£–∂–∞—Å–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ, –ø–æ–ª–Ω–æ–µ —Ä–∞–∑–æ—á–∞—Ä–æ–≤–∞–Ω–∏–µ. –ù–µ –ø–æ–∫—É–ø–∞–π—Ç–µ!",
        "–ù–æ—Ä–º–∞–ª—å–Ω—ã–π —Ç–æ–≤–∞—Ä –∑–∞ —Å–≤–æ–∏ –¥–µ–Ω—å–≥–∏. –ï—Å—Ç—å –ø–ª—é—Å—ã –∏ –º–∏–Ω—É—Å—ã.",
        "–ü—Ä–æ—Å—Ç–æ –≤ –≤–æ—Å—Ç–æ—Ä–≥–µ! –õ—É—á—à–∞—è –ø–æ–∫—É–ø–∫–∞ –≤ –º–æ–µ–π –∂–∏–∑–Ω–∏! üòä",
        "–ö–æ—à–º–∞—Ä! –•—É–∂–µ –Ω–µ –≤–∏–¥–µ–ª! –î–µ–Ω—å–≥–∏ –Ω–∞ –≤–µ—Ç–µ—Ä! üò°",
        "–ü–æ–ª—å–∑—É—é—Å—å –Ω–µ–¥–µ–ª—é. –ü–æ–∫–∞ –≤—Å–µ —Ä–∞–±–æ—Ç–∞–µ—Ç –Ω–æ—Ä–º–∞–ª—å–Ω–æ.",
        "–°–£–ü–ï–†!!! –û–ë–û–ñ–ê–Æ!!! ‚ù§Ô∏è‚ù§Ô∏è‚ù§Ô∏è",
        "–Ω–µ —Å–æ–≤–µ—Ç—É—é –Ω–∏–∫–æ–º—É(( –æ—á–µ–Ω—å —Ä–∞—Å—Å—Ç—Ä–æ–µ–Ω",
        "–ó–∞–∫–∞–∑ –ø—Ä–∏—à–µ–ª –≤–æ–≤—Ä–µ–º—è. –£–ø–∞–∫–æ–≤–∫–∞ —Ü–µ–ª–∞—è. –°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –æ–ø–∏—Å–∞–Ω–∏—é."
    ]
    
    for text in test_texts:
        # –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞
        processed = preprocess_text(text)
        sequence = tokenizer.texts_to_sequences([processed])
        padded = pad_sequences(sequence, maxlen=MAX_LENGTH)
        
        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
        prediction = model.predict(padded, verbose=0)[0]
        predicted_class = np.argmax(prediction)
        label = label_encoder.inverse_transform([predicted_class])[0]
        confidence = prediction[predicted_class]
        
        print(f"\n   –¢–µ–∫—Å—Ç: '{text}'")
        print(f"   ‚Üí {label} (—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {confidence:.2%})")
        print(f"   –î–µ—Ç–∞–ª–∏: neg={prediction[0]:.2%}, neu={prediction[1]:.2%}, pos={prediction[2]:.2%}")
    
    print("\n‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ!")
    print(f"\n–ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤: {MODELS_DIR}")
    print("\n–¢–µ–ø–µ—Ä—å –º–æ–∂–µ—Ç–µ –∑–∞–ø—É—Å—Ç–∏—Ç—å –±–æ—Ç–∞: python src/bot.py")
    print("–ò–ª–∏ —É–ø—Ä–æ—â–µ–Ω–Ω–æ–≥–æ –±–æ—Ç–∞: python bot_simple.py")

if __name__ == "__main__":
    main()