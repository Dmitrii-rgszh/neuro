"""
–ü–û–õ–ù–û–¶–ï–ù–ù–û–ï –û–ë–£–ß–ï–ù–ò–ï –ú–û–î–ï–õ–ò SENTIMENT –ê–ù–ê–õ–ò–ó–ê –ù–ê GPU
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç PyTorch –∏ RTX 3060 Ti –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
"""
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, TensorDataset, Dataset
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import time
import os
import sys
import joblib
from tqdm import tqdm
import json
import random
from collections import Counter

# –ü—Ä–æ–≤–µ—Ä–∫–∞ GPU
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
if device.type == 'cuda':
    print(f"üéâ –ò—Å–ø–æ–ª—å–∑—É–µ–º GPU: {torch.cuda.get_device_name(0)}")
    print(f"üíæ –ü–∞–º—è—Ç—å GPU: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
else:
    print("‚ùå GPU –Ω–µ –Ω–∞–π–¥–µ–Ω! –ò—Å–ø–æ–ª—å–∑—É–µ–º CPU (–±—É–¥–µ—Ç –º–µ–¥–ª–µ–Ω–Ω–æ)")
    sys.exit(1)

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø—É—Ç–µ–π
BASE_DIR = Path(__file__).parent
DATA_DIR = BASE_DIR / "data"
MODELS_DIR = BASE_DIR / "models"
PROCESSED_DIR = DATA_DIR / "processed"
RAW_DIR = DATA_DIR / "raw"

# –°–æ–∑–¥–∞–Ω–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π
for dir_path in [DATA_DIR, MODELS_DIR, PROCESSED_DIR, RAW_DIR]:
    dir_path.mkdir(parents=True, exist_ok=True)

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –º–æ–¥–µ–ª–∏
CONFIG = {
    "max_words": 50000,
    "max_length": 256,
    "embedding_dim": 300,
    "hidden_dim": 256,
    "num_classes": 3,
    "batch_size": 64,  # –£–≤–µ–ª–∏—á–µ–Ω –¥–ª—è GPU
    "epochs": 150,     # –ú–Ω–æ–≥–æ —ç–ø–æ—Ö –¥–ª—è –≤—ã—Å–æ–∫–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏
    "learning_rate": 0.001,
    "weight_decay": 0.0001,
    "dropout": 0.4,
    "patience": 15,
    "min_delta": 0.001
}

class RussianTokenizer:
    """–ü—Ä–æ—Å—Ç–æ–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞"""
    
    def __init__(self, num_words=50000):
        self.num_words = num_words
        self.word_index = {}
        self.index_word = {}
        self.word_counts = Counter()
        
    def fit_on_texts(self, texts):
        """–û–±—É—á–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –Ω–∞ —Ç–µ–∫—Å—Ç–∞—Ö"""
        for text in tqdm(texts, desc="–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ —Å–ª–æ–≤–∞—Ä—è"):
            words = text.lower().split()
            self.word_counts.update(words)
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–æ–≤ –¥–ª—è —Ç–æ–ø —Å–ª–æ–≤
        most_common = self.word_counts.most_common(self.num_words - 2)
        
        self.word_index = {"<PAD>": 0, "<UNK>": 1}
        self.index_word = {0: "<PAD>", 1: "<UNK>"}
        
        for idx, (word, count) in enumerate(most_common, start=2):
            self.word_index[word] = idx
            self.index_word[idx] = word
            
        print(f"–†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è: {len(self.word_index)}")
    
    def texts_to_sequences(self, texts):
        """–ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–æ–≤ –≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏–Ω–¥–µ–∫—Å–æ–≤"""
        sequences = []
        for text in texts:
            words = text.lower().split()
            sequence = [self.word_index.get(word, 1) for word in words]  # 1 = <UNK>
            sequences.append(sequence)
        return sequences

class SentimentDataset(Dataset):
    """PyTorch Dataset –¥–ª—è sentiment –∞–Ω–∞–ª–∏–∑–∞"""
    
    def __init__(self, sequences, labels, max_length):
        self.sequences = sequences
        self.labels = labels
        self.max_length = max_length
        
    def __len__(self):
        return len(self.sequences)
    
    def __getitem__(self, idx):
        # Padding/truncating
        seq = self.sequences[idx]
        if len(seq) > self.max_length:
            seq = seq[:self.max_length]
        else:
            seq = seq + [0] * (self.max_length - len(seq))
        
        return torch.LongTensor(seq), torch.LongTensor([self.labels[idx]])

class SentimentModelGPU(nn.Module):
    """–£–ª—É—á—à–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è GPU —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π —Ç–æ—á–Ω–æ—Å—Ç—å—é"""
    
    def __init__(self, vocab_size, config):
        super().__init__()
        
        # Embedding —Å–ª–æ–π
        self.embedding = nn.Embedding(vocab_size, config["embedding_dim"], padding_idx=0)
        self.embedding_dropout = nn.Dropout(0.2)
        
        # Bidirectional LSTM —Å–ª–æ–∏
        self.lstm1 = nn.LSTM(
            config["embedding_dim"], 
            config["hidden_dim"],
            batch_first=True,
            bidirectional=True,
            dropout=0.3 if config["epochs"] > 1 else 0
        )
        
        self.lstm2 = nn.LSTM(
            config["hidden_dim"] * 2,  # bidirectional
            config["hidden_dim"],
            batch_first=True,
            bidirectional=True,
            dropout=0.3 if config["epochs"] > 1 else 0
        )
        
        # Attention –º–µ—Ö–∞–Ω–∏–∑–º
        self.attention = nn.MultiheadAttention(
            config["hidden_dim"] * 2,
            num_heads=8,
            batch_first=True,
            dropout=config["dropout"]
        )
        
        # CNN –¥–ª—è –ª–æ–∫–∞–ª—å–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
        self.conv1 = nn.Conv1d(config["hidden_dim"] * 2, 256, kernel_size=3, padding=1)
        self.conv2 = nn.Conv1d(256, 128, kernel_size=3, padding=1)
        self.conv3 = nn.Conv1d(128, 64, kernel_size=3, padding=1)
        
        # –ü–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã–µ —Å–ª–æ–∏
        self.fc1 = nn.Linear(config["hidden_dim"] * 2 + 64, 512)
        self.fc2 = nn.Linear(512, 256)
        self.fc3 = nn.Linear(256, 128)
        self.fc4 = nn.Linear(128, config["num_classes"])
        
        self.dropout = nn.Dropout(config["dropout"])
        self.layer_norm1 = nn.LayerNorm(config["hidden_dim"] * 2)
        self.layer_norm2 = nn.LayerNorm(512)
        
    def forward(self, x):
        # Embedding
        embedded = self.embedding(x)
        embedded = self.embedding_dropout(embedded)
        
        # LSTM —Å–ª–æ–∏
        lstm_out1, _ = self.lstm1(embedded)
        lstm_out2, _ = self.lstm2(lstm_out1)
        lstm_out = self.layer_norm1(lstm_out2)
        
        # Self-attention
        attn_out, _ = self.attention(lstm_out, lstm_out, lstm_out)
        
        # CNN branch
        cnn_input = lstm_out.transpose(1, 2)  # (batch, features, seq_len)
        cnn_out = F.relu(self.conv1(cnn_input))
        cnn_out = F.relu(self.conv2(cnn_out))
        cnn_out = F.relu(self.conv3(cnn_out))
        cnn_features = F.max_pool1d(cnn_out, kernel_size=cnn_out.size(2)).squeeze(2)
        
        # Global pooling –¥–ª—è LSTM+Attention
        avg_pool = torch.mean(attn_out, dim=1)
        max_pool = torch.max(attn_out, dim=1)[0]
        
        # –ö–æ–Ω–∫–∞—Ç–µ–Ω–∞—Ü–∏—è –≤—Å–µ—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        combined = torch.cat([avg_pool, max_pool, cnn_features], dim=1)
        
        # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è
        x = F.relu(self.fc1(combined))
        x = self.layer_norm2(x)
        x = self.dropout(x)
        x = F.relu(self.fc2(x))
        x = self.dropout(x)
        x = F.relu(self.fc3(x))
        x = self.dropout(x)
        output = self.fc4(x)
        
        return output

def create_quality_dataset(num_samples=100000):
    """–°–æ–∑–¥–∞–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"""
    print(f"\nüìä –°–æ–∑–¥–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞ –∏–∑ {num_samples} –ø—Ä–∏–º–µ—Ä–æ–≤...")
    
    data = []
    
    # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ —à–∞–±–ª–æ–Ω—ã –¥–ª—è –ø–æ–∑–∏—Ç–∏–≤–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤
    positive_templates = [
        "–≠—Ç–æ –ø—Ä–æ—Å—Ç–æ {amazing}! –Ø {emotion}!",
        "–û—Ç–ª–∏—á–Ω—ã–π {product}, –≤—Å–µ–º {recommend}!",
        "{quality} –∫–∞—á–µ—Å—Ç–≤–æ, {service} —Å–µ—Ä–≤–∏—Å!",
        "–û—á–µ–Ω—å {satisfied} –ø–æ–∫—É–ø–∫–æ–π, {worth} —Å–≤–æ–∏—Ö –¥–µ–Ω–µ–≥!",
        "–ù–∞–∫–æ–Ω–µ—Ü-—Ç–æ –Ω–∞—à–µ–ª —Ç–æ —á—Ç–æ –∏—Å–∫–∞–ª! {emotion}!",
        "–ü—Ä–µ–≤–∑–æ—à–ª–æ –≤—Å–µ –æ–∂–∏–¥–∞–Ω–∏—è! {amazing}!",
        "–õ—É—á—à–∏–π {product} –∫–æ—Ç–æ—Ä—ã–π —è {action}!",
        "–†–µ–∫–æ–º–µ–Ω–¥—É—é –≤—Å–µ–º! {quality} —Ç–æ–≤–∞—Ä!",
        "–°–ø–∞—Å–∏–±–æ –∑–∞ {service} –æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏–µ!",
        "{emotion}! –ë—É–¥—É –∑–∞–∫–∞–∑—ã–≤–∞—Ç—å –µ—â–µ!",
    ]
    
    positive_words = {
        "amazing": ["–ø–æ—Ç—Ä—è—Å–∞—é—â–µ", "–≤–µ–ª–∏–∫–æ–ª–µ–ø–Ω–æ", "–≤–æ—Å—Ö–∏—Ç–∏—Ç–µ–ª—å–Ω–æ", "–ø—Ä–µ–∫—Ä–∞—Å–Ω–æ", "—á—É–¥–µ—Å–Ω–æ", "–æ—Ñ–∏–≥–µ–Ω–Ω–æ", "—Å—É–ø–µ—Ä", "–∫—Ä—É—Ç–æ"],
        "emotion": ["–≤ –≤–æ—Å—Ç–æ—Ä–≥–µ", "—Å—á–∞—Å—Ç–ª–∏–≤", "–¥–æ–≤–æ–ª–µ–Ω", "—Ä–∞–¥", "–≤–æ—Å—Ö–∏—â–µ–Ω", "–≤–ø–µ—á–∞—Ç–ª–µ–Ω", "–≤–æ–æ–¥—É—à–µ–≤–ª–µ–Ω"],
        "product": ["—Ç–æ–≤–∞—Ä", "–ø—Ä–æ–¥—É–∫—Ç", "—Å–µ—Ä–≤–∏—Å", "–º–∞–≥–∞–∑–∏–Ω", "–≤—ã–±–æ—Ä", "–∞—Å—Å–æ—Ä—Ç–∏–º–µ–Ω—Ç", "–∫–∞—á–µ—Å—Ç–≤–æ"],
        "recommend": ["—Ä–µ–∫–æ–º–µ–Ω–¥—É—é", "—Å–æ–≤–µ—Ç—É—é", "–±—É–¥—É —Å–æ–≤–µ—Ç–æ–≤–∞—Ç—å", "–ø–æ—Ä–µ–∫–æ–º–µ–Ω–¥—É—é –¥—Ä—É–∑—å—è–º"],
        "quality": ["–æ—Ç–ª–∏—á–Ω–æ–µ", "–ø—Ä–µ–≤–æ—Å—Ö–æ–¥–Ω–æ–µ", "–≤—ã—Å–æ–∫–æ–µ", "–ø—Ä–µ–º–∏–∞–ª—å–Ω–æ–µ", "–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–µ", "–Ω–∞–¥–µ–∂–Ω–æ–µ"],
        "service": ["–±—ã—Å—Ç—Ä—ã–π", "–≤–µ–∂–ª–∏–≤—ã–π", "–ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π", "–æ–ø–µ—Ä–∞—Ç–∏–≤–Ω—ã–π", "–≤–Ω–∏–º–∞—Ç–µ–ª—å–Ω—ã–π"],
        "satisfied": ["–¥–æ–≤–æ–ª–µ–Ω", "—É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä–µ–Ω", "—Ä–∞–¥", "—Å—á–∞—Å—Ç–ª–∏–≤", "–≤–ø–µ—á–∞—Ç–ª–µ–Ω"],
        "worth": ["—Å—Ç–æ–∏—Ç", "–æ–ø—Ä–∞–≤–¥—ã–≤–∞–µ—Ç", "—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç", "–ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç"],
        "action": ["–ø–æ–∫—É–ø–∞–ª", "–∑–∞–∫–∞–∑—ã–≤–∞–ª", "–≤–∏–¥–µ–ª", "–ø—Ä–æ–±–æ–≤–∞–ª", "–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª", "—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–ª"]
    }
    
    # –ù–µ–≥–∞—Ç–∏–≤–Ω—ã–µ —à–∞–±–ª–æ–Ω—ã
    negative_templates = [
        "–£–∂–∞—Å–Ω—ã–π {product}! {emotion}!",
        "–ü–æ–ª–Ω–æ–µ {disappointment}, –Ω–µ {recommend}!",
        "{quality} –∫–∞—á–µ—Å—Ç–≤–æ, {waste} –¥–µ–Ω–µ–≥!",
        "–û—á–µ–Ω—å {dissatisfied} –ø–æ–∫—É–ø–∫–æ–π!",
        "–•—É–¥—à–∏–π {product} –∫–æ—Ç–æ—Ä—ã–π —è {action}!",
        "–û–±–º–∞–Ω –∏ {disappointment}! {emotion}!",
        "–ù–µ –ø–æ–∫—É–ø–∞–π—Ç–µ —ç—Ç–æ! {quality}!",
        "{service} —Å–µ—Ä–≤–∏—Å, {emotion}!",
        "–†–∞–∑–æ—á–∞—Ä–æ–≤–∞–Ω –ø–æ–ª–Ω–æ—Å—Ç—å—é! {waste}!",
        "–ö–æ—à–º–∞—Ä! –ù–∏–∫–æ–º—É –Ω–µ {recommend}!",
    ]
    
    negative_words = {
        "product": ["—Ç–æ–≤–∞—Ä", "–ø—Ä–æ–¥—É–∫—Ç", "—Å–µ—Ä–≤–∏—Å", "–º–∞–≥–∞–∑–∏–Ω", "–æ–ø—ã—Ç", "–≤—ã–±–æ—Ä"],
        "emotion": ["—Ä–∞–∑–æ—á–∞—Ä–æ–≤–∞–Ω", "–∑–æ–ª", "—Ä–∞–∑–¥—Ä–∞–∂–µ–Ω", "—Ä–∞—Å—Å—Ç—Ä–æ–µ–Ω", "–≤–æ–∑–º—É—â–µ–Ω", "–Ω–µ–¥–æ–≤–æ–ª–µ–Ω", "–æ–≥–æ—Ä—á–µ–Ω"],
        "disappointment": ["—Ä–∞–∑–æ—á–∞—Ä–æ–≤–∞–Ω–∏–µ", "–∫–æ—à–º–∞—Ä", "—É–∂–∞—Å", "–ø—Ä–æ–≤–∞–ª", "—Ñ–∏–∞—Å–∫–æ", "–∫–∞—Ç–∞—Å—Ç—Ä–æ—Ñ–∞"],
        "recommend": ["—Ä–µ–∫–æ–º–µ–Ω–¥—É—é", "—Å–æ–≤–µ—Ç—É—é", "–ø–æ–∫—É–ø–∞–π—Ç–µ", "–±–µ—Ä–∏—Ç–µ", "—Å–≤—è–∑—ã–≤–∞–π—Ç–µ—Å—å"],
        "quality": ["—É–∂–∞—Å–Ω–æ–µ", "–æ—Ç–≤—Ä–∞—Ç–∏—Ç–µ–ª—å–Ω–æ–µ", "–ø–ª–æ—Ö–æ–µ", "–Ω–∏–∑–∫–æ–µ", "–Ω–∏–∫—É–¥—ã—à–Ω–æ–µ", "–∫–æ—à–º–∞—Ä–Ω–æ–µ"],
        "waste": ["–ø–æ—Ç–µ—Ä—è", "–≤—ã–±—Ä–æ—à–µ–Ω–Ω—ã–µ", "–∑—Ä—è –ø–æ—Ç—Ä–∞—á–µ–Ω–Ω—ã–µ", "–ø—É—Å—Ç–∞—è —Ç—Ä–∞—Ç–∞"],
        "dissatisfied": ["–Ω–µ–¥–æ–≤–æ–ª–µ–Ω", "—Ä–∞–∑–æ—á–∞—Ä–æ–≤–∞–Ω", "—Ä–∞—Å—Å—Ç—Ä–æ–µ–Ω", "–æ–≥–æ—Ä—á–µ–Ω", "—Ä–∞–∑–¥—Ä–∞–∂–µ–Ω"],
        "service": ["—É–∂–∞—Å–Ω—ã–π", "–º–µ–¥–ª–µ–Ω–Ω—ã–π", "–≥—Ä—É–±—ã–π", "–Ω–µ–∫–æ–º–ø–µ—Ç–µ–Ω—Ç–Ω—ã–π", "–æ—Ç–≤—Ä–∞—Ç–∏—Ç–µ–ª—å–Ω—ã–π"],
        "action": ["–ø–æ–∫—É–ø–∞–ª", "–∑–∞–∫–∞–∑—ã–≤–∞–ª", "–≤–∏–¥–µ–ª", "–ø—Ä–æ–±–æ–≤–∞–ª", "–±—Ä–∞–ª", "–ø–æ–ª—É—á–∏–ª"]
    }
    
    # –ù–µ–π—Ç—Ä–∞–ª—å–Ω—ã–µ —à–∞–±–ª–æ–Ω—ã
    neutral_templates = [
        "{product} –æ–±—ã—á–Ω—ã–π, –Ω–∏—á–µ–≥–æ –æ—Å–æ–±–µ–Ω–Ω–æ–≥–æ",
        "–ù–æ—Ä–º–∞–ª—å–Ω—ã–π {product}, –µ—Å—Ç—å –ø–ª—é—Å—ã –∏ –º–∏–Ω—É—Å—ã",
        "–°—Ä–µ–¥–Ω–∏–π {product}, {price} —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –∫–∞—á–µ—Å—Ç–≤—É",
        "–ü–æ–π–¥–µ—Ç –¥–ª—è —Å–≤–æ–∏—Ö –∑–∞–¥–∞—á",
        "–û–±—ã—á–Ω—ã–π {product}, –∫–∞–∫ –∏ –æ–∂–∏–¥–∞–ª–æ—Å—å",
        "–ù–∏—á–µ–≥–æ –≤—ã–¥–∞—é—â–µ–≥–æ—Å—è, –Ω–æ –∏ –ø–ª–æ—Ö–æ–≥–æ –Ω–µ —Å–∫–∞–∂—É",
        "–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π {product}, –±–µ–∑ –∏–∑–ª–∏—à–µ—Å—Ç–≤",
        "–°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –æ–ø–∏—Å–∞–Ω–∏—é",
        "–ù–æ—Ä–º–∞–ª—å–Ω–æ, –Ω–æ –µ—Å—Ç—å {alternative}",
        "–ü—Ä–∏–µ–º–ª–µ–º—ã–π –≤–∞—Ä–∏–∞–Ω—Ç –∑–∞ —Å–≤–æ–∏ –¥–µ–Ω—å–≥–∏",
    ]
    
    neutral_words = {
        "product": ["—Ç–æ–≤–∞—Ä", "–ø—Ä–æ–¥—É–∫—Ç", "–≤–∞—Ä–∏–∞–Ω—Ç", "–≤—ã–±–æ—Ä", "—ç–∫–∑–µ–º–ø–ª—è—Ä"],
        "price": ["—Ü–µ–Ω–∞", "—Å—Ç–æ–∏–º–æ—Å—Ç—å", "—Ü–µ–Ω–Ω–∏–∫", "–ø—Ä–∞–π—Å"],
        "alternative": ["–∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤—ã", "–≤–∞—Ä–∏–∞–Ω—Ç—ã –ª—É—á—à–µ", "–¥—Ä—É–≥–∏–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã", "–∞–Ω–∞–ª–æ–≥–∏"]
    }
    
    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
    samples_per_class = num_samples // 3
    
    # –ü–æ–∑–∏—Ç–∏–≤–Ω—ã–µ
    for _ in range(samples_per_class):
        template = random.choice(positive_templates)
        text = template
        for placeholder, words in positive_words.items():
            if f"{{{placeholder}}}" in text:
                text = text.replace(f"{{{placeholder}}}", random.choice(words))
        data.append({"text": text, "label": "positive"})
    
    # –ù–µ–≥–∞—Ç–∏–≤–Ω—ã–µ
    for _ in range(samples_per_class):
        template = random.choice(negative_templates)
        text = template
        for placeholder, words in negative_words.items():
            if f"{{{placeholder}}}" in text:
                text = text.replace(f"{{{placeholder}}}", random.choice(words))
        data.append({"text": text, "label": "negative"})
    
    # –ù–µ–π—Ç—Ä–∞–ª—å–Ω—ã–µ
    for _ in range(num_samples - 2 * samples_per_class):
        template = random.choice(neutral_templates)
        text = template
        for placeholder, words in neutral_words.items():
            if f"{{{placeholder}}}" in text:
                text = text.replace(f"{{{placeholder}}}", random.choice(words))
        data.append({"text": text, "label": "neutral"})
    
    # –ü–µ—Ä–µ–º–µ—à–∏–≤–∞–Ω–∏–µ
    random.shuffle(data)
    df = pd.DataFrame(data)
    
    print(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ {len(df)} –ø—Ä–∏–º–µ—Ä–æ–≤")
    print(f"   –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ: {df['label'].value_counts().to_dict()}")
    
    return df

def train_model_gpu():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è –Ω–∞ GPU"""
    print("="*70)
    print("üöÄ –û–ë–£–ß–ï–ù–ò–ï –ú–û–î–ï–õ–ò SENTIMENT –ê–ù–ê–õ–ò–ó–ê –ù–ê GPU")
    print("="*70)
    
    # 1. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    print("\n1Ô∏è‚É£ –ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–ù–ù–´–•")
    print("-"*50)
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö
    train_path = PROCESSED_DIR / "train_data.csv"
    test_path = PROCESSED_DIR / "test_data.csv"
    
    if train_path.exists() and test_path.exists():
        print("üìÇ –ó–∞–≥—Ä—É–∑–∫–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö...")
        train_df = pd.read_csv(train_path)
        test_df = pd.read_csv(test_path)
        
        # –ï—Å–ª–∏ –Ω–µ—Ç –Ω—É–∂–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫, —Å–æ–∑–¥–∞–µ–º –∑–∞–Ω–æ–≤–æ
        if 'text' not in train_df.columns:
            print("‚ö†Ô∏è  –î–∞–Ω–Ω—ã–µ –ø–æ–≤—Ä–µ–∂–¥–µ–Ω—ã, —Å–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–µ...")
            df = create_quality_dataset(100000)
        else:
            df = pd.concat([train_df, test_df], ignore_index=True)
    else:
        print("üìä –°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞...")
        df = create_quality_dataset(100000)
    
    # Label encoding
    label_encoder = LabelEncoder()
    df['label_encoded'] = label_encoder.fit_transform(df['label'])
    
    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
    X_train, X_test, y_train, y_test = train_test_split(
        df['text'].values, 
        df['label_encoded'].values,
        test_size=0.2,
        random_state=42,
        stratify=df['label_encoded']
    )
    
    print(f"\nüìä –†–∞–∑–º–µ—Ä—ã –¥–∞–Ω–Ω—ã—Ö:")
    print(f"   –û–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞: {len(X_train)}")
    print(f"   –¢–µ—Å—Ç–æ–≤–∞—è –≤—ã–±–æ—Ä–∫–∞: {len(X_test)}")
    
    # 2. –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
    print("\n2Ô∏è‚É£ –¢–û–ö–ï–ù–ò–ó–ê–¶–ò–Ø")
    print("-"*50)
    
    tokenizer = RussianTokenizer(num_words=CONFIG["max_words"])
    tokenizer.fit_on_texts(X_train)
    
    # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    X_train_seq = tokenizer.texts_to_sequences(X_train)
    X_test_seq = tokenizer.texts_to_sequences(X_test)
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤
    train_dataset = SentimentDataset(X_train_seq, y_train, CONFIG["max_length"])
    test_dataset = SentimentDataset(X_test_seq, y_test, CONFIG["max_length"])
    
    # DataLoaders
    train_loader = DataLoader(
        train_dataset, 
        batch_size=CONFIG["batch_size"], 
        shuffle=True,
        num_workers=0,  # –î–ª—è Windows
        pin_memory=True  # –î–ª—è GPU
    )
    
    test_loader = DataLoader(
        test_dataset, 
        batch_size=CONFIG["batch_size"], 
        shuffle=False,
        num_workers=0,
        pin_memory=True
    )
    
    # 3. –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
    print("\n3Ô∏è‚É£ –°–û–ó–î–ê–ù–ò–ï –ú–û–î–ï–õ–ò")
    print("-"*50)
    
    vocab_size = len(tokenizer.word_index) + 1
    model = SentimentModelGPU(vocab_size, CONFIG).to(device)
    
    # –ü–æ–¥—Å—á–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"üìä –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏:")
    print(f"   –í—Å–µ–≥–æ: {total_params:,}")
    print(f"   –û–±—É—á–∞–µ–º—ã—Ö: {trainable_params:,}")
    
    # 4. –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ–±—É—á–µ–Ω–∏—è
    print("\n4Ô∏è‚É£ –ù–ê–°–¢–†–û–ô–ö–ê –û–ë–£–ß–ï–ù–ò–Ø")
    print("-"*50)
    
    criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.AdamW(
        model.parameters(), 
        lr=CONFIG["learning_rate"],
        weight_decay=CONFIG["weight_decay"]
    )
    
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, 
        mode='min',
        factor=0.5,
        patience=5)
    
    # 5. –û–±—É—á–µ–Ω–∏–µ
    print("\n5Ô∏è‚É£ –û–ë–£–ß–ï–ù–ò–ï –ú–û–î–ï–õ–ò")
    print("-"*50)
    print(f"‚ö° –ò—Å–ø–æ–ª—å–∑—É–µ–º GPU: {torch.cuda.get_device_name(0)}")
    print(f"üìä –ë–∞—Ç—á–∏: {len(train_loader)}")
    print(f"üîÑ –≠–ø–æ—Ö: {CONFIG['epochs']}")
    print("-"*50)
    
    best_accuracy = 0
    patience_counter = 0
    train_losses = []
    val_accuracies = []
    
    start_time = time.time()
    
    for epoch in range(CONFIG["epochs"]):
        # Training
        model.train()
        train_loss = 0
        train_correct = 0
        train_total = 0
        
        progress_bar = tqdm(train_loader, desc=f"–≠–ø–æ—Ö–∞ {epoch+1}/{CONFIG['epochs']}")
        
        for batch_x, batch_y in progress_bar:
            batch_x = batch_x.to(device)
            batch_y = batch_y.squeeze(1).to(device)
            
            optimizer.zero_grad()
            outputs = model(batch_x)
            loss = criterion(outputs, batch_y)
            loss.backward()
            
            # Gradient clipping
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            
            optimizer.step()
            
            train_loss += loss.item()
            _, predicted = torch.max(outputs.data, 1)
            train_total += batch_y.size(0)
            train_correct += (predicted == batch_y).sum().item()
            
            # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–∞
            progress_bar.set_postfix({
                'loss': f'{loss.item():.4f}',
                'acc': f'{100.*train_correct/train_total:.2f}%'
            })
        
        # Validation
        model.eval()
        val_loss = 0
        val_correct = 0
        val_total = 0
        
        with torch.no_grad():
            for batch_x, batch_y in test_loader:
                batch_x = batch_x.to(device)
                batch_y = batch_y.squeeze(1).to(device)
                
                outputs = model(batch_x)
                loss = criterion(outputs, batch_y)
                
                val_loss += loss.item()
                _, predicted = torch.max(outputs.data, 1)
                val_total += batch_y.size(0)
                val_correct += (predicted == batch_y).sum().item()
        
        # –ú–µ—Ç—Ä–∏–∫–∏ —ç–ø–æ—Ö–∏
        avg_train_loss = train_loss / len(train_loader)
        train_accuracy = 100. * train_correct / train_total
        avg_val_loss = val_loss / len(test_loader)
        val_accuracy = 100. * val_correct / val_total
        
        train_losses.append(avg_train_loss)
        val_accuracies.append(val_accuracy)
        
        print(f"\nüìä –≠–ø–æ—Ö–∞ {epoch+1}:")
        print(f"   Train Loss: {avg_train_loss:.4f}, Accuracy: {train_accuracy:.2f}%")
        print(f"   Val Loss: {avg_val_loss:.4f}, Accuracy: {val_accuracy:.2f}%")
        
        # Learning rate scheduling
        scheduler.step(avg_val_loss)
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏
        if val_accuracy > best_accuracy:
            best_accuracy = val_accuracy
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'accuracy': val_accuracy,
                'config': CONFIG
            }, MODELS_DIR / 'best_sentiment_model_gpu.pth')
            print(f"   üíæ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ (—Ç–æ—á–Ω–æ—Å—Ç—å: {val_accuracy:.2f}%)")
            patience_counter = 0
        else:
            patience_counter += 1
            
        # Early stopping
        if patience_counter >= CONFIG["patience"]:
            print(f"\n‚èπÔ∏è  Early stopping –Ω–∞ —ç–ø–æ—Ö–µ {epoch+1}")
            break
    
    total_time = time.time() - start_time
    print(f"\n‚è±Ô∏è  –û–±—â–µ–µ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è: {total_time/60:.1f} –º–∏–Ω—É—Ç")
    print(f"üèÜ –õ—É—á—à–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å: {best_accuracy:.2f}%")
    
    # 6. –§–∏–Ω–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞
    print("\n6Ô∏è‚É£ –§–ò–ù–ê–õ–¨–ù–ê–Ø –û–¶–ï–ù–ö–ê")
    print("-"*50)
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏
    checkpoint = torch.load(MODELS_DIR / 'best_sentiment_model_gpu.pth')
    model.load_state_dict(checkpoint['model_state_dict'])
    model.eval()
    
    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ
    all_predictions = []
    all_labels = []
    
    with torch.no_grad():
        for batch_x, batch_y in test_loader:
            batch_x = batch_x.to(device)
            batch_y = batch_y.squeeze(1)
            
            outputs = model(batch_x)
            _, predicted = torch.max(outputs.data, 1)
            
            all_predictions.extend(predicted.cpu().numpy())
            all_labels.extend(batch_y.numpy())
    
    # Classification report
    print("\nüìä –î–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç:")
    print(classification_report(
        all_labels, 
        all_predictions, 
        target_names=['–ù–µ–≥–∞—Ç–∏–≤–Ω—ã–π', '–ù–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π', '–ü–æ–∑–∏—Ç–∏–≤–Ω—ã–π'],
        digits=4
    ))
    
    # Confusion matrix
    cm = confusion_matrix(all_labels, all_predictions)
    plt.figure(figsize=(10, 8))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=['–ù–µ–≥–∞—Ç–∏–≤–Ω—ã–π', '–ù–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π', '–ü–æ–∑–∏—Ç–∏–≤–Ω—ã–π'],
                yticklabels=['–ù–µ–≥–∞—Ç–∏–≤–Ω—ã–π', '–ù–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π', '–ü–æ–∑–∏—Ç–∏–≤–Ω—ã–π'])
    plt.title('–ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫')
    plt.ylabel('–ò—Å—Ç–∏–Ω–Ω—ã–µ –º–µ—Ç–∫–∏')
    plt.xlabel('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –º–µ—Ç–∫–∏')
    plt.savefig(MODELS_DIR / 'confusion_matrix_gpu.png')
    plt.close()
    
    # –ì—Ä–∞—Ñ–∏–∫ –æ–±—É—á–µ–Ω–∏—è
    plt.figure(figsize=(12, 5))
    
    plt.subplot(1, 2, 1)
    plt.plot(train_losses)
    plt.title('–ü–æ—Ç–µ—Ä–∏ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏')
    plt.xlabel('–≠–ø–æ—Ö–∞')
    plt.ylabel('Loss')
    
    plt.subplot(1, 2, 2)
    plt.plot(val_accuracies)
    plt.title('–¢–æ—á–Ω–æ—Å—Ç—å –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏')
    plt.xlabel('–≠–ø–æ—Ö–∞')
    plt.ylabel('Accuracy (%)')
    
    plt.tight_layout()
    plt.savefig(MODELS_DIR / 'training_history_gpu.png')
    plt.close()
    
    # 7. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤
    print("\n7Ô∏è‚É£ –°–û–•–†–ê–ù–ï–ù–ò–ï –ê–†–¢–ï–§–ê–ö–¢–û–í")
    print("-"*50)
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞
    joblib.dump(tokenizer, MODELS_DIR / 'tokenizer_gpu.pkl')
    print("‚úÖ –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä —Å–æ—Ö—Ä–∞–Ω–µ–Ω")
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ label encoder
    joblib.dump(label_encoder, MODELS_DIR / 'label_encoder_gpu.pkl')
    print("‚úÖ Label encoder —Å–æ—Ö—Ä–∞–Ω–µ–Ω")
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
    with open(MODELS_DIR / 'config_gpu.json', 'w') as f:
        json.dump(CONFIG, f, indent=2)
    print("‚úÖ –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞")
    
    # 8. –¢–µ—Å—Ç–æ–≤—ã–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
    print("\n8Ô∏è‚É£ –¢–ï–°–¢–û–í–´–ï –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–Ø")
    print("-"*50)
    
    test_texts = [
        "–û—Ç–ª–∏—á–Ω—ã–π —Ç–æ–≤–∞—Ä, –≤—Å–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥—É—é!",
        "–£–∂–∞—Å–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ, –ø–æ–ª–Ω–æ–µ —Ä–∞–∑–æ—á–∞—Ä–æ–≤–∞–Ω–∏–µ",
        "–ù–æ—Ä–º–∞–ª—å–Ω—ã–π –ø—Ä–æ–¥—É–∫—Ç, –Ω–∏—á–µ–≥–æ –æ—Å–æ–±–µ–Ω–Ω–æ–≥–æ",
        "–°—É–ø–µ—Ä! –û—á–µ–Ω—å –¥–æ–≤–æ–ª–µ–Ω –ø–æ–∫—É–ø–∫–æ–π!",
        "–ö–æ—à–º–∞—Ä! –•—É–¥—à–µ–µ —á—Ç–æ —è –≤–∏–¥–µ–ª",
        "–û–±—ã—á–Ω—ã–π —Ç–æ–≤–∞—Ä, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –æ–ø–∏—Å–∞–Ω–∏—é",
        "–í–æ—Å—Ö–∏—Ç–∏—Ç–µ–ª—å–Ω–æ! –ü—Ä–µ–≤–∑–æ—à–ª–æ –≤—Å–µ –æ–∂–∏–¥–∞–Ω–∏—è!",
        "–ù–µ —Ä–µ–∫–æ–º–µ–Ω–¥—É—é, –º–Ω–æ–≥–æ –Ω–µ–¥–æ—Å—Ç–∞—Ç–∫–æ–≤",
        "–°—Ä–µ–¥–Ω–µ–Ω—å–∫–æ, –µ—Å—Ç—å –ø–ª—é—Å—ã –∏ –º–∏–Ω—É—Å—ã"
    ]
    
    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
    sequences = tokenizer.texts_to_sequences(test_texts)
    padded = []
    for seq in sequences:
        if len(seq) > CONFIG["max_length"]:
            seq = seq[:CONFIG["max_length"]]
        else:
            seq = seq + [0] * (CONFIG["max_length"] - len(seq))
        padded.append(seq)
    
    X_test_tensor = torch.LongTensor(padded).to(device)
    
    with torch.no_grad():
        outputs = model(X_test_tensor)
        probabilities = F.softmax(outputs, dim=1)
        _, predictions = torch.max(outputs, 1)
    
    print("\n–ü—Ä–∏–º–µ—Ä—ã –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π:")
    print("-"*80)
    
    for text, pred, probs in zip(test_texts, predictions, probabilities):
        label = label_encoder.inverse_transform([pred.cpu().numpy()])[0]
        label_ru = {'negative': '–ù–µ–≥–∞—Ç–∏–≤–Ω—ã–π', 'neutral': '–ù–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π', 'positive': '–ü–æ–∑–∏—Ç–∏–≤–Ω—ã–π'}[label]
        confidence = probs[pred].item()
        
        print(f"–¢–µ–∫—Å—Ç: '{text}'")
        print(f"‚Üí {label_ru} (—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {confidence:.2%})")
        print(f"  –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏: –ù–µ–≥–∞—Ç–∏–≤={probs[0]:.3f}, –ù–µ–π—Ç—Ä–∞–ª={probs[1]:.3f}, –ü–æ–∑–∏—Ç–∏–≤={probs[2]:.3f}")
        print("-"*80)
    
    print("\n‚úÖ –û–ë–£–ß–ï–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û –£–°–ü–ï–®–ù–û!")
    print(f"\nüéØ –§–∏–Ω–∞–ª—å–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å: {best_accuracy:.2f}%")
    print(f"‚ö° –£—Å–∫–æ—Ä–µ–Ω–∏–µ GPU: –ø—Ä–∏–º–µ—Ä–Ω–æ {10 if device.type == 'cuda' else 1}x")
    print(f"\nüìÅ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤: {MODELS_DIR}")
    print("\nüöÄ –¢–µ–ø–µ—Ä—å –º–æ–∂–µ—Ç–µ –∑–∞–ø—É—Å—Ç–∏—Ç—å –±–æ—Ç–∞!")

if __name__ == "__main__":
    train_model_gpu()