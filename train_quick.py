"""
–ë—ã—Å—Ç—Ä–æ–µ –æ–±—É—á–µ–Ω–∏–µ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
"""
import sys
sys.path.append('src')

from data_loader import DataLoader
from model import SentimentModel
from config import MODELS_DIR

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout
import joblib

def create_simple_model(vocab_size=1000, max_length=50):
    """–°–æ–∑–¥–∞–Ω–∏–µ –æ—á–µ–Ω—å –ø—Ä–æ—Å—Ç–æ–π –º–æ–¥–µ–ª–∏"""
    model = Sequential([
        Embedding(vocab_size, 50),  # –£–±—Ä–∞–ª–∏ input_length
        LSTM(32),
        Dropout(0.5),
        Dense(3, activation='softmax')
    ])
    
    model.compile(
        optimizer='adam',
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy']
    )
    
    return model

def quick_train():
    """–ë—ã—Å—Ç—Ä–æ–µ –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –º–∞–ª–µ–Ω—å–∫–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ"""
    print("=" * 50)
    print("–ë–´–°–¢–†–û–ï –û–ë–£–ß–ï–ù–ò–ï –î–õ–Ø –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–Ø")
    print("=" * 50)
    
    try:
        # 1. –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–æ—Å—Ç–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞
        print("\n1. –°–æ–∑–¥–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö...")
        loader = DataLoader()
        df = loader.create_synthetic_dataset(num_samples=300)
        
        # 2. –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞
        print("\n2. –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤...")
        df['processed_text'] = df['text'].apply(loader.preprocess_text)
        
        # 3. –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –º–µ—Ç–æ–∫
        print("\n3. –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –º–µ—Ç–æ–∫...")
        df['label_encoded'] = loader.label_encoder.fit_transform(df['label'])
        
        # 4. –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/test
        X_train, X_test, y_train, y_test = train_test_split(
            df['processed_text'], df['label_encoded'], 
            test_size=0.2, random_state=42, stratify=df['label_encoded']
        )
        
        print(f"   –û–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞: {len(X_train)}")
        print(f"   –¢–µ—Å—Ç–æ–≤–∞—è –≤—ã–±–æ—Ä–∫–∞: {len(X_test)}")
        
        # 5. –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
        print("\n4. –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è...")
        tokenizer = Tokenizer(num_words=1000)
        tokenizer.fit_on_texts(X_train)
        
        X_train_seq = tokenizer.texts_to_sequences(X_train)
        X_test_seq = tokenizer.texts_to_sequences(X_test)
        
        X_train_pad = pad_sequences(X_train_seq, maxlen=50)
        X_test_pad = pad_sequences(X_test_seq, maxlen=50)
        
        # 6. –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–æ—Å—Ç–æ–π –º–æ–¥–µ–ª–∏
        print("\n5. –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏...")
        model = create_simple_model(vocab_size=1000, max_length=50)
        # –°—Ç—Ä–æ–∏–º –º–æ–¥–µ–ª—å –ø–µ—Ä–µ–¥ –ø–æ–¥—Å—á–µ—Ç–æ–º –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
        model.build(input_shape=(None, 50))
        print(f"   –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏: {model.count_params():,}")
        
        # 7. –û–±—É—á–µ–Ω–∏–µ (–≤—Å–µ–≥–æ 5 —ç–ø–æ—Ö)
        print("\n6. –û–±—É—á–µ–Ω–∏–µ (5 —ç–ø–æ—Ö)...")
        history = model.fit(
            X_train_pad, y_train,
            batch_size=32,
            epochs=5,
            validation_split=0.2,
            verbose=1
        )
        
        # 8. –û—Ü–µ–Ω–∫–∞
        print("\n7. –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏...")
        loss, accuracy = model.evaluate(X_test_pad, y_test, verbose=0)
        print(f"   –¢–æ—á–Ω–æ—Å—Ç—å –Ω–∞ —Ç–µ—Å—Ç–µ: {accuracy:.2%}")
        
        # 9. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
        print("\n8. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏...")
        MODELS_DIR.mkdir(parents=True, exist_ok=True)
        model.save(MODELS_DIR / "quick_model.h5")
        joblib.dump(tokenizer, MODELS_DIR / "quick_tokenizer.pkl")
        joblib.dump(loader.label_encoder, MODELS_DIR / "quick_label_encoder.pkl")
        
        # 10. –¢–µ—Å—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
        print("\n9. –¢–µ—Å—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π:")
        test_texts = ["–û—Ç–ª–∏—á–Ω–æ!", "–£–∂–∞—Å–Ω–æ!", "–ù–æ—Ä–º–∞–ª—å–Ω–æ"]
        test_seq = tokenizer.texts_to_sequences(test_texts)
        test_pad = pad_sequences(test_seq, maxlen=50)
        predictions = model.predict(test_pad, verbose=0)
        
        for text, pred in zip(test_texts, predictions):
            pred_class = np.argmax(pred)
            label = loader.label_encoder.inverse_transform([pred_class])[0]
            confidence = pred[pred_class]
            print(f"   '{text}' -> {label} ({confidence:.2%})")
        
        print("\n‚úÖ –ë—ã—Å—Ç—Ä–æ–µ –æ–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
        print("\n–¢–µ–ø–µ—Ä—å –º–æ–∂–µ—Ç–µ –∑–∞–ø—É—Å—Ç–∏—Ç—å –ø–æ–ª–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ:")
        print("  python src/train.py")
        
        return True
        
    except Exception as e:
        print(f"\n‚ùå –û—à–∏–±–∫–∞: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = quick_train()
    if not success:
        print("\nüí° –ü–æ–¥—Å–∫–∞–∑–∫–∏:")
        print("1. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —É—Å—Ç–∞–Ω–æ–≤–∫—É –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π: pip install -r requirements.txt")
        print("2. –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø—Ä–æ–µ–∫—Ç–∞ —Å–æ–∑–¥–∞–Ω–∞: python setup_project.py")
        print("3. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –æ—á–∏—Å—Ç–∏—Ç—å –¥–∞–Ω–Ω—ã–µ: python clean_data.py")