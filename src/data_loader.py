"""
–†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö —Å –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º–∏
"""
import os
import re
import json
import pandas as pd
import numpy as np
from typing import Tuple, List
import requests
import zipfile
from tqdm import tqdm
import gdown

import nltk
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

from config import RAW_DATA_DIR, PROCESSED_DATA_DIR, TEXT_PREPROCESSING, LABEL_ENCODER_PATH

# –ó–∞–≥—Ä—É–∑–∫–∞ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤ NLTK
nltk.download('stopwords', quiet=True)
nltk.download('punkt', quiet=True)
nltk.download('wordnet', quiet=True)
from nltk.corpus import stopwords

try:
    russian_stopwords = set(stopwords.words('russian'))
except:
    print("–ó–∞–≥—Ä—É–∑–∫–∞ —Å—Ç–æ–ø-—Å–ª–æ–≤...")
    nltk.download('stopwords')
    russian_stopwords = set(stopwords.words('russian'))


class EnhancedDataLoader:
    """–†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –∫–ª–∞—Å—Å –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö –∏–∑ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤"""
    
    def __init__(self):
        self.label_encoder = LabelEncoder()
        self.datasets = []
        
    def download_rusentiment(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ RuSentiment"""
        urls = [
            "http://text-machine.cs.uml.edu/projects/rusentiment/rusentiment_random_posts.csv",
            "https://raw.githubusercontent.com/text-machine-lab/rusentiment/master/Dataset/rusentiment_random_posts.csv",
            "https://github.com/text-machine-lab/rusentiment/raw/master/Dataset/rusentiment_random_posts.csv"
        ]
        
        filepath = RAW_DATA_DIR / "rusentiment.csv"
        
        if not filepath.exists():
            print("–ó–∞–≥—Ä—É–∑–∫–∞ RuSentiment –¥–∞—Ç–∞—Å–µ—Ç–∞...")
            
            for url in urls:
                try:
                    response = requests.get(url, timeout=30)
                    if response.status_code == 200:
                        with open(filepath, 'wb') as f:
                            f.write(response.content)
                        print("‚úÖ RuSentiment –∑–∞–≥—Ä—É–∂–µ–Ω!")
                        break
                except Exception as e:
                    print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Å {url}: {e}")
                    continue
            else:
                print("‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å RuSentiment")
                return None
        
        return filepath

    def download_russian_twitter_corpus(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ Russian Twitter Corpus"""
        # –ó–∞–≥–ª—É—à–∫–∞ - —Ç—Ä–µ–±—É–µ—Ç —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏
        print("üì± Russian Twitter Corpus —Ç—Ä–µ–±—É–µ—Ç —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏ –Ω–∞ —Å–∞–π—Ç–µ")
        print("   –î–æ—Å—Ç—É–ø–µ–Ω –ø–æ –∞–¥—Ä–µ—Å—É: http://study.mokoron.com/")
        return None

    def download_linis_crowd(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ LINIS Crowd dataset"""
        url = "https://github.com/nicolay-r/RuSentRel/raw/master/data/linis_crowd.csv"
        filepath = RAW_DATA_DIR / "linis_crowd.csv"
        
        if not filepath.exists():
            try:
                print("üìä –ó–∞–≥—Ä—É–∑–∫–∞ LINIS Crowd...")
                response = requests.get(url, timeout=30)
                if response.status_code == 200:
                    with open(filepath, 'wb') as f:
                        f.write(response.content)
                    print("‚úÖ LINIS Crowd –∑–∞–≥—Ä—É–∂–µ–Ω!")
                    return filepath
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ LINIS Crowd: {e}")
        
        return filepath if filepath.exists() else None

    def download_kaggle_russian_news(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ —Ä—É—Å—Å–∫–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π —Å Kaggle (—Ç—Ä–µ–±—É–µ—Ç API –∫–ª—é—á)"""
        try:
            import kaggle
            filepath = RAW_DATA_DIR / "russian_news.csv"
            
            if not filepath.exists():
                print("üì∞ –ó–∞–≥—Ä—É–∑–∫–∞ —Ä—É—Å—Å–∫–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π —Å Kaggle...")
                kaggle.api.dataset_download_files(
                    'blackmoon/russian-language-toxic-comments',
                    path=str(RAW_DATA_DIR),
                    unzip=True
                )
                print("‚úÖ Kaggle dataset –∑–∞–≥—Ä—É–∂–µ–Ω!")
            
            return filepath if filepath.exists() else None
            
        except Exception as e:
            print(f"‚ö†Ô∏è Kaggle API –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {e}")
            print("   –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install kaggle")
            print("   –ù–∞—Å—Ç—Ä–æ–π—Ç–µ API –∫–ª—é—á: https://github.com/Kaggle/kaggle-api")
            return None

    def load_rutweetcorp(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ RuTweetCorp (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω)"""
        filepath = RAW_DATA_DIR / "rutweetcorp.csv"
        
        if filepath.exists():
            try:
                df = pd.read_csv(filepath, encoding='utf-8')
                print(f"‚úÖ RuTweetCorp –∑–∞–≥—Ä—É–∂–µ–Ω: {len(df)} –∑–∞–ø–∏—Å–µ–π")
                return df
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ RuTweetCorp: {e}")
        
        return None

    def create_extended_synthetic_dataset(self, num_samples: int = 50000) -> pd.DataFrame:
        """–°–æ–∑–¥–∞–Ω–∏–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–≥–æ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞"""
        print(f"üîÑ –°–æ–∑–¥–∞–Ω–∏–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–≥–æ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ ({num_samples} –ø—Ä–∏–º–µ—Ä–æ–≤)...")
        
        # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ —à–∞–±–ª–æ–Ω—ã
        positive_patterns = {
            "reviews": [
                "–û—Ç–ª–∏—á–Ω—ã–π {product}! –í—Å–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥—É—é –∫—É–ø–∏—Ç—å.",
                "–ü—Ä–µ–≤–æ—Å—Ö–æ–¥–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ {product}. –û—á–µ–Ω—å –¥–æ–≤–æ–ª–µ–Ω –ø–æ–∫—É–ø–∫–æ–π.",
                "–õ—É—á—à–∏–π {product} –∫–æ—Ç–æ—Ä—ã–π —è –∫–æ–≥–¥–∞-–ª–∏–±–æ {action}!",
                "–°—É–ø–µ—Ä {product}! –°—Ç–æ–∏—Ç –∫–∞–∂–¥–æ–π –∫–æ–ø–µ–π–∫–∏.",
                "–í–µ–ª–∏–∫–æ–ª–µ–ø–Ω—ã–π {product}. –ë—É–¥—É –∑–∞–∫–∞–∑—ã–≤–∞—Ç—å –µ—â–µ.",
                "–ü–æ—Ç—Ä—è—Å–∞—é—â–∏–π {product}! –ü—Ä–µ–≤–∑–æ—à–µ–ª –≤—Å–µ –æ–∂–∏–¥–∞–Ω–∏—è.",
                "–ó–∞–º–µ—á–∞—Ç–µ–ª—å–Ω—ã–π {product}. –°–ø–∞—Å–∏–±–æ –ø—Ä–æ–¥–∞–≤—Ü—É!",
                "–ò–¥–µ–∞–ª—å–Ω—ã–π {product} –¥–ª—è {purpose}. –†–µ–∫–æ–º–µ–Ω–¥—É—é!",
                "–í–æ—Å—Ö–∏—Ç–∏—Ç–µ–ª—å–Ω—ã–π {product}! –ö–∞—á–µ—Å—Ç–≤–æ –Ω–∞ –≤—ã—Å–æ—Ç–µ.",
                "–ü—Ä–µ–∫—Ä–∞—Å–Ω—ã–π {product}. –û—á–µ–Ω—å –±—ã—Å—Ç—Ä–∞—è –¥–æ—Å—Ç–∞–≤–∫–∞."
            ],
            "emotions": [
                "–°–µ–≥–æ–¥–Ω—è {feeling}! –í—Å–µ –ø–æ–ª—É—á–∞–µ—Ç—Å—è –æ—Ç–ª–∏—á–Ω–æ.",
                "–ö–∞–∫–æ–π {adj} –¥–µ–Ω—å! –ù–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ {mood}.",
                "–û—á–µ–Ω—å {feeling} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–º. –í—Å–µ —Å—É–ø–µ—Ä!",
                "–ü—Ä–µ–∫—Ä–∞—Å–Ω–æ–µ {feeling}! –ñ–∏–∑–Ω—å —É–¥–∞–ª–∞—Å—å.",
                "–°–µ–≥–æ–¥–Ω—è –æ—Å–æ–±–µ–Ω–Ω–æ {feeling}. –í—Å–µ –∏–¥–µ—Ç –∫–∞–∫ –Ω–∞–¥–æ.",
                "–û—Ç–ª–∏—á–Ω–æ–µ {feeling}! –°–æ–ª–Ω—Ü–µ —Å–≤–µ—Ç–∏—Ç —è—Ä–∫–æ.",
                "–ó–∞–º–µ—á–∞—Ç–µ–ª—å–Ω–æ–µ {feeling} –æ—Ç {activity}.",
                "–í–æ—Å—Ö–∏—Ç–∏—Ç–µ–ª—å–Ω–æ–µ {feeling}! –°–ø–∞—Å–∏–±–æ –≤—Å–µ–º.",
                "–ü–æ—Ç—Ä—è—Å–∞—é—â–µ–µ {feeling} –æ—Ç –ø—Ä–æ–¥–µ–ª–∞–Ω–Ω–æ–π —Ä–∞–±–æ—Ç—ã.",
                "–í–µ–ª–∏–∫–æ–ª–µ–ø–Ω–æ–µ {feeling}! –¢–∞–∫ –¥–µ—Ä–∂–∞—Ç—å!"
            ],
            "services": [
                "–û—Ç–ª–∏—á–Ω—ã–π —Å–µ—Ä–≤–∏—Å –≤ {place}. –ü–µ—Ä—Å–æ–Ω–∞–ª {adj}.",
                "–°—É–ø–µ—Ä –æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏–µ! –û—Ñ–∏—Ü–∏–∞–Ω—Ç –±—ã–ª {adj}.",
                "–ü—Ä–µ–∫—Ä–∞—Å–Ω—ã–π {service}. –ë—É–¥—É –æ–±—Ä–∞—â–∞—Ç—å—Å—è –µ—â–µ.",
                "–ö–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π {service} –ø–æ –¥–æ—Å—Ç—É–ø–Ω–æ–π —Ü–µ–Ω–µ.",
                "–ë—ã—Å—Ç—Ä—ã–π –∏ {adj} {service}. –†–µ–∫–æ–º–µ–Ω–¥—É—é!",
                "–ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π {service}. –í—Å–µ –Ω–∞ –≤—ã—Å—à–µ–º —É—Ä–æ–≤–Ω–µ.",
                "–û—Ç–∑—ã–≤—á–∏–≤—ã–π –ø–µ—Ä—Å–æ–Ω–∞–ª –∏ {adj} {service}.",
                "–£–¥–æ–±–Ω—ã–π {service} —Å {adj} –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–æ–º.",
                "–ù–∞–¥–µ–∂–Ω—ã–π {service}. –ü–æ–ª—å–∑—É—é—Å—å —É–∂–µ –¥–∞–≤–Ω–æ.",
                "–°–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–π {service} —Å {adj} –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π."
            ]
        }
        
        negative_patterns = {
            "reviews": [
                "–£–∂–∞—Å–Ω—ã–π {product}! –ù–µ —Ä–µ–∫–æ–º–µ–Ω–¥—É—é –Ω–∏–∫–æ–º—É.",
                "–û—Ç–≤—Ä–∞—Ç–∏—Ç–µ–ª—å–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ {product}. –î–µ–Ω—å–≥–∏ –Ω–∞ –≤–µ—Ç–µ—Ä.",
                "–•—É–¥—à–∏–π {product} –∫–æ—Ç–æ—Ä—ã–π —è {action}. –ö–æ—à–º–∞—Ä!",
                "–ü—Ä–æ–≤–∞–ª—å–Ω—ã–π {product}. –ü–æ–ª–Ω–æ–µ —Ä–∞–∑–æ—á–∞—Ä–æ–≤–∞–Ω–∏–µ.",
                "–ù–µ–ø—Ä–∏–µ–º–ª–µ–º—ã–π {product}. –í–µ—Ä–Ω—É –æ–±—Ä–∞—Ç–Ω–æ.",
                "–ë–µ–∑–æ–±—Ä–∞–∑–Ω—ã–π {product}! –ö–∞–∫ —Ç–∞–∫–æ–µ –º–æ–∂–Ω–æ –ø—Ä–æ–¥–∞–≤–∞—Ç—å?",
                "–ù–∏–∫—É–¥–∞ –Ω–µ –≥–æ–¥–Ω—ã–π {product}. –ù–µ –ø–æ–∫—É–ø–∞–π—Ç–µ!",
                "–ë—Ä–∞–∫–æ–≤–∞–Ω–Ω—ã–π {product}. –ü–æ—Ç–µ—Ä—è–Ω–Ω—ã–µ –¥–µ–Ω—å–≥–∏.",
                "–ù–µ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π {product}. –û–±–º–∞–Ω –ø–æ–∫—É–ø–∞—Ç–µ–ª–µ–π!",
                "–ü–æ–ª–æ–º–∞–Ω–Ω—ã–π {product}. –£–∂–∞—Å–Ω–∞—è —Ä–∞–±–æ—Ç–∞ –º–∞–≥–∞–∑–∏–Ω–∞."
            ],
            "emotions": [
                "–°–µ–≥–æ–¥–Ω—è {feeling}. –í—Å–µ –∏–¥–µ—Ç –Ω–µ —Ç–∞–∫.",
                "–£–∂–∞—Å–Ω–æ–µ {feeling}! –î–µ–Ω—å –Ω–µ –∑–∞–¥–∞–ª—Å—è.",
                "–û—á–µ–Ω—å {feeling} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–º. –í—Å–µ –ø–ª–æ—Ö–æ.",
                "–î–µ–ø—Ä–µ—Å—Å–∏–≤–Ω–æ–µ {feeling} –æ—Ç {activity}.",
                "–°–µ–≥–æ–¥–Ω—è –æ—Å–æ–±–µ–Ω–Ω–æ {feeling}. –ù–∏—á–µ–≥–æ –Ω–µ –ø–æ–ª—É—á–∞–µ—Ç—Å—è.",
                "–ü–µ—á–∞–ª—å–Ω–æ–µ {feeling} –æ—Ç –ø—Ä–æ–∏—Å—Ö–æ–¥—è—â–µ–≥–æ.",
                "–†–∞—Å—Å—Ç—Ä–æ–µ–Ω–Ω–æ–µ {feeling} –∏–∑-–∑–∞ –Ω–µ—É–¥–∞—á.",
                "–ë–æ–ª–µ–∑–Ω–µ–Ω–Ω–æ–µ {feeling} –æ—Ç –ø–æ—Ç–µ—Ä—å.",
                "–ú—Ä–∞—á–Ω–æ–µ {feeling} –Ω–∞ –¥—É—à–µ.",
                "–¢—è–∂–µ–ª–æ–µ {feeling} –≤–µ—Å—å –¥–µ–Ω—å."
            ],
            "services": [
                "–£–∂–∞—Å–Ω—ã–π —Å–µ—Ä–≤–∏—Å –≤ {place}. –ü–µ—Ä—Å–æ–Ω–∞–ª {adj}.",
                "–û—Ç–≤—Ä–∞—Ç–∏—Ç–µ–ª—å–Ω–æ–µ –æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏–µ! –û—Ñ–∏—Ü–∏–∞–Ω—Ç –±—ã–ª {adj}.",
                "–ü—Ä–æ–≤–∞–ª—å–Ω—ã–π {service}. –ë–æ–ª—å—à–µ –Ω–µ –æ–±—Ä–∞—â—É—Å—å.",
                "–ù–µ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π {service} –∑–∞ –±–æ–ª—å—à–∏–µ –¥–µ–Ω—å–≥–∏.",
                "–ú–µ–¥–ª–µ–Ω–Ω—ã–π –∏ {adj} {service}. –ù–µ —Ä–µ–∫–æ–º–µ–Ω–¥—É—é!",
                "–ù–µ–ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π {service}. –í—Å–µ –Ω–∞ –Ω–∏–∑–∫–æ–º —É—Ä–æ–≤–Ω–µ.",
                "–ì—Ä—É–±—ã–π –ø–µ—Ä—Å–æ–Ω–∞–ª –∏ {adj} {service}.",
                "–ù–µ—É–¥–æ–±–Ω—ã–π {service} —Å {adj} –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–æ–º.",
                "–ù–µ–Ω–∞–¥–µ–∂–Ω—ã–π {service}. –ü–æ—Å—Ç–æ—è–Ω–Ω—ã–µ —Å–±–æ–∏.",
                "–£—Å—Ç–∞—Ä–µ–≤—à–∏–π {service} —Å {adj} –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π."
            ]
        }
        
        neutral_patterns = [
            "–û–±—ã—á–Ω—ã–π {product}. –ù–∏—á–µ–≥–æ –æ—Å–æ–±–µ–Ω–Ω–æ–≥–æ, –Ω–æ –ø–æ–π–¥–µ—Ç.",
            "–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ {product}. –°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç —Ü–µ–Ω–µ.",
            "–°—Ä–µ–¥–Ω–∏–π {product}. –ï—Å—Ç—å –ø–ª—é—Å—ã –∏ –º–∏–Ω—É—Å—ã.",
            "–ü—Ä–∏–µ–º–ª–µ–º—ã–π {product} –¥–ª—è —Å–≤–æ–∏—Ö –∑–∞–¥–∞—á.",
            "–ù–æ—Ä–º–∞–ª—å–Ω—ã–π {product}. –ë–µ–∑ –∏–∑–ª–∏—à–µ—Å—Ç–≤, –Ω–æ —Ä–∞–±–æ—Ç–∞–µ—Ç.",
            "–¢–∏–ø–∏—á–Ω—ã–π {product} –≤ —Å–≤–æ–µ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏.",
            "–ù–µ–ø–ª–æ—Ö–æ–π {product}, –Ω–æ –º–æ–∂–Ω–æ –Ω–∞–π—Ç–∏ –ª—É—á—à–µ.",
            "–£–¥–æ–≤–ª–µ—Ç–≤–æ—Ä–∏—Ç–µ–ª—å–Ω—ã–π {product} –∑–∞ —Å–≤–æ–∏ –¥–µ–Ω—å–≥–∏.",
            "–û–±—ã–∫–Ω–æ–≤–µ–Ω–Ω—ã–π {product}. –°–ø—Ä–∞–≤–ª—è–µ—Ç—Å—è —Å –∑–∞–¥–∞—á–∞–º–∏.",
            "–†—è–¥–æ–≤–æ–π {product}. –û–∂–∏–¥–∞–Ω–∏—è –æ–ø—Ä–∞–≤–¥–∞–ª–∏—Å—å."
        ]
        
        # –°–ª–æ–≤–∞—Ä–∏ –¥–ª—è –ø–æ–¥—Å—Ç–∞–Ω–æ–≤–æ–∫
        products = [
            "—Ç–æ–≤–∞—Ä", "–ø—Ä–æ–¥—É–∫—Ç", "—Ç–µ–ª–µ—Ñ–æ–Ω", "–Ω–æ—É—Ç–±—É–∫", "–ø–ª–∞–Ω—à–µ—Ç", "–Ω–∞—É—à–Ω–∏–∫–∏",
            "–∫–Ω–∏–≥–∞", "—Ñ–∏–ª—å–º", "–∏–≥—Ä–∞", "–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ", "—Å–∞–π—Ç", "—Å–µ—Ä–≤–∏—Å",
            "—Ä–µ—Å—Ç–æ—Ä–∞–Ω", "–∫–∞—Ñ–µ", "–æ—Ç–µ–ª—å", "–º–∞–≥–∞–∑–∏–Ω", "–∫—É—Ä—Å", "–ø—Ä–æ–≥—Ä–∞–º–º–∞"
        ]
        
        positive_adj = [
            "–æ—Ç–ª–∏—á–Ω—ã–π", "–ø—Ä–µ–≤–æ—Å—Ö–æ–¥–Ω—ã–π", "–∑–∞–º–µ—á–∞—Ç–µ–ª—å–Ω—ã–π", "–≤–µ–ª–∏–∫–æ–ª–µ–ø–Ω—ã–π",
            "–ø–æ—Ç—Ä—è—Å–∞—é—â–∏–π", "–≤–æ—Å—Ö–∏—Ç–∏—Ç–µ–ª—å–Ω—ã–π", "–ø—Ä–µ–∫—Ä–∞—Å–Ω—ã–π", "–∏–¥–µ–∞–ª—å–Ω—ã–π",
            "–ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π", "–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π", "–±—ã—Å—Ç—Ä—ã–π", "—É–¥–æ–±–Ω—ã–π"
        ]
        
        negative_adj = [
            "—É–∂–∞—Å–Ω—ã–π", "–æ—Ç–≤—Ä–∞—Ç–∏—Ç–µ–ª—å–Ω—ã–π", "–ø—Ä–æ–≤–∞–ª—å–Ω—ã–π", "–Ω–µ–ø—Ä–∏–µ–º–ª–µ–º—ã–π",
            "–±–µ–∑–æ–±—Ä–∞–∑–Ω—ã–π", "–Ω–µ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π", "–º–µ–¥–ª–µ–Ω–Ω—ã–π", "–Ω–µ—É–¥–æ–±–Ω—ã–π",
            "–≥—Ä—É–±—ã–π", "–Ω–µ–ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π", "–Ω–µ–Ω–∞–¥–µ–∂–Ω—ã–π", "—É—Å—Ç–∞—Ä–µ–≤—à–∏–π"
        ]
        
        positive_feelings = [
            "—Ä–∞–¥—É—é—Å—å", "–¥–æ–≤–æ–ª–µ–Ω", "—Å—á–∞—Å—Ç–ª–∏–≤", "–≤–æ—Å—Ö–∏—â–µ–Ω", "–≤–¥–æ—Ö–Ω–æ–≤–ª–µ–Ω",
            "–≤–æ–æ–¥—É—à–µ–≤–ª–µ–Ω", "–±–ª–∞–≥–æ–¥–∞—Ä–µ–Ω", "—É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä–µ–Ω", "–≤–∑–≤–æ–ª–Ω–æ–≤–∞–Ω"
        ]
        
        negative_feelings = [
            "—Ä–∞—Å—Å—Ç—Ä–æ–µ–Ω", "—Ä–∞–∑–æ—á–∞—Ä–æ–≤–∞–Ω", "–æ–≥–æ—Ä—á–µ–Ω", "—Ä–∞–∑–¥—Ä–∞–∂–µ–Ω", "–Ω–µ–¥–æ–≤–æ–ª–µ–Ω",
            "–≤–æ–∑–º—É—â–µ–Ω", "–æ–±–∏–∂–µ–Ω", "–∑–ª—é—Å—å", "–≥—Ä—É—â—É", "–ø–µ—á–∞–ª—é—Å—å"
        ]
        
        services = [
            "—Å–µ—Ä–≤–∏—Å", "–æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏–µ", "–ø–æ–¥–¥–µ—Ä–∂–∫–∞", "–¥–æ—Å—Ç–∞–≤–∫–∞", "—Ä–µ–º–æ–Ω—Ç",
            "–∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏—è", "—É—Å—Ç–∞–Ω–æ–≤–∫–∞", "–Ω–∞—Å—Ç—Ä–æ–π–∫–∞", "–æ–±—É—á–µ–Ω–∏–µ"
        ]
        
        places = [
            "—Ä–µ—Å—Ç–æ—Ä–∞–Ω–µ", "–∫–∞—Ñ–µ", "–º–∞–≥–∞–∑–∏–Ω–µ", "—Å–∞–ª–æ–Ω–µ", "–±–∞–Ω–∫–µ", "–ø–æ–ª–∏–∫–ª–∏–Ω–∏–∫–µ",
            "–æ—Ñ–∏—Å–µ", "–º–∞—Å—Ç–µ—Ä—Å–∫–æ–π", "—Ü–µ–Ω—Ç—Ä–µ", "–∫–æ–º–ø–∞–Ω–∏–∏"
        ]
        
        activities = [
            "—Ä–∞–±–æ—Ç—ã", "—É—á–µ–±—ã", "–ø–æ–∫—É–ø–∫–∏", "–ø—É—Ç–µ—à–µ—Å—Ç–≤–∏—è", "–æ—Ç–¥—ã—Ö–∞",
            "—Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏", "–≤—Å—Ç—Ä–µ—á–∏", "–ø—Ä–æ–µ–∫—Ç–∞", "–º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏—è"
        ]
        
        purposes = [
            "—Ä–∞–±–æ—Ç—ã", "–¥–æ–º–∞", "—É—á–µ–±—ã", "–æ—Ç–¥—ã—Ö–∞", "–ø—É—Ç–µ—à–µ—Å—Ç–≤–∏–π",
            "—Å–ø–æ—Ä—Ç–∞", "—Ö–æ–±–±–∏", "–±–∏–∑–Ω–µ—Å–∞", "—Ä–∞–∑–≤–ª–µ—á–µ–Ω–∏–π"
        ]
        
        actions = ["–ø–æ–∫—É–ø–∞–ª", "–∑–∞–∫–∞–∑—ã–≤–∞–ª", "–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª", "–ø—Ä–æ–±–æ–≤–∞–ª", "—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–ª"]
        moods = ["–æ—Ç–ª–∏—á–Ω–æ–µ", "–ø—Ä–µ–∫—Ä–∞—Å–Ω–æ–µ", "–∑–∞–º–µ—á–∞—Ç–µ–ª—å–Ω–æ–µ", "–≤–µ–ª–∏–∫–æ–ª–µ–ø–Ω–æ–µ"]
        
        data = []
        samples_per_class = num_samples // 3
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–æ–∑–∏—Ç–∏–≤–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤
        for _ in range(samples_per_class):
            category = np.random.choice(list(positive_patterns.keys()))
            template = np.random.choice(positive_patterns[category])
            
            text = template.format(
                product=np.random.choice(products),
                adj=np.random.choice(positive_adj),
                feeling=np.random.choice(positive_feelings),
                mood=np.random.choice(moods),
                service=np.random.choice(services),
                place=np.random.choice(places),
                activity=np.random.choice(activities),
                purpose=np.random.choice(purposes),
                action=np.random.choice(actions)
            )
            
            data.append({"text": text, "label": "positive"})
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤
        for _ in range(samples_per_class):
            category = np.random.choice(list(negative_patterns.keys()))
            template = np.random.choice(negative_patterns[category])
            
            text = template.format(
                product=np.random.choice(products),
                adj=np.random.choice(negative_adj),
                feeling=np.random.choice(negative_feelings),
                service=np.random.choice(services),
                place=np.random.choice(places),
                activity=np.random.choice(activities)
            )
            
            data.append({"text": text, "label": "negative"})
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –Ω–µ–π—Ç—Ä–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤
        for _ in range(num_samples - 2 * samples_per_class):
            template = np.random.choice(neutral_patterns)
            text = template.format(product=np.random.choice(products))
            data.append({"text": text, "label": "neutral"})
        
        df = pd.DataFrame(data)
        return df.sample(frac=1).reset_index(drop=True)

    def load_all_available_datasets(self) -> pd.DataFrame:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –≤—Å–µ—Ö –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤"""
        combined_data = []
        
        # 1. –ü–æ–ø—ã—Ç–∫–∞ –∑–∞–≥—Ä—É–∑–∏—Ç—å RuSentiment
        rusentiment_path = self.download_rusentiment()
        if rusentiment_path:
            try:
                encodings = ['utf-8', 'cp1251', 'latin1']
                df = None
                
                for encoding in encodings:
                    try:
                        df = pd.read_csv(rusentiment_path, encoding=encoding)
                        print(f"‚úÖ RuSentiment –∑–∞–≥—Ä—É–∂–µ–Ω ({len(df)} –∑–∞–ø–∏—Å–µ–π)")
                        break
                    except UnicodeDecodeError:
                        continue
                
                if df is not None and len(df) > 0:
                    # –û–±—Ä–∞–±–æ—Ç–∫–∞ RuSentiment
                    df = self._process_rusentiment(df)
                    combined_data.append(df)
                    
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ RuSentiment: {e}")
        
        # 2. –ü–æ–ø—ã—Ç–∫–∞ –∑–∞–≥—Ä—É–∑–∏—Ç—å LINIS Crowd
        linis_path = self.download_linis_crowd()
        if linis_path:
            try:
                df = pd.read_csv(linis_path, encoding='utf-8')
                df = self._process_linis_crowd(df)
                if len(df) > 0:
                    print(f"‚úÖ LINIS Crowd –¥–æ–±–∞–≤–ª–µ–Ω ({len(df)} –∑–∞–ø–∏—Å–µ–π)")
                    combined_data.append(df)
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ LINIS Crowd: {e}")
        
        # 3. –ü–æ–ø—ã—Ç–∫–∞ –∑–∞–≥—Ä—É–∑–∏—Ç—å Kaggle –¥–∞–Ω–Ω—ã–µ
        kaggle_path = self.download_kaggle_russian_news()
        if kaggle_path:
            try:
                df = pd.read_csv(kaggle_path, encoding='utf-8')
                df = self._process_kaggle_data(df)
                if len(df) > 0:
                    print(f"‚úÖ Kaggle –¥–∞–Ω–Ω—ã–µ –¥–æ–±–∞–≤–ª–µ–Ω—ã ({len(df)} –∑–∞–ø–∏—Å–µ–π)")
                    combined_data.append(df)
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ Kaggle –¥–∞–Ω–Ω—ã—Ö: {e}")
        
        # 4. –°–æ–∑–¥–∞–Ω–∏–µ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö
        synthetic_size = 50000 if not combined_data else 20000
        synthetic_df = self.create_extended_synthetic_dataset(synthetic_size)
        print(f"‚úÖ –°–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ —Å–æ–∑–¥–∞–Ω—ã ({len(synthetic_df)} –∑–∞–ø–∏—Å–µ–π)")
        combined_data.append(synthetic_df)
        
        # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤
        if combined_data:
            final_df = pd.concat(combined_data, ignore_index=True)
            final_df = final_df.drop_duplicates(subset=['text']).reset_index(drop=True)
            print(f"\nüéØ –ò—Ç–æ–≥–æ–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç: {len(final_df)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π")
            
            # –ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ –∫–ª–∞—Å—Å–æ–≤
            final_df = self._balance_dataset(final_df)
            
            return final_df
        
        return self.create_extended_synthetic_dataset()

    def _process_rusentiment(self, df: pd.DataFrame) -> pd.DataFrame:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ RuSentiment"""
        # –ü–æ–∏—Å–∫ –Ω—É–∂–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫
        text_cols = [col for col in df.columns if any(name in col.lower() for name in ['text', 'comment', 'post'])]
        label_cols = [col for col in df.columns if any(name in col.lower() for name in ['label', 'sentiment', 'class'])]
        
        if text_cols and label_cols:
            df = df[[text_cols[0], label_cols[0]]].copy()
            df.columns = ['text', 'label']
            
            # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –º–µ—Ç–æ–∫
            df['label'] = df['label'].astype(str).str.lower()
            df['label'] = df['label'].map({
                'positive': 'positive', 'negative': 'negative', 'neutral': 'neutral',
                'pos': 'positive', 'neg': 'negative', 'neu': 'neutral',
                '1': 'positive', '0': 'neutral', '-1': 'negative',
                'speech': 'neutral', 'skip': 'neutral'
            }).fillna('neutral')
            
            return df[df['text'].notna() & (df['text'] != '')]
        
        return pd.DataFrame(columns=['text', 'label'])

    def _process_linis_crowd(self, df: pd.DataFrame) -> pd.DataFrame:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ LINIS Crowd"""
        if 'text' in df.columns and 'label' in df.columns:
            df = df[['text', 'label']].copy()
            df['label'] = df['label'].astype(str).str.lower()
            return df[df['text'].notna() & (df['text'] != '')]
        return pd.DataFrame(columns=['text', 'label'])

    def _process_kaggle_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ Kaggle –¥–∞–Ω–Ω—ã—Ö"""
        # –ê–¥–∞–ø—Ç–∏—Ä–æ–≤–∞—Ç—å –ø–æ–¥ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É Kaggle –¥–∞—Ç–∞—Å–µ—Ç–∞
        if 'comment' in df.columns and 'toxic' in df.columns:
            df = df[['comment', 'toxic']].copy()
            df.columns = ['text', 'label']
            df['label'] = df['label'].map({1: 'negative', 0: 'neutral'})
            return df[df['text'].notna() & (df['text'] != '')]
        return pd.DataFrame(columns=['text', 'label'])

    def _balance_dataset(self, df: pd.DataFrame) -> pd.DataFrame:
        """–ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞"""
        label_counts = df['label'].value_counts()
        min_count = label_counts.min()
        
        balanced_dfs = []
        for label in ['positive', 'negative', 'neutral']:
            label_df = df[df['label'] == label]
            if len(label_df) > min_count:
                label_df = label_df.sample(n=min_count, random_state=42)
            balanced_dfs.append(label_df)
        
        balanced_df = pd.concat(balanced_dfs, ignore_index=True)
        balanced_df = balanced_df.sample(frac=1, random_state=42).reset_index(drop=True)
        
        print(f"üìä –î–∞—Ç–∞—Å–µ—Ç —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω:")
        print(balanced_df['label'].value_counts())
        
        return balanced_df

    def preprocess_text(self, text: str) -> str:
        """–£–ª—É—á—à–µ–Ω–Ω–∞—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞"""
        if pd.isna(text):
            return ""
        
        text = str(text)
        
        # –ü—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É
        if TEXT_PREPROCESSING["lowercase"]:
            text = text.lower()
        
        # –£–¥–∞–ª–µ–Ω–∏–µ URL
        if TEXT_PREPROCESSING["remove_urls"]:
            text = re.sub(r'http\S+|www\S+|https\S+', '', text, flags=re.MULTILINE)
        
        # –£–¥–∞–ª–µ–Ω–∏–µ email
        if TEXT_PREPROCESSING["remove_emails"]:
            text = re.sub(r'\S+@\S+', '', text)
        
        # –£–¥–∞–ª–µ–Ω–∏–µ —á–∏—Å–µ–ª (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        if TEXT_PREPROCESSING["remove_numbers"]:
            text = re.sub(r'\d+', '', text)
        
        # –£–¥–∞–ª–µ–Ω–∏–µ –ª–∏—à–Ω–∏—Ö –ø—Ä–æ–±–µ–ª–æ–≤
        text = re.sub(r'\s+', ' ', text)
        
        # –£–¥–∞–ª–µ–Ω–∏–µ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏ (–∫—Ä–æ–º–µ —ç–º–æ–¥–∑–∏)
        if TEXT_PREPROCESSING["remove_punctuation"]:
            text = re.sub(r'[^\w\s\U0001F600-\U0001F64F\U0001F300-\U0001F5FF\U0001F680-\U0001F6FF\U0001F1E0-\U0001F1FF]', ' ', text)
        
        # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
        try:
            tokens = nltk.word_tokenize(text, language='russian')
        except:
            tokens = text.split()
        
        # –£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–æ–ø-—Å–ª–æ–≤
        if TEXT_PREPROCESSING["remove_stopwords"]:
            tokens = [token for token in tokens if token not in russian_stopwords and len(token) > 2]
        
        return ' '.join(tokens)

    def prepare_final_dataset(self) -> Tuple[pd.DataFrame, pd.DataFrame]:
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞"""
        print("üöÄ –ù–∞—á–∏–Ω–∞–µ–º –ø–æ–¥–≥–æ—Ç–æ–≤–∫—É —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞...")
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –≤—Å–µ—Ö –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        df = self.load_all_available_datasets()
        
        # –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤
        print("üîÑ –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤...")
        df['processed_text'] = df['text'].apply(self.preprocess_text)
        
        # –£–¥–∞–ª–µ–Ω–∏–µ –ø—É—Å—Ç—ã—Ö —Å—Ç—Ä–æ–∫
        df = df[df['processed_text'].str.len() > 0]
        
        # –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –º–µ—Ç–æ–∫
        df['label_encoded'] = self.label_encoder.fit_transform(df['label'])
        
        # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/test
        train_df, test_df = train_test_split(
            df[['text', 'processed_text', 'label_encoded', 'label']], 
            test_size=0.2, 
            random_state=42,
            stratify=df['label_encoded']
        )
        
        print(f"\n‚úÖ –§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
        print(f"   –û–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞: {len(train_df)} –ø—Ä–∏–º–µ—Ä–æ–≤")
        print(f"   –¢–µ—Å—Ç–æ–≤–∞—è –≤—ã–±–æ—Ä–∫–∞: {len(test_df)} –ø—Ä–∏–º–µ—Ä–æ–≤")
        print(f"   –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤ –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–µ:")
        print(f"   {train_df['label'].value_counts()}")
        
        return train_df, test_df

    def save_processed_data(self, train_df: pd.DataFrame, test_df: pd.DataFrame):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
        train_df.to_csv(PROCESSED_DATA_DIR / "train_data.csv", index=False)
        test_df.to_csv(PROCESSED_DATA_DIR / "test_data.csv", index=False)
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ label encoder
        import joblib
        joblib.dump(self.label_encoder, PROCESSED_DATA_DIR / "label_encoder.pkl")
        joblib.dump(self.label_encoder, LABEL_ENCODER_PATH)
        
        print(f"\nüíæ –î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {PROCESSED_DATA_DIR}")


if __name__ == "__main__":
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è tqdm –¥–ª—è pandas
    tqdm.pandas()
    
    # –°–æ–∑–¥–∞–Ω–∏–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–≥–æ –∑–∞–≥—Ä—É–∑—á–∏–∫–∞
    loader = EnhancedDataLoader()
    
    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    train_df, test_df = loader.prepare_final_dataset()
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
    loader.save_processed_data(train_df, test_df)