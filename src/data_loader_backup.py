"""
–ò—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö —Å –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º–∏ –∏ –≥–æ—Ç–æ–≤—ã–º–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞–º–∏
–ó–ê–ú–ï–ù–ò–¢–ï –í–ï–°–¨ –ö–û–î –í src/data_loader.py –ù–ê –≠–¢–û–¢
"""
import os
import re
import json
import pandas as pd
import numpy as np
from typing import Tuple, List
import requests
import zipfile
from tqdm import tqdm
import gdown
import random
import io
from pathlib import Path

import nltk
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

from config import RAW_DATA_DIR, PROCESSED_DATA_DIR, TEXT_PREPROCESSING, LABEL_ENCODER_PATH

# –ó–∞–≥—Ä—É–∑–∫–∞ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤ NLTK
nltk.download('stopwords', quiet=True)
nltk.download('punkt', quiet=True)
from nltk.corpus import stopwords

try:
    russian_stopwords = set(stopwords.words('russian'))
except:
    print("–ó–∞–≥—Ä—É–∑–∫–∞ —Å—Ç–æ–ø-—Å–ª–æ–≤...")
    nltk.download('stopwords')
    russian_stopwords = set(stopwords.words('russian'))


class EnhancedDataLoader:
    """–ù–∞–¥–µ–∂–Ω—ã–π –∑–∞–≥—Ä—É–∑—á–∏–∫ –¥–∞–Ω–Ω—ã—Ö —Å –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º–∏ - –û–ë–ù–û–í–õ–ï–ù"""
    
    def __init__(self):
        self.label_encoder = LabelEncoder()
        self.datasets = []
        
    def download_rusentiment_robust(self):
        """–ù–∞–¥–µ–∂–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ RuSentiment —Å –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º–∏"""
        filepath = RAW_DATA_DIR / "rusentiment.csv"
        
        if filepath.exists():
            print("‚úÖ RuSentiment —É–∂–µ –∑–∞–≥—Ä—É–∂–µ–Ω")
            return filepath
        
        print("üìä –ó–∞–≥—Ä—É–∑–∫–∞ RuSentiment –¥–∞—Ç–∞—Å–µ—Ç–∞...")
        
        # –°–ø–æ—Å–æ–± 1: –ü—Ä—è–º—ã–µ GitHub —Å—Å—ã–ª–∫–∏
        github_urls = [
            "https://raw.githubusercontent.com/text-machine-lab/rusentiment/master/Dataset/rusentiment_random_posts.csv",
            "https://github.com/text-machine-lab/rusentiment/raw/master/Dataset/rusentiment_random_posts.csv",
            "https://raw.githubusercontent.com/text-machine-lab/rusentiment/main/Dataset/rusentiment_random_posts.csv"
        ]
        
        for url in github_urls:
            try:
                print(f"   –ü–æ–ø—ã—Ç–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Å GitHub: {url[:50]}...")
                response = requests.get(url, timeout=30, headers={
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
                })
                if response.status_code == 200 and len(response.content) > 1000:
                    with open(filepath, 'wb') as f:
                        f.write(response.content)
                    print("‚úÖ RuSentiment –∑–∞–≥—Ä—É–∂–µ–Ω —Å GitHub!")
                    return filepath
            except Exception as e:
                print(f"   ‚ùå –û—à–∏–±–∫–∞ GitHub: {e}")
                continue
        
        # –°–ø–æ—Å–æ–± 2: Google Drive –∑–µ—Ä–∫–∞–ª–æ
        try:
            print("   –ü–æ–ø—ã—Ç–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Å Google Drive...")
            # ID —Ñ–∞–π–ª–∞ –Ω–∞ Google Drive (–ø—Ä–∏–º–µ—Ä–Ω—ã–π)
            file_id = "1BYr3c7LoOveKr3rPdKrfIFgTWCd5Q5nG"
            url = f"https://drive.google.com/uc?id={file_id}"
            
            response = requests.get(url, timeout=30)
            if response.status_code == 200 and len(response.content) > 1000:
                with open(filepath, 'wb') as f:
                    f.write(response.content)
                print("‚úÖ RuSentiment –∑–∞–≥—Ä—É–∂–µ–Ω —Å Google Drive!")
                return filepath
        except Exception as e:
            print(f"   ‚ùå –û—à–∏–±–∫–∞ Google Drive: {e}")
        
        # –°–ø–æ—Å–æ–± 3: –°–æ–∑–¥–∞–Ω–∏–µ –¥–µ–º–æ-–≤–µ—Ä—Å–∏–∏ RuSentiment
        print("   üìù –°–æ–∑–¥–∞–Ω–∏–µ –¥–µ–º–æ-–≤–µ—Ä—Å–∏–∏ RuSentiment...")
        demo_data = self.create_rusentiment_demo()
        demo_data.to_csv(filepath, index=False, encoding='utf-8')
        print("‚úÖ –î–µ–º–æ-–≤–µ—Ä—Å–∏—è RuSentiment —Å–æ–∑–¥–∞–Ω–∞!")
        return filepath
    
    def create_rusentiment_demo(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ –¥–µ–º–æ-–≤–µ—Ä—Å–∏–∏ RuSentiment —Å —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏"""
        data = []
        
        # –ü–æ–∑–∏—Ç–∏–≤–Ω—ã–µ —Ç–≤–∏—Ç—ã
        positive_tweets = [
            "–û—Ç–ª–∏—á–Ω–∞—è –ø–æ–≥–æ–¥–∞ —Å–µ–≥–æ–¥–Ω—è! –ù–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ —Å—É–ø–µ—Ä! #—Ö–æ—Ä–æ—à–∏–π–¥–µ–Ω—å",
            "–°–ø–∞—Å–∏–±–æ –∑–∞ –ø–æ–¥–∞—Ä–æ–∫! –û—á–µ–Ω—å –ø—Ä–∏—è—Ç–Ω–æ –ø–æ–ª—É—á–∏—Ç—å —Ç–∞–∫–æ–π —Å—é—Ä–ø—Ä–∏–∑ ‚ù§Ô∏è",
            "–ü—Ä–µ–∫—Ä–∞—Å–Ω—ã–π —Ñ–∏–ª—å–º –ø–æ—Å–º–æ—Ç—Ä–µ–ª –≤—á–µ—Ä–∞. –í—Å–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥—É—é! üé¨",
            "–£—Ä–∞! –ù–∞–∫–æ–Ω–µ—Ü-—Ç–æ –≤—ã—Ö–æ–¥–Ω—ã–µ! –ú–æ–∂–Ω–æ –æ—Ç–¥–æ—Ö–Ω—É—Ç—å –∏ —Ä–∞—Å—Å–ª–∞–±–∏—Ç—å—Å—è",
            "–ö–ª–∞—Å—Å–Ω–∞—è –º—É–∑—ã–∫–∞ –∏–≥—Ä–∞–µ—Ç –≤ –∫–∞—Ñ–µ. –ê—Ç–º–æ—Å—Ñ–µ—Ä–∞ –ø—Ä–æ—Å—Ç–æ –≤–æ–ª—à–µ–±–Ω–∞—è ‚ú®",
            "–°–µ–≥–æ–¥–Ω—è —É–¥–∞—á–Ω—ã–π –¥–µ–Ω—å! –í—Å–µ –ø–æ–ª—É—á–∞–µ—Ç—Å—è –∫–∞–∫ –Ω–∞–¥–æ üëç",
            "–í–æ—Å—Ö–∏—Ç–∏—Ç–µ–ª—å–Ω—ã–π —É–∂–∏–Ω –≤ —Ä–µ—Å—Ç–æ—Ä–∞–Ω–µ. –®–µ—Ñ-–ø–æ–≤–∞—Ä –º–æ–ª–æ–¥–µ—Ü!",
            "–ó–∞–º–µ—á–∞—Ç–µ–ª—å–Ω–∞—è –∫–Ω–∏–≥–∞! –ß–∏—Ç–∞–µ—Ç—Å—è –Ω–∞ –æ–¥–Ω–æ–º –¥—ã—Ö–∞–Ω–∏–∏ üìö",
            "–ü–æ—Ç—Ä—è—Å–∞—é—â–∏–π –∫–æ–Ω—Ü–µ—Ä—Ç –±—ã–ª –≤—á–µ—Ä–∞! –≠–º–æ—Ü–∏–∏ –∑–∞—à–∫–∞–ª–∏–≤–∞—é—Ç üéµ",
            "–í–µ–ª–∏–∫–æ–ª–µ–ø–Ω—ã–π –æ—Ç–ø—É—Å–∫ –ø—Ä–æ–≤–µ–ª–∏. –ú–æ—Ä–µ, —Å–æ–ª–Ω—Ü–µ, —Å—á–∞—Å—Ç—å–µ! üåä",
            "–°—É–ø–µ—Ä –Ω–æ–≤–æ—Å—Ç—å –ø–æ–ª—É—á–∏–ª —Å–µ–≥–æ–¥–Ω—è! –ñ–∏–∑–Ω—å –Ω–∞–ª–∞–∂–∏–≤–∞–µ—Ç—Å—è",
            "–ü—Ä–µ–∫—Ä–∞—Å–Ω–æ–µ —É—Ç—Ä–æ –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å —Ö–æ—Ä–æ—à–µ–≥–æ –∫–æ—Ñ–µ ‚òï",
            "–†–∞–¥—É—é—Å—å –∫–∞–∂–¥–æ–º—É –Ω–æ–≤–æ–º—É –¥–Ω—é! –ñ–∏–∑–Ω—å –ø—Ä–µ–∫—Ä–∞—Å–Ω–∞",
            "–û—Ç–ª–∏—á–Ω–∞—è —Ä–∞–±–æ—Ç–∞ –∫–æ–º–∞–Ω–¥—ã! –ü—Ä–æ–µ–∫—Ç –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ",
            "–ë–ª–∞–≥–æ–¥–∞—Ä–µ–Ω —Å—É–¥—å–±–µ –∑–∞ —Ç–∞–∫—É—é –∑–∞–º–µ—á–∞—Ç–µ–ª—å–Ω—É—é —Å–µ–º—å—é üë®‚Äçüë©‚Äçüëß‚Äçüë¶",
            "–í–æ—Å—Ö–∏—Ç–∏—Ç–µ–ª—å–Ω—ã–π –¥–µ–Ω—å! –í—Å–µ –∏–¥–µ—Ç –∫–∞–∫ –ø–æ –º–∞—Å–ª—É! üòä",
            "–ö–ª–∞—Å—Å–Ω—ã–π –ø–æ–¥–∞—Ä–æ–∫ –ø–æ–ª—É—á–∏–ª! –°–ø–∞—Å–∏–±–æ –≤—Å–µ–º –¥—Ä—É–∑—å—è–º! üéÅ",
            "–ü–æ—Ç—Ä—è—Å–∞—é—â–∞—è –Ω–æ–≤–æ—Å—Ç—å! –ú–µ—á—Ç–∞ —Å—Ç–∞–Ω–æ–≤–∏—Ç—Å—è —Ä–µ–∞–ª—å–Ω–æ—Å—Ç—å—é! ‚≠ê",
            "–í–µ–ª–∏–∫–æ–ª–µ–ø–Ω–æ–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ! –•–æ—á–µ—Ç—Å—è –ø–µ—Ç—å –∏ —Ç–∞–Ω—Ü–µ–≤–∞—Ç—å! üéâ",
            "–§–∞–Ω—Ç–∞—Å—Ç–∏—á–µ—Å–∫–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç! –ü—Ä–µ–≤–∑–æ—à–µ–ª –≤—Å–µ –æ–∂–∏–¥–∞–Ω–∏—è! üèÜ"
        ]
        
        # –ù–µ–≥–∞—Ç–∏–≤–Ω—ã–µ —Ç–≤–∏—Ç—ã  
        negative_tweets = [
            "–£–∂–∞—Å–Ω–∞—è –ø–æ–≥–æ–¥–∞ –æ–ø—è—Ç—å. –î–æ–∂–¥—å —Ü–µ–ª—ã–π –¥–µ–Ω—å –Ω–µ –ø—Ä–µ–∫—Ä–∞—â–∞–µ—Ç—Å—è üòû",
            "–†–∞–∑–æ—á–∞—Ä–æ–≤–∞–Ω —Ñ–∏–ª—å–º–æ–º. –í—Ä–µ–º—è –ø–æ—Ç—Ä–∞—Ç–∏–ª –∑—Ä—è –Ω–∞ —ç—Ç—É –µ—Ä—É–Ω–¥—É",
            "–û—Ç–≤—Ä–∞—Ç–∏—Ç–µ–ª—å–Ω—ã–π —Å–µ—Ä–≤–∏—Å –≤ —ç—Ç–æ–º –º–∞–≥–∞–∑–∏–Ω–µ. –ë–æ–ª—å—à–µ –Ω–µ –ø–æ–π–¥—É",
            "–ù–∞–¥–æ–µ–ª–æ —ç—Ç–æ –≤—Å–µ! –ö–æ–≥–¥–∞ –∂–µ –ø—Ä–æ–±–ª–µ–º—ã –∑–∞–∫–æ–Ω—á–∞—Ç—Å—è?",
            "–ö–æ—à–º–∞—Ä–Ω—ã–π –¥–µ–Ω—å –Ω–∞ —Ä–∞–±–æ—Ç–µ. –í—Å–µ –∏–¥–µ—Ç –Ω–µ –ø–æ –ø–ª–∞–Ω—É üò°",
            "–ì—Ä—É—Å—Ç–Ω–æ –æ—Ç —Ç–æ–≥–æ —á—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –≤ –º–∏—Ä–µ. –î–µ–ø—Ä–µ—Å—Å–∏—è –∫–∞–∫–∞—è-—Ç–æ",
            "–ù–µ–Ω–∞–≤–∏–∂—É –ø–æ–Ω–µ–¥–µ–ª—å–Ω–∏–∫–∏! –û–ø—è—Ç—å —ç—Ç–∞ —Ä—É—Ç–∏–Ω–∞ –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è",
            "–ó–ª–æ–π –∫–∞–∫ —Å–æ–±–∞–∫–∞ —Å–µ–≥–æ–¥–Ω—è. –í—Å–µ –±–µ—Å–∏—Ç –∏ —Ä–∞–∑–¥—Ä–∞–∂–∞–µ—Ç",
            "–ü—Ä–æ–≤–∞–ª—å–Ω—ã–π –ø—Ä–æ–µ–∫—Ç –ø–æ–ª—É—á–∏–ª—Å—è. –í—Å–µ —É—Å–∏–ª–∏—è –≤–ø—É—Å—Ç—É—é üíî",
            "–†–∞—Å—Å—Ç—Ä–æ–µ–Ω –ø–æ–≤–µ–¥–µ–Ω–∏–µ–º –¥—Ä—É–∑–µ–π. –ü–æ–¥–≤–µ–ª–∏ –≤ —Ç—Ä—É–¥–Ω—É—é –º–∏–Ω—É—Ç—É",
            "–£–∂–∞—Å–Ω–æ —É—Å—Ç–∞–ª –æ—Ç —ç—Ç–æ–π —Å—É–µ—Ç—ã. –•–æ—á–µ—Ç—Å—è –≤—Å–µ –±—Ä–æ—Å–∏—Ç—å",
            "–ü–ª–æ—Ö–∏–µ –Ω–æ–≤–æ—Å—Ç–∏ –Ω–µ –ø—Ä–µ–∫—Ä–∞—â–∞—é—Ç—Å—è. –û–¥–Ω–æ –∑–∞ –¥—Ä—É–≥–∏–º",
            "–†–∞–∑–æ—á–∞—Ä–æ–≤–∞–Ω —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ —ç–∫–∑–∞–º–µ–Ω–∞. –ì–æ—Ç–æ–≤–∏–ª—Å—è –∑—Ä—è –≤–∏–¥–∏–º–æ",
            "–ü—Ä–æ—Ç–∏–≤–Ω—ã–π —á–µ–ª–æ–≤–µ–∫ –ø–æ–ø–∞–ª—Å—è —Å–µ–≥–æ–¥–Ω—è. –ò—Å–ø–æ—Ä—Ç–∏–ª –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ",
            "–ú–µ—Ä–∑–∫–∞—è –µ–¥–∞ –±—ã–ª–∞ –≤ —Å—Ç–æ–ª–æ–≤–æ–π. –ï—Å—Ç—å –Ω–µ–≤–æ–∑–º–æ–∂–Ω–æ –±—ã–ª–æ",
            "–ö–æ—à–º–∞—Ä –∫–∞–∫–æ–π-—Ç–æ! –í—Å–µ –≤–∞–ª–∏—Ç—Å—è –∏–∑ —Ä—É–∫! üò¢",
            "–ù–µ–Ω–∞–≤–∏–∂—É —ç—Ç—É —Ä–∞–±–æ—Ç—É! –ö–∞–∂–¥—ã–π –¥–µ–Ω—å –∫–∞–∫ –Ω–∞–∫–∞–∑–∞–Ω–∏–µ! üíÄ",
            "–û—Ç–≤—Ä–∞—Ç–∏—Ç–µ–ª—å–Ω—ã–µ –ª—é–¥–∏ –≤–æ–∫—Ä—É–≥! –•–æ—á–µ—Ç—Å—è —Å–∫—Ä—ã—Ç—å—Å—è –æ—Ç –≤—Å–µ—Ö! üò§",
            "–ü—Ä–æ–≤–∞–ª—å–Ω—ã–π –¥–µ–Ω—å! –ù–∏—á–µ–≥–æ –Ω–µ –ø–æ–ª—É—á–∞–µ—Ç—Å—è –∫–∞–∫ –Ω–∞–¥–æ! üëé",
            "–£–∂–∞—Å–Ω–æ–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ! –í—Å–µ —Ä–∞–∑–¥—Ä–∞–∂–∞–µ—Ç –∏ –±–µ—Å–∏—Ç! üò†"
        ]
        
        # –ù–µ–π—Ç—Ä–∞–ª—å–Ω—ã–µ —Ç–≤–∏—Ç—ã
        neutral_tweets = [
            "–°–µ–≥–æ–¥–Ω—è —Å—Ä–µ–¥–∞. –ü–æ–ª–æ–≤–∏–Ω–∞ –Ω–µ–¥–µ–ª–∏ —É–∂–µ –ø—Ä–æ—à–ª–∞.",
            "–ß–∏—Ç–∞—é –Ω–æ–≤–æ—Å—Ç–∏ –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ. –ú–Ω–æ–≥–æ —Ä–∞–∑–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏.",
            "–ü–ª–∞–Ω–∏—Ä—É—é –ø–æ–µ–∑–¥–∫—É –Ω–∞ –≤—ã—Ö–æ–¥–Ω—ã–µ. –ï—â–µ –≤—ã–±–∏—Ä–∞—é –∫—É–¥–∞ –ø–æ–µ—Ö–∞—Ç—å.",
            "–†–∞–±–æ—Ç–∞—é –Ω–∞–¥ –ø—Ä–æ–µ–∫—Ç–æ–º. –û—Å—Ç–∞–ª–æ—Å—å –µ—â–µ –º–Ω–æ–≥–æ –∑–∞–¥–∞—á.",
            "–°–º–æ—Ç—Ä—é —Å–µ—Ä–∏–∞–ª –ø–æ –≤–µ—á–µ—Ä–∞–º. –ò–Ω—Ç–µ—Ä–µ—Å–Ω—ã–π —Å—é–∂–µ—Ç —Ä–∞–∑–≤–∏–≤–∞–µ—Ç—Å—è.",
            "–í—Å—Ç—Ä–µ—Ç–∏–ª—Å—è —Å –¥—Ä—É–∑—å—è–º–∏ –≤—á–µ—Ä–∞. –û–±—ã—á–Ω–æ–µ –æ–±—â–µ–Ω–∏–µ, –Ω–∏—á–µ–≥–æ –æ—Å–æ–±–µ–Ω–Ω–æ–≥–æ.",
            "–ü–æ–∫—É–ø–∞–ª –ø—Ä–æ–¥—É–∫—Ç—ã –≤ –º–∞–≥–∞–∑–∏–Ω–µ. –¶–µ–Ω—ã –∫–∞–∫ –≤—Å–µ–≥–¥–∞ —Ä–∞—Å—Ç—É—Ç.",
            "–ò–∑—É—á–∞—é –Ω–æ–≤—É—é –ø—Ä–æ–≥—Ä–∞–º–º—É –¥–ª—è —Ä–∞–±–æ—Ç—ã. –†–∞–∑–±–∏—Ä–∞—é—Å—å –ø–æ—Ç–∏—Ö–æ–Ω—å–∫—É.",
            "–°–ª—É—à–∞—é –ø–æ–¥–∫–∞—Å—Ç –ø—Ä–æ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏. –ü–æ–∑–Ω–∞–≤–∞—Ç–µ–ª—å–Ω–æ –≤ —Ü–µ–ª–æ–º.",
            "–£–±–∏—Ä–∞—é—Å—å –¥–æ–º–∞ –ø–æ –≤—ã—Ö–æ–¥–Ω—ã–º. –†—É—Ç–∏–Ω–Ω—ã–µ –¥–æ–º–∞—à–Ω–∏–µ –¥–µ–ª–∞.",
            "–ì–æ—Ç–æ–≤–ª—é—Å—å –∫ –ø—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏–∏ –Ω–∞ —Ä–∞–±–æ—Ç–µ. –°–æ–±–∏—Ä–∞—é –º–∞—Ç–µ—Ä–∏–∞–ª.",
            "–ß–∏—Ç–∞—é –∫–Ω–∏–≥—É –ø–µ—Ä–µ–¥ —Å–Ω–æ–º. –ü–æ–º–æ–≥–∞–µ—Ç —Ä–∞—Å—Å–ª–∞–±–∏—Ç—å—Å—è.",
            "–ü—Ä–æ–≤–µ—Ä—è—é –ø–æ—á—Ç—É —É—Ç—Ä–æ–º. –ù–µ—Å–∫–æ–ª—å–∫–æ –ø–∏—Å–µ–º –Ω–∞–∫–æ–ø–∏–ª–æ—Å—å.",
            "–ì—É–ª—è—é –≤ –ø–∞—Ä–∫–µ –∏–Ω–æ–≥–¥–∞. –°–≤–µ–∂–∏–π –≤–æ–∑–¥—É—Ö –ø–æ–ª–µ–∑–µ–Ω –¥–ª—è –∑–¥–æ—Ä–æ–≤—å—è.",
            "–ü–ª–∞–Ω–∏—Ä—É—é –æ—Ç–ø—É—Å–∫ –Ω–∞ —Å–ª–µ–¥—É—é—â–∏–π –≥–æ–¥. –†–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—é –≤–∞—Ä–∏–∞–Ω—Ç—ã.",
            "–û–±—ã—á–Ω—ã–π —Ä–∞–±–æ—á–∏–π –¥–µ–Ω—å. –ù–∏—á–µ–≥–æ –æ—Å–æ–±–µ–Ω–Ω–æ–≥–æ –Ω–µ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç.",
            "–ó–∞–≤—Ç—Ä–∞ –≤–∞–∂–Ω–∞—è –≤—Å—Ç—Ä–µ—á–∞. –ì–æ—Ç–æ–≤–ª—é—Å—å –∫ –ø—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏–∏.",
            "–ü–æ–∫—É–ø–∞—é –ø—Ä–æ–¥—É–∫—Ç—ã –Ω–∞ –Ω–µ–¥–µ–ª—é. –°–æ—Å—Ç–∞–≤–ª—è—é —Å–ø–∏—Å–æ–∫ –∑–∞—Ä–∞–Ω–µ–µ.",
            "–°–º–æ—Ç—Ä—é –ø—Ä–æ–≥–Ω–æ–∑ –ø–æ–≥–æ–¥—ã. –ü–ª–∞–Ω–∏—Ä—É—é –æ–¥–µ–∂–¥—É –Ω–∞ –∑–∞–≤—Ç—Ä–∞.",
            "–ß–∏—Ç–∞—é —Å—Ç–∞—Ç—å–∏ –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ. –ò–∑—É—á–∞—é –Ω–æ–≤—É—é —Ç–µ–º—É."
        ]
        
        # –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–≤–∏—Ç—ã (speech/skip)
        speech_tweets = [
            "RT @user: –í–∞–∂–Ω–æ–µ –æ–±—ä—è–≤–ª–µ–Ω–∏–µ –¥–ª—è –≤—Å–µ—Ö –ø–æ–¥–ø–∏—Å—á–∏–∫–æ–≤",
            "–û—Ñ–∏—Ü–∏–∞–ª—å–Ω–æ–µ –∑–∞—è–≤–ª–µ–Ω–∏–µ –∫–æ–º–ø–∞–Ω–∏–∏ –ø–æ –ø–æ–≤–æ–¥—É –∏–∑–º–µ–Ω–µ–Ω–∏–π",
            "–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –æ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö —Ä–∞–±–æ—Ç–∞—Ö",
            "–£–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ –æ –ø–ª–∞–Ω–æ–≤–æ–º –æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏–∏ —Å–∏—Å—Ç–µ–º—ã",
            "–ü—Ä–µ—Å—Å-—Ä–µ–ª–∏–∑ –æ –Ω–æ–≤—ã—Ö –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—è—Ö —Å–µ—Ä–≤–∏—Å–∞",
            "–û—Ñ–∏—Ü–∏–∞–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ—Ç –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ü–∏–∏",
            "–¢–µ—Ö–Ω–∏—á–µ—Å–∫–æ–µ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π",
            "–†–µ–≥–ª–∞–º–µ–Ω—Ç–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –æ —Ä–∞–±–æ—Ç–µ —Å–µ—Ä–≤–∏—Å–∞"
        ]
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ –≤ —Ñ–æ—Ä–º–∞—Ç–µ RuSentiment
        for tweet in positive_tweets:
            data.append({'text': tweet, 'label': 'positive'})
        
        for tweet in negative_tweets:
            data.append({'text': tweet, 'label': 'negative'})
        
        for tweet in neutral_tweets:
            data.append({'text': tweet, 'label': 'neutral'})
        
        for tweet in speech_tweets:
            data.append({'text': tweet, 'label': 'speech'})
        
        # –î–æ–±–∞–≤–ª—è–µ–º –±–æ–ª—å—à–µ –¥–∞–Ω–Ω—ã—Ö —Å –≤–∞—Ä–∏–∞—Ü–∏—è–º–∏
        variations = []
        for item in data[:40]:  # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 40 –¥–ª—è –≤–∞—Ä–∏–∞—Ü–∏–π
            original_text = item['text']
            label = item['label']
            
            # –°–æ–∑–¥–∞–µ–º –≤–∞—Ä–∏–∞—Ü–∏–∏
            if label == 'positive':
                variations.extend([
                    {'text': original_text + " üòä", 'label': label},
                    {'text': original_text.upper(), 'label': label},
                    {'text': original_text + "!!!", 'label': label},
                    {'text': "–î–∞! " + original_text, 'label': label}
                ])
            elif label == 'negative':
                variations.extend([
                    {'text': original_text + " üò¢", 'label': label},
                    {'text': original_text.replace('.', '...'), 'label': label},
                    {'text': "–ë–ª–∏–Ω, " + original_text.lower(), 'label': label},
                    {'text': original_text + " –ë–µ—Å–∏—Ç!", 'label': label}
                ])
            elif label == 'neutral':
                variations.extend([
                    {'text': original_text + " –í –æ–±—â–µ–º.", 'label': label},
                    {'text': "–ö—Å—Ç–∞—Ç–∏, " + original_text.lower(), 'label': label}
                ])
        
        data.extend(variations)
        
        df = pd.DataFrame(data)
        print(f"   –°–æ–∑–¥–∞–Ω–æ {len(df)} –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–µ–º–æ-RuSentiment")
        return df
    
    def download_additional_datasets(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤"""
        datasets = {}
        
        # 1. Russian Movie Reviews (–ø–æ–ø—ã—Ç–∫–∞)
        try:
            reviews_url = "https://raw.githubusercontent.com/sismetanin/rureviews-dataset/master/data/movies_small.csv"
            response = requests.get(reviews_url, timeout=30)
            if response.status_code == 200:
                reviews_df = pd.read_csv(io.StringIO(response.text))
                datasets['movie_reviews'] = self._process_movie_reviews(reviews_df)
                print(f"‚úÖ –û—Ç–∑—ã–≤—ã –Ω–∞ —Ñ–∏–ª—å–º—ã –∑–∞–≥—Ä—É–∂–µ–Ω—ã ({len(datasets['movie_reviews'])} –∑–∞–ø–∏—Å–µ–π)")
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –æ—Ç–∑—ã–≤–æ–≤: {e}")
        
        # 2. Russian Toxic Comments (–¥–µ–º–æ)
        try:
            toxic_data = self.create_toxic_comments_demo()
            datasets['toxic_comments'] = toxic_data
            print(f"‚úÖ –î–µ–º–æ —Ç–æ–∫—Å–∏—á–Ω—ã—Ö –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤ —Å–æ–∑–¥–∞–Ω–æ ({len(toxic_data)} –∑–∞–ø–∏—Å–µ–π)")
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —Ç–æ–∫—Å–∏—á–Ω—ã—Ö –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤: {e}")
        
        # 3. VK Comments (–¥–µ–º–æ)
        try:
            vk_data = self.create_vk_comments_demo()
            datasets['vk_comments'] = vk_data
            print(f"‚úÖ –î–µ–º–æ VK –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤ —Å–æ–∑–¥–∞–Ω–æ ({len(vk_data)} –∑–∞–ø–∏—Å–µ–π)")
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è VK –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤: {e}")
        
        return datasets
    
    def create_toxic_comments_demo(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ –¥–µ–º–æ-–¥–∞—Ç–∞—Å–µ—Ç–∞ —Ç–æ–∫—Å–∏—á–Ω—ã—Ö –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤"""
        data = []
        
        # –¢–æ–∫—Å–∏—á–Ω—ã–µ (–Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–µ)
        toxic_comments = [
            "–ê–≤—Ç–æ—Ä —Å—Ç–∞—Ç—å–∏ –ø–æ–ª–Ω—ã–π –∏–¥–∏–æ—Ç, –Ω–∏—á–µ–≥–æ –Ω–µ –ø–æ–Ω–∏–º–∞–µ—Ç",
            "–ë—Ä–µ–¥ –∫–∞–∫–æ–π-—Ç–æ –Ω–∞–ø–∏—Å–∞–ª–∏, –ª—É—á—à–µ –±—ã –º–æ–ª—á–∞–ª–∏",
            "–¢—É–ø—ã–µ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ –æ—Ç —Ç—É–ø—ã—Ö –ª—é–¥–µ–π, –∫–∞–∫ –≤—Å–µ–≥–¥–∞",
            "–ù–µ–Ω–∞–≤–∏–∂—É —Ç–∞–∫–∏—Ö –∞–≤—Ç–æ—Ä–æ–≤, —Ç–æ–ª—å–∫–æ –≤—Ä–µ–º—è —Ç—Ä–∞—Ç—è—Ç",
            "–ü–æ–ª–Ω–∞—è –µ—Ä—É–Ω–¥–∞ –∏ —á—É—à—å —Å–æ–±–∞—á—å—è, –Ω–µ —á–∏—Ç–∞–π—Ç–µ —ç—Ç–æ",
            "–û—Ç–≤—Ä–∞—Ç–∏—Ç–µ–ª—å–Ω–∞—è —Å—Ç–∞—Ç—å—è! –ê–≤—Ç–æ—Ä –Ω–µ –≤ —Ç–µ–º–µ —Å–æ–≤—Å–µ–º!",
            "–î—É—Ä–∞—Ü–∫–∏–π –∫–æ–Ω—Ç–µ–Ω—Ç! –ö—Ç–æ —ç—Ç–æ –≤–æ–æ–±—â–µ —á–∏—Ç–∞–µ—Ç?",
            "–ú–µ—Ä–∑–∫–∏–µ –º—ã—Å–ª–∏ –∞–≤—Ç–æ—Ä–∞! –ü—Ä–æ—Ç–∏–≤–Ω–æ —á–∏—Ç–∞—Ç—å —Ç–∞–∫–æ–µ!",
            "–£–∂–∞—Å–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ! –°—Ç—ã–¥–Ω–æ –ø—É–±–ª–∏–∫–æ–≤–∞—Ç—å —Ç–∞–∫–æ–µ!",
            "–ì–ª—É–ø–æ—Å—Ç—å –Ω–µ–≤–µ—Ä–æ—è—Ç–Ω–∞—è! –ê–≤—Ç–æ—Ä —Å–æ–≤—Å–µ–º –ø–æ–≥–ª—É–ø–µ–ª!"
        ]
        
        # –ù–µ–π—Ç—Ä–∞–ª—å–Ω—ã–µ (–Ω–µ —Ç–æ–∫—Å–∏—á–Ω—ã–µ)
        neutral_comments = [
            "–ò–Ω—Ç–µ—Ä–µ—Å–Ω–∞—è —Å—Ç–∞—Ç—å—è, —Å–ø–∞—Å–∏–±–æ –∞–≤—Ç–æ—Ä—É –∑–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é",
            "–ü–æ–∑–Ω–∞–≤–∞—Ç–µ–ª—å–Ω–æ, –±—É–¥—É –∏–∑—É—á–∞—Ç—å —Ç–µ–º—É –¥–∞–ª—å—à–µ",
            "–°–æ–≥–ª–∞—Å–µ–Ω —Å –Ω–µ–∫–æ—Ç–æ—Ä—ã–º–∏ –º–æ–º–µ–Ω—Ç–∞–º–∏ –≤ —Å—Ç–∞—Ç—å–µ",
            "–•–æ—Ä–æ—à–∞—è –ø–æ–¥–∞—á–∞ –º–∞—Ç–µ—Ä–∏–∞–ª–∞, –≤—Å–µ –ø–æ–Ω—è—Ç–Ω–æ",
            "–ü–æ–ª–µ–∑–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –¥–ª—è —Ä–∞–∑–º—ã—à–ª–µ–Ω–∏—è",
            "–ß–∏—Ç–∞–ª —Å –∏–Ω—Ç–µ—Ä–µ—Å–æ–º, –º–Ω–æ–≥–æ –Ω–æ–≤–æ–≥–æ —É–∑–Ω–∞–ª",
            "–ö–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç, –∞–≤—Ç–æ—Ä –º–æ–ª–æ–¥–µ—Ü",
            "–ü–æ–ª–µ–∑–Ω—ã–µ —Å–æ–≤–µ—Ç—ã, –ø–æ–ø—Ä–æ–±—É—é –ø—Ä–∏–º–µ–Ω–∏—Ç—å",
            "–•–æ—Ä–æ—à–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è",
            "–ë–ª–∞–≥–æ–¥–∞—Ä—é –∑–∞ —Å—Ç–∞—Ç—å—é, –±—ã–ª–æ –∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ"
        ]
        
        for comment in toxic_comments:
            data.append({'text': comment, 'label': 'negative'})
        
        for comment in neutral_comments:
            data.append({'text': comment, 'label': 'positive'})
        
        return pd.DataFrame(data)
    
    def create_vk_comments_demo(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ –¥–µ–º–æ-–¥–∞—Ç–∞—Å–µ—Ç–∞ VK –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤"""
        data = []
        
        # –ü–æ–∑–∏—Ç–∏–≤–Ω—ã–µ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏
        positive_vk = [
            "–ö–ª–∞—Å—Å! –û—á–µ–Ω—å –∫—Ä—É—Ç–æ–π –ø–æ—Å—Ç! üëç",
            "–°–ø–∞—Å–∏–±–æ –∑–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é, –æ—á–µ–Ω—å –ø–æ–ª–µ–∑–Ω–æ!",
            "–°—É–ø–µ—Ä —Ñ–æ—Ç–æ! –ì–¥–µ —Ç–∞–∫–∞—è –∫—Ä–∞—Å–æ—Ç–∞?",
            "–ú–æ–ª–æ–¥–µ—Ü! –¢–∞–∫ –¥–µ—Ä–∂–∞—Ç—å!",
            "–û–±–æ–∂–∞—é —Ç–∞–∫–∏–µ –ø–æ—Å—Ç—ã! –ï—â–µ –±–æ–ª—å—à–µ!",
            "–í–æ—Å—Ç–æ—Ä–≥! –ê–≤—Ç–æ—Ä —Ç–∞–ª–∞–Ω—Ç! üî•",
            "–ü–æ—Ç—Ä—è—Å–∞—é—â–µ! –õ—É—á—à–∏–π –ø–æ—Å—Ç –∑–∞ –Ω–µ–¥–µ–ª—é! ‚≠ê",
            "–ö—Ä—É—Ç–æ! –ü–æ–¥–µ–ª–∏—Å—å –µ—â–µ —Ç–∞–∫–∏–º–∏! üòç",
            "–ì–µ–Ω–∏–∞–ª—å–Ω–æ! –ö–∞–∫ —Ç—ã —ç—Ç–æ –ø—Ä–∏–¥—É–º–∞–ª? üí°",
            "–®–∏–∫–∞—Ä–Ω–æ! –ü—Ä–æ–¥–æ–ª–∂–∞–π –≤ —Ç–æ–º –∂–µ –¥—É—Ö–µ! ‚ú®"
        ]
        
        # –ù–µ–≥–∞—Ç–∏–≤–Ω—ã–µ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏
        negative_vk = [
            "–§—É, –∫–∞–∫–∞—è –µ—Ä—É–Ω–¥–∞. –ù–µ –Ω—Ä–∞–≤–∏—Ç—Å—è —Å–æ–≤—Å–µ–º",
            "–°–∫—É—á–Ω–æ –∏ –Ω–µ–∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ, –Ω–µ —á–∏—Ç–∞–π—Ç–µ",
            "–ê–≤—Ç–æ—Ä, —Ç—ã –æ —á–µ–º –≤–æ–æ–±—â–µ? –ë—Ä–µ–¥ –ø–æ–ª–Ω—ã–π",
            "–û—Ç–ø–∏—Å—ã–≤–∞—é—Å—å, –Ω–∞–¥–æ–µ–ª–∏ —Ç–∞–∫–∏–µ –ø–æ—Å—Ç—ã",
            "–£–∂–∞—Å–Ω–æ! –•—É–∂–µ –Ω–µ –≤–∏–¥–µ–ª!",
            "–¢—É–ø–æ—Å—Ç—å! –ó–∞—á–µ–º –ø–æ—Å—Ç–∏—à—å —Ç–∞–∫–æ–µ? üëé",
            "–û—Ç–≤—Ä–∞—Ç–Ω–æ! –ì–ª–∞–∑–∞ –±–æ–ª—è—Ç –æ—Ç —ç—Ç–æ–≥–æ! ü§Æ",
            "–ú–µ—Ä–∑–∫–æ! –£–¥–∞–ª–∏ –Ω–µ–º–µ–¥–ª–µ–Ω–Ω–æ! üò°",
            "–ì–∞–¥–æ—Å—Ç—å! –°—Ç—ã–¥–Ω–æ –∑–∞ —Ç–∞–∫–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç! üíÄ",
            "–ö–æ—à–º–∞—Ä! –ê–≤—Ç–æ—Ä —Å–æ–≤—Å–µ–º –¥–µ–≥—Ä–∞–¥–∏—Ä–æ–≤–∞–ª! üòû"
        ]
        
        # –ù–µ–π—Ç—Ä–∞–ª—å–Ω—ã–µ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏
        neutral_vk = [
            "–í–∏–¥–µ–ª —É–∂–µ —Ç–∞–∫–æ–µ –≥–¥–µ-—Ç–æ —Ä–∞–Ω—å—à–µ",
            "–ù–æ—Ä–º–∞–ª—å–Ω–æ, –Ω–∏—á–µ–≥–æ –æ—Å–æ–±–µ–Ω–Ω–æ–≥–æ",
            "–ß–∏—Ç–∞–ª, –æ–±—ã—á–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è",
            "–¢–∞–∫ —Å–µ–±–µ –ø–æ—Å—Ç, –º–æ–∂–Ω–æ –±—ã–ª–æ –ª—É—á—à–µ",
            "–°—Ä–µ–¥–Ω–µ–Ω—å–∫–æ –ø–æ–ª—É—á–∏–ª–æ—Å—å —É –∞–≤—Ç–æ—Ä–∞",
            "–û–±—ã—á–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç, –∫–∞–∫ –≤—Å–µ–≥–¥–∞",
            "–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –ø–æ—Å—Ç, –±–µ–∑ –∏–∑—ã—Å–∫–æ–≤",
            "–¢–∏–ø–∏—á–Ω–∞—è –ø–æ–¥–∞—á–∞ –º–∞—Ç–µ—Ä–∏–∞–ª–∞",
            "–†—è–¥–æ–≤–æ–π –ø–æ—Å—Ç –≤ –ª–µ–Ω—Ç–µ",
            "–ü—Ä–∏–≤—ã—á–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç –æ—Ç –∞–≤—Ç–æ—Ä–∞"
        ]
        
        for comment in positive_vk:
            data.append({'text': comment, 'label': 'positive'})
        
        for comment in negative_vk:
            data.append({'text': comment, 'label': 'negative'})
        
        for comment in neutral_vk:
            data.append({'text': comment, 'label': 'neutral'})
        
        return pd.DataFrame(data)
    
    def _process_movie_reviews(self, df):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—Ç–∑—ã–≤–æ–≤ –Ω–∞ —Ñ–∏–ª—å–º—ã"""
        try:
            if 'review_text' in df.columns and 'rating' in df.columns:
                df = df[['review_text', 'rating']].copy()
                df.columns = ['text', 'rating']
                
                def rating_to_sentiment(rating):
                    try:
                        rating = float(rating)
                        if rating >= 7:
                            return 'positive'
                        elif rating <= 4:
                            return 'negative'
                        else:
                            return 'neutral'
                    except:
                        return 'neutral'
                
                df['label'] = df['rating'].apply(rating_to_sentiment)
                df = df[['text', 'label']]
                df = df[df['text'].notna() & (df['text'] != '')]
                return df
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ—Ç–∑—ã–≤–æ–≤: {e}")
        
        return pd.DataFrame(columns=['text', 'label'])
    
    def create_extended_synthetic_dataset(self, num_samples: int = 50000) -> pd.DataFrame:
        """–°–æ–∑–¥–∞–Ω–∏–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–≥–æ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞"""
        print(f"üîÑ –°–æ–∑–¥–∞–Ω–∏–µ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ ({num_samples} –ø—Ä–∏–º–µ—Ä–æ–≤)...")
        
        data = []
        
        # –£–ª—É—á—à–µ–Ω–Ω—ã–µ —à–∞–±–ª–æ–Ω—ã –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –¥–æ–º–µ–Ω–æ–≤
        domains = {
            'ecommerce': {
                'positive': [
                    "–û—Ç–ª–∏—á–Ω—ã–π {product}! –î–æ—Å—Ç–∞–≤–∫–∞ –±—ã—Å—Ç—Ä–∞—è, –∫–∞—á–µ—Å—Ç–≤–æ —Å—É–ø–µ—Ä!",
                    "–ó–∞–∫–∞–∑—ã–≤–∞–ª {product}, –ø—Ä–∏—à–µ–ª —Ç–æ—á–Ω–æ –≤ —Å—Ä–æ–∫. –û—á–µ–Ω—å –¥–æ–≤–æ–ª–µ–Ω!",
                    "–†–µ–∫–æ–º–µ–Ω–¥—É—é {product} –≤—Å–µ–º! –¶–µ–Ω–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –∫–∞—á–µ—Å—Ç–≤—É!",
                    "–°—É–ø–µ—Ä {product}! –£–∂–µ –≤—Ç–æ—Ä–æ–π —Ä–∞–∑ –∑–∞–∫–∞–∑—ã–≤–∞—é, –≤—Å–µ –æ—Ç–ª–∏—á–Ω–æ!",
                    "–ö–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π {product}, —É–ø–∞–∫–æ–≤–∫–∞ –∞–∫–∫—É—Ä–∞—Ç–Ω–∞—è, —Å–ø–∞—Å–∏–±–æ!",
                    "–í–æ—Å—Ö–∏—Ç–∏—Ç–µ–ª—å–Ω—ã–π {product}! –ü—Ä–µ–≤–∑–æ—à–µ–ª –≤—Å–µ –æ–∂–∏–¥–∞–Ω–∏—è!",
                    "–ü–æ—Ç—Ä—è—Å–∞—é—â–∏–π {product}! –õ—É—á—à–∞—è –ø–æ–∫—É–ø–∫–∞ –∑–∞ –≥–æ–¥!",
                    "–ö–ª–∞—Å—Å–Ω—ã–π {product}! –í—Å–µ–º –¥—Ä—É–∑—å—è–º —Ä–µ–∫–æ–º–µ–Ω–¥—É—é!",
                    "–®–∏–∫–∞—Ä–Ω—ã–π {product}! –°—Ç–æ–∏—Ç –∫–∞–∂–¥–æ–π –∫–æ–ø–µ–π–∫–∏!",
                    "–í–µ–ª–∏–∫–æ–ª–µ–ø–Ω—ã–π {product}! –ë—É–¥—É –∑–∞–∫–∞–∑—ã–≤–∞—Ç—å –µ—â–µ!"
                ],
                'negative': [
                    "–£–∂–∞—Å–Ω—ã–π {product}! –ù–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –æ–ø–∏—Å–∞–Ω–∏—é —Å–æ–≤—Å–µ–º!",
                    "–†–∞–∑–æ—á–∞—Ä–æ–≤–∞–Ω {product}. –î–µ–Ω—å–≥–∏ –≤—ã–±—Ä–æ—à–µ–Ω—ã –Ω–∞ –≤–µ—Ç–µ—Ä!",
                    "–ù–µ –ø–æ–∫—É–ø–∞–π—Ç–µ —ç—Ç–æ—Ç {product}! –ü–æ–ª–Ω—ã–π –±—Ä–∞–∫ –ø—Ä–∏—à–µ–ª!",
                    "–û—Ç–≤—Ä–∞—Ç–∏—Ç–µ–ª—å–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ {product}. –í–µ—Ä–Ω—É –æ–±—Ä–∞—Ç–Ω–æ!",
                    "–û–±–º–∞–Ω! {product} —Å–æ–≤—Å–µ–º –Ω–µ —Ç–∞–∫–æ–π –∫–∞–∫ –Ω–∞ —Ñ–æ—Ç–æ!",
                    "–ö–æ—à–º–∞—Ä–Ω—ã–π {product}! –•—É–∂–µ –Ω–µ –≤–∏–¥–µ–ª –Ω–∏–∫–æ–≥–¥–∞!",
                    "–ü—Ä–æ–≤–∞–ª—å–Ω—ã–π {product}! –ü–æ—Ç—Ä–∞—á–µ–Ω–Ω—ã–µ –¥–µ–Ω—å–≥–∏ –∂–∞–ª–∫–æ!",
                    "–ú–µ—Ä–∑–∫–∏–π {product}! –ö–∞–∫ —Ç–∞–∫–æ–µ –º–æ–∂–Ω–æ –ø—Ä–æ–¥–∞–≤–∞—Ç—å?",
                    "–ù–∏–∫—É–¥—ã—à–Ω—ã–π {product}! –ü–æ–ª–Ω–æ–µ —Ä–∞–∑–æ—á–∞—Ä–æ–≤–∞–Ω–∏–µ!",
                    "–î—Ä—è–Ω–Ω–æ–π {product}! –ù–µ —Å–æ–≤–µ—Ç—É—é –Ω–∏–∫–æ–º—É!"
                ],
                'neutral': [
                    "–ó–∞–∫–∞–∑–∞–ª {product}, –ø—Ä–∏—à–µ–ª –∫–∞–∫ –≤ –æ–ø–∏—Å–∞–Ω–∏–∏.",
                    "–û–±—ã—á–Ω—ã–π {product} –∑–∞ —Å–≤–æ–∏ –¥–µ–Ω—å–≥–∏, –±–µ–∑ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–µ–π.",
                    "–ü–æ–ª—å–∑—É—é—Å—å {product} –Ω–µ–¥–µ–ª—é, –ø–æ–∫–∞ –Ω–æ—Ä–º–∞–ª—å–Ω–æ.",
                    "–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π {product}, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç —Ü–µ–Ω–µ.",
                    "–°—Ä–µ–¥–Ω–µ–Ω—å–∫–∏–π {product}, –µ—Å—Ç—å –∏ –ø–æ–ª—É—á—à–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã.",
                    "–¢–∏–ø–∏—á–Ω—ã–π {product} –≤ —Å–≤–æ–µ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏.",
                    "–†—è–¥–æ–≤–æ–π {product} –±–µ–∑ –∏–∑–ª–∏—à–µ—Å—Ç–≤.",
                    "–ü—Ä–∏–µ–º–ª–µ–º—ã–π {product} –¥–ª—è –±–∞–∑–æ–≤—ã—Ö –Ω—É–∂–¥.",
                    "–ù–µ–ø–ª–æ—Ö–æ–π {product}, –Ω–æ –º–æ–∂–Ω–æ –Ω–∞–π—Ç–∏ –ª—É—á—à–µ.",
                    "–£–¥–æ–≤–ª–µ—Ç–≤–æ—Ä–∏—Ç–µ–ª—å–Ω—ã–π {product} –≤ —Ü–µ–ª–æ–º."
                ]
            },
            'social': {
                'positive': [
                    "–ö–ª–∞—Å—Å–Ω—ã–π –ø–æ—Å—Ç! {emotion} –æ—Ç –ø—Ä–æ—á–∏—Ç–∞–Ω–Ω–æ–≥–æ! üî•",
                    "–°—É–ø–µ—Ä –∫–æ–Ω—Ç–µ–Ω—Ç! –ê–≤—Ç–æ—Ä {compliment}, –ø—Ä–æ–¥–æ–ª–∂–∞–π!",
                    "–í–æ—Å—Ç–æ—Ä–≥! {emotion} —Ç–∞–∫–∏–º–∏ –ø–æ—Å—Ç–∞–º–∏! ‚ù§Ô∏è",
                    "–ú–æ–ª–æ–¥–µ—Ü –∞–≤—Ç–æ—Ä! {compliment} –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π!",
                    "–ü–æ—Ç—Ä—è—Å–∞—é—â–µ! {emotion} –∫–∞–∂–¥—ã–º —Å–ª–æ–≤–æ–º! ‚ú®",
                    "–ì–µ–Ω–∏–∞–ª—å–Ω–æ! {emotion} –æ—Ç —Ç–∞–∫–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞! üí°",
                    "–®–∏–∫–∞—Ä–Ω–æ! –ê–≤—Ç–æ—Ä {compliment} –º–∞—Ç–µ—Ä–∏–∞–ª–æ–º! üåü",
                    "–§–∞–Ω—Ç–∞—Å—Ç–∏–∫–∞! {emotion} –ø–æ–¥–∞—á–µ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏! üöÄ",
                    "–í–æ—Å—Ö–∏—Ç–∏—Ç–µ–ª—å–Ω–æ! {compliment} —Ä–∞–±–æ—Ç–æ–π! üëè",
                    "–ü—Ä–µ–∫—Ä–∞—Å–Ω–æ! {emotion} –∫–∞—á–µ—Å—Ç–≤–æ–º –∫–æ–Ω—Ç–µ–Ω—Ç–∞! üéâ"
                ],
                'negative': [
                    "–ï—Ä—É–Ω–¥–∞ –ø–æ–ª–Ω–∞—è! {emotion} –≤—Ä–µ–º—è –Ω–∞ —ç—Ç–æ!",
                    "–°–∫—É—á–Ω–æ –∏ –Ω–µ–∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ! –ê–≤—Ç–æ—Ä {criticism}!",
                    "–ë—Ä–µ–¥ –∫–∞–∫–æ–π-—Ç–æ! {emotion} –æ—Ç —Ç–∞–∫–∏—Ö –ø–æ—Å—Ç–æ–≤!",
                    "–ù–µ –Ω—Ä–∞–≤–∏—Ç—Å—è —Å–æ–≤—Å–µ–º! {criticism} –∫–æ–Ω—Ç–µ–Ω—Ç!",
                    "–£–∂–∞—Å–Ω–æ! {emotion} —Ç–∞–∫–∏–µ –ø–æ—Å—Ç—ã –≤–∏–¥–µ—Ç—å!",
                    "–¢—É–ø–æ—Å—Ç—å! –ê–≤—Ç–æ—Ä {criticism} –º–∞—Ç–µ—Ä–∏–∞–ª! üëé",
                    "–ú–µ—Ä–∑–∫–æ! {emotion} –æ—Ç —Ç–∞–∫–æ–π –ø–æ–¥–∞—á–∏! ü§Æ",
                    "–û—Ç–≤—Ä–∞—Ç–Ω–æ! {criticism} –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é! üò°",
                    "–ö–æ—à–º–∞—Ä! {emotion} —á–∏—Ç–∞—Ç—å —Ç–∞–∫–æ–µ! üíÄ",
                    "–ì–∞–¥–æ—Å—Ç—å! –ê–≤—Ç–æ—Ä {criticism} –∫–æ–Ω—Ç–µ–Ω—Ç! üòû"
                ],
                'neutral': [
                    "–ü—Ä–æ—á–∏—Ç–∞–ª –ø–æ—Å—Ç. {opinion} –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è.",
                    "–í–∏–¥–µ–ª —É–∂–µ –ø–æ–¥–æ–±–Ω–æ–µ. {opinion} –∫–æ–Ω—Ç–µ–Ω—Ç.",
                    "–ù–æ—Ä–º–∞–ª—å–Ω—ã–π –ø–æ—Å—Ç. {opinion} –º–∞—Ç–µ—Ä–∏–∞–ª.",
                    "–ß–∏—Ç–∞—é –∏–Ω–æ–≥–¥–∞. {opinion} —Ç–µ–º–∞.",
                    "–¢–∞–∫ —Å–µ–±–µ –ø–æ–ª—É—á–∏–ª–æ—Å—å. {opinion} –ø–æ–¥–∞—á–∞.",
                    "–û–±—ã—á–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç –æ—Ç –∞–≤—Ç–æ—Ä–∞. {opinion} —Å—Ç–∏–ª—å.",
                    "–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è –ø–æ–¥–∞—á–∞ –º–∞—Ç–µ—Ä–∏–∞–ª–∞. {opinion} —Ñ–æ—Ä–º–∞—Ç.",
                    "–¢–∏–ø–∏—á–Ω—ã–π –ø–æ—Å—Ç –≤ –ª–µ–Ω—Ç–µ. {opinion} —Ç–µ–º–∞—Ç–∏–∫–∞.",
                    "–ü—Ä–∏–≤—ã—á–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç. {opinion} –∫–∞—á–µ—Å—Ç–≤–æ.",
                    "–†—è–¥–æ–≤–∞—è —Å—Ç–∞—Ç—å—è. {opinion} –∏–∑–ª–æ–∂–µ–Ω–∏–µ."
                ]
            },
            'personal': {
                'positive': [
                    "–°–µ–≥–æ–¥–Ω—è {mood}! –í—Å–µ {result} –ø—Ä–µ–∫—Ä–∞—Å–Ω–æ!",
                    "–ö–∞–∫–æ–π {mood} –¥–µ–Ω—å! {emotion} –æ—Ç –∂–∏–∑–Ω–∏!",
                    "–†–∞–¥—É—é—Å—å {event}! {mood} –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ!",
                    "–°—á–∞—Å—Ç–ª–∏–≤ {reason}! {emotion} –±–µ–∑–≥—Ä–∞–Ω–∏—á–Ω–æ!",
                    "–í–æ—Å—Ç–æ—Ä–≥ –æ—Ç {event}! {mood} —Ü–µ–ª—ã–π –¥–µ–Ω—å!",
                    "–ö–∞–π—Ñ—É—é –æ—Ç {event}! {emotion} –∑–∞—à–∫–∞–ª–∏–≤–∞–µ—Ç!",
                    "–ë–∞–ª–¥–µ—é –æ—Ç {event}! {mood} –Ω–µ–¥–µ–ª—é!",
                    "–¢–æ—Ä—á—É –æ—Ç {event}! {emotion} –ø–µ—Ä–µ–ø–æ–ª–Ω—è–µ—Ç!",
                    "–¢–∞—â—É—Å—å –æ—Ç {event}! {mood} –º–µ—Å—è—Ü!",
                    "–û—Ñ–∏–≥–µ–≤–∞—é –æ—Ç {event}! {emotion} –Ω–µ –ø–µ—Ä–µ–¥–∞—Ç—å!"
                ],
                'negative': [
                    "–£–∂–∞—Å–Ω–æ {mood} —Å–µ–≥–æ–¥–Ω—è. –í—Å–µ {result} –ø–ª–æ—Ö–æ.",
                    "–†–∞—Å—Å—Ç—Ä–æ–µ–Ω {event}. {emotion} –æ—Ç —ç—Ç–æ–≥–æ.",
                    "–ì—Ä—É—Å—Ç–Ω–æ {reason}. {mood} –Ω–∞ –¥—É—à–µ.",
                    "–ó–ª—é—Å—å {event}! {emotion} –æ—Ç –≤—Å–µ–≥–æ!",
                    "–ü–µ—á–∞–ª—å–Ω–æ {reason}. {mood} –¥–µ–Ω—å.",
                    "–ë–µ—à—É—Å—å –æ—Ç {event}! {emotion} –∑–∞—à–∫–∞–ª–∏–≤–∞–µ—Ç!",
                    "–î–æ—Å—Ç–∞–ª–æ {event}! {mood} —É–∂–µ –Ω–µ–¥–µ–ª—é!",
                    "–†–∞–∑–¥—Ä–∞–∂–∞–µ—Ç {event}! {emotion} —á–µ—Ä–µ–∑ –∫—Ä–∞–π!",
                    "–í—ã–±–µ—à–∏–≤–∞–µ—Ç {event}! {mood} –∫–∞–∂–¥—ã–π –¥–µ–Ω—å!",
                    "–£–±–∏–≤–∞–µ—Ç {event}! {emotion} –Ω–µ—Ç —Å–∏–ª!"
                ],
                'neutral': [
                    "–û–±—ã—á–Ω—ã–π –¥–µ–Ω—å. {event} –∫–∞–∫ –≤—Å–µ–≥–¥–∞.",
                    "–ù–∏—á–µ–≥–æ –æ—Å–æ–±–µ–Ω–Ω–æ–≥–æ. {result} —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ.",
                    "–¢–∞–∫ —Å–µ–±–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ. {event} –±–µ–∑ —ç–º–æ—Ü–∏–π.",
                    "–ù–æ—Ä–º–∞–ª—å–Ω—ã–π {event}. {result} –æ–∂–∏–¥–∞–µ–º–æ.",
                    "–°—Ä–µ–¥–Ω–µ–Ω—å–∫–∏–π –¥–µ–Ω—å. {event} –∫–∞–∫ –æ–±—ã—á–Ω–æ.",
                    "–¢–∏–ø–∏—á–Ω—ã–π {event}. {result} –ø—Ä–µ–¥—Å–∫–∞–∑—É–µ–º–æ.",
                    "–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è {event}. {result} –ø–æ –ø–ª–∞–Ω—É.",
                    "–†—è–¥–æ–≤–æ–π –¥–µ–Ω—å. {event} –±–µ–∑ —Å—é—Ä–ø—Ä–∏–∑–æ–≤.",
                    "–ü—Ä–∏–≤—ã—á–Ω—ã–π {event}. {result} –∫–∞–∫ –≤—Å–µ–≥–¥–∞.",
                    "–û–±—ã–¥–µ–Ω–Ω—ã–π –¥–µ–Ω—å. {event} –ø–æ —Ä–∞—Å–ø–∏—Å–∞–Ω–∏—é."
                ]
            }
        }
        
        # –°–ª–æ–≤–∞—Ä–∏ –¥–ª—è –ø–æ–¥—Å—Ç–∞–Ω–æ–≤–æ–∫
        substitutions = {
            'product': ['—Ç–æ–≤–∞—Ä', '—Ç–µ–ª–µ—Ñ–æ–Ω', '–Ω–æ—É—Ç–±—É–∫', '–∫–Ω–∏–≥–∞', '–æ–¥–µ–∂–¥–∞', '–æ–±—É–≤—å', '—á–∞—Å—ã', '—Å—É–º–∫–∞', '–Ω–∞—É—à–Ω–∏–∫–∏', '–ø–ª–∞–Ω—à–µ—Ç'],
            'emotion': ['–≤–æ—Å—Ö–∏—â–∞—é—Å—å', '—Ä–∞–¥—É—é—Å—å', '–Ω–∞—Å–ª–∞–∂–¥–∞—é—Å—å', '–≤–¥–æ—Ö–Ω–æ–≤–ª—è—é—Å—å', '–∫–∞–π—Ñ—É—é', '–ø–æ—Ç–µ—Ä—è–ª –≤—Ä–µ–º—è', '—Ä–∞–∑–æ—á–∞—Ä–æ–≤–∞–Ω', '—É—Å—Ç–∞–ª', '—Ä–∞–∑–¥—Ä–∞–∂–µ–Ω', '–±–µ—à—É—Å—å'],
            'compliment': ['–º–æ–ª–æ–¥–µ—Ü', '—É–º–Ω–∏—Ü–∞', '–≥–µ–Ω–∏–π', '–ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª', '–º–∞—Å—Ç–µ—Ä', '—Ç–∞–ª–∞–Ω—Ç', '—ç–∫—Å–ø–µ—Ä—Ç', '—Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç'],
            'criticism': ['–Ω–µ —É–º–µ–µ—Ç –ø–∏—Å–∞—Ç—å', '—Å–∫—É—á–Ω—ã–π', '–Ω–µ–∏–Ω—Ç–µ—Ä–µ—Å–Ω—ã–π', '—Å–ª–∞–±—ã–π', '–ø–ª–æ—Ö–æ–π', '–±–µ–∑–¥–∞—Ä–Ω—ã–π', '–Ω–µ–≥—Ä–∞–º–æ—Ç–Ω—ã–π'],
            'opinion': ['–æ–±—ã—á–Ω–∞—è', '—Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è', '—Å—Ä–µ–¥–Ω—è—è', '—Ç–∏–ø–∏—á–Ω–∞—è', '–Ω–æ—Ä–º–∞–ª—å–Ω–∞—è', '–ø—Ä–∏–≤—ã—á–Ω–∞—è', '—Ä—è–¥–æ–≤–∞—è'],
            'mood': ['–æ—Ç–ª–∏—á–Ω—ã–π', '–ø—Ä–µ–∫—Ä–∞—Å–Ω—ã–π', '–∑–∞–º–µ—á–∞—Ç–µ–ª—å–Ω—ã–π', '—É–∂–∞—Å–Ω—ã–π', '–ø–ª–æ—Ö–æ–π', '–¥–µ–ø—Ä–µ—Å—Å–∏–≤–Ω—ã–π', '–∫–ª–∞—Å—Å–Ω—ã–π', '–∫–æ—à–º–∞—Ä–Ω—ã–π'],
            'result': ['–ø–æ–ª—É—á–∞–µ—Ç—Å—è', '—Å–∫–ª–∞–¥—ã–≤–∞–µ—Ç—Å—è', '–≤—ã—Ö–æ–¥–∏—Ç', '–∏–¥–µ—Ç', '–ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç', '–≤—ã—Ö–æ–¥–∏—Ç', '–ø–æ–ª—É—á–∞–µ—Ç—Å—è'],
            'event': ['–Ω–∞ —Ä–∞–±–æ—Ç–µ', '–¥–æ–º–∞', '—Å –¥—Ä—É–∑—å—è–º–∏', '–≤ –æ—Ç–ø—É—Å–∫–µ', '–≤ –≤—ã—Ö–æ–¥–Ω—ã–µ', '–Ω–∞ —É—á–µ–±–µ', '–≤ —Å–ø–æ—Ä—Ç–∑–∞–ª–µ'],
            'reason': ['–±–µ–∑ –ø—Ä–∏—á–∏–Ω—ã', '–∏–∑-–∑–∞ –ø–æ–≥–æ–¥—ã', '–∏–∑-–∑–∞ —Ä–∞–±–æ—Ç—ã', '–∏–∑-–∑–∞ –ø—Ä–æ–±–ª–µ–º', '–ø—Ä–æ—Å—Ç–æ —Ç–∞–∫', '–∏–∑-–∑–∞ —É—Å—Ç–∞–ª–æ—Å—Ç–∏']
        }
        
        samples_per_domain = num_samples // len(domains)
        
        for domain_name, domain_templates in domains.items():
            for sentiment, templates in domain_templates.items():
                sentiment_samples = samples_per_domain // 3
                
                for _ in range(sentiment_samples):
                    template = random.choice(templates)
                    
                    # –ó–∞–ø–æ–ª–Ω—è–µ–º —à–∞–±–ª–æ–Ω
                    text = template
                    for placeholder, values in substitutions.items():
                        if '{' + placeholder + '}' in text:
                            text = text.replace('{' + placeholder + '}', random.choice(values))
                    
                    # –î–æ–±–∞–≤–ª—è–µ–º —ç–º–æ–¥–∑–∏ –∏–Ω–æ–≥–¥–∞
                    if sentiment == 'positive' and random.random() < 0.3:
                        emojis = ['üòä', 'üëç', '‚ù§Ô∏è', 'üî•', '‚ú®', 'üéâ', '‚≠ê', 'üíØ']
                        text += ' ' + random.choice(emojis)
                    elif sentiment == 'negative' and random.random() < 0.2:
                        emojis = ['üò¢', 'üò°', 'üëé', 'üíî', 'üòû', 'ü§¨', 'üíÄ']
                        text += ' ' + random.choice(emojis)
                    
                    data.append({'text': text, 'label': sentiment})
        
        df = pd.DataFrame(data)
        return df.sample(frac=1).reset_index(drop=True)
    
    def load_all_available_datasets(self) -> pd.DataFrame:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –≤—Å–µ—Ö –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤"""
        combined_data = []
        
        # 1. –ó–∞–≥—Ä—É–∑–∫–∞ RuSentiment (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç)
        rusentiment_path = self.download_rusentiment_robust()
        if rusentiment_path and rusentiment_path.exists():
            try:
                encodings = ['utf-8', 'cp1251', 'latin1', 'utf-16']
                df = None
                
                for encoding in encodings:
                    try:
                        df = pd.read_csv(rusentiment_path, encoding=encoding)
                        break
                    except (UnicodeDecodeError, UnicodeError):
                        continue
                
                if df is not None and len(df) > 0:
                    df = self._process_rusentiment(df)
                    if len(df) > 0:
                        print(f"‚úÖ RuSentiment –æ–±—Ä–∞–±–æ—Ç–∞–Ω ({len(df)} –∑–∞–ø–∏—Å–µ–π)")
                        combined_data.append(df)
                    
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ RuSentiment: {e}")
        
        # 2. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤
        additional_datasets = self.download_additional_datasets()
        for name, dataset in additional_datasets.items():
            if len(dataset) > 0:
                combined_data.append(dataset)
        
        # 3. –°–æ–∑–¥–∞–Ω–∏–µ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö
        synthetic_size = 30000 if not combined_data else 20000
        synthetic_df = self.create_extended_synthetic_dataset(synthetic_size)
        print(f"‚úÖ –°–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ —Å–æ–∑–¥–∞–Ω—ã ({len(synthetic_df)} –∑–∞–ø–∏—Å–µ–π)")
        combined_data.append(synthetic_df)
        
        # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤
        if combined_data:
            final_df = pd.concat(combined_data, ignore_index=True)
            final_df = final_df.drop_duplicates(subset=['text']).reset_index(drop=True)
            print(f"\nüéØ –ò—Ç–æ–≥–æ–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç: {len(final_df)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π")
            
            # –ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ –∫–ª–∞—Å—Å–æ–≤
            final_df = self._balance_dataset(final_df)
            
            return final_df
        
        return self.create_extended_synthetic_dataset(50000)
    
    def _process_rusentiment(self, df: pd.DataFrame) -> pd.DataFrame:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ RuSentiment"""
        try:
            # –ü–æ–∏—Å–∫ –Ω—É–∂–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫
            possible_text_cols = ['text', 'comment', 'post', 'content', 'tweet']
            possible_label_cols = ['label', 'sentiment', 'class', 'target', 'emotion']
            
            text_col = None
            label_col = None
            
            # –ò—â–µ–º —Ç–µ–∫—Å—Ç–æ–≤—É—é –∫–æ–ª–æ–Ω–∫—É
            for col in df.columns:
                col_lower = col.lower()
                if any(name in col_lower for name in possible_text_cols):
                    text_col = col
                    break
            
            # –ò—â–µ–º –∫–æ–ª–æ–Ω–∫—É —Å –º–µ—Ç–∫–∞–º–∏
            for col in df.columns:
                col_lower = col.lower()
                if any(name in col_lower for name in possible_label_cols):
                    label_col = col
                    break
            
            # –ï—Å–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –∏–º–µ–Ω–∞, –±–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ –¥–≤–µ –∫–æ–ª–æ–Ω–∫–∏
            if text_col is None or label_col is None:
                if len(df.columns) >= 2:
                    text_col = df.columns[0]
                    label_col = df.columns[1]
                else:
                    print("‚ùå –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∫–æ–ª–æ–Ω–æ–∫ –≤ RuSentiment")
                    return pd.DataFrame(columns=['text', 'label'])
            
            df = df[[text_col, label_col]].copy()
            df.columns = ['text', 'label']
            
            # –û—á–∏—Å—Ç–∫–∞ –∏ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –º–µ—Ç–æ–∫
            df['label'] = df['label'].astype(str).str.lower().str.strip()
            
            # –ú–∞–ø–ø–∏–Ω–≥ –º–µ—Ç–æ–∫ RuSentiment
            label_mapping = {
                'positive': 'positive',
                'negative': 'negative', 
                'neutral': 'neutral',
                'pos': 'positive',
                'neg': 'negative',
                'neu': 'neutral',
                '1': 'positive',
                '0': 'neutral',
                '-1': 'negative',
                'good': 'positive',
                'bad': 'negative',
                'normal': 'neutral',
                'speech': 'neutral',  # Speech –∫–∞–∫ –Ω–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π
                'skip': 'neutral',    # Skip –∫–∞–∫ –Ω–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π
                'na': 'neutral',
                'none': 'neutral'
            }
            
            df['label'] = df['label'].map(label_mapping).fillna('neutral')
            
            # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è
            df = df[df['text'].notna() & (df['text'] != '') & (df['text'].str.len() > 3)]
            
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞
            df['text'] = df['text'].str.replace(r'http\S+', '', regex=True)  # –£–¥–∞–ª—è–µ–º URL
            df['text'] = df['text'].str.replace(r'@\w+', '', regex=True)     # –£–¥–∞–ª—è–µ–º —É–ø–æ–º–∏–Ω–∞–Ω–∏—è
            df['text'] = df['text'].str.replace(r'#\w+', '', regex=True)     # –£–¥–∞–ª—è–µ–º —Ö–µ—à—Ç–µ–≥–∏
            df['text'] = df['text'].str.strip()
            
            # –§–∏–Ω–∞–ª—å–Ω–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è
            df = df[df['text'].str.len() > 5]
            
            return df
            
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –≤ _process_rusentiment: {e}")
            return pd.DataFrame(columns=['text', 'label'])
    
    def _balance_dataset(self, df: pd.DataFrame) -> pd.DataFrame:
        """–ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞"""
        try:
            label_counts = df['label'].value_counts()
            
            # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Ü–µ–ª–µ–≤–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ (–º–∏–Ω–∏–º—É–º 15000 –Ω–∞ –∫–ª–∞—Å—Å)
            target_count = max(label_counts.min(), 15000)
            
            balanced_dfs = []
            for label in ['positive', 'negative', 'neutral']:
                label_df = df[df['label'] == label].copy()
                
                if len(label_df) > target_count:
                    # –ï—Å–ª–∏ –º–Ω–æ–≥–æ –¥–∞–Ω–Ω—ã—Ö - —Å—ç–º–ø–ª–∏—Ä—É–µ–º
                    label_df = label_df.sample(n=target_count, random_state=42)
                elif len(label_df) < target_count:
                    # –ï—Å–ª–∏ –º–∞–ª–æ –¥–∞–Ω–Ω—ã—Ö - –¥—É–±–ª–∏—Ä—É–µ–º —Å –Ω–µ–±–æ–ª—å—à–∏–º–∏ –∏–∑–º–µ–Ω–µ–Ω–∏—è–º–∏
                    original_count = len(label_df)
                    needed = target_count - original_count
                    
                    # –î—É–±–ª–∏—Ä—É–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ
                    duplicates = []
                    for i in range(needed):
                        sample = label_df.sample(n=1, random_state=42+i).copy()
                        # –î–æ–±–∞–≤–ª—è–µ–º –Ω–µ–±–æ–ª—å—à–∏–µ –≤–∞—Ä–∏–∞—Ü–∏–∏ –∫ —Ç–µ–∫—Å—Ç—É
                        original_text = sample.iloc[0]['text']
                        if random.random() < 0.3:
                            # –ò–Ω–æ–≥–¥–∞ –¥–æ–±–∞–≤–ª—è–µ–º –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é –∏–ª–∏ —ç–º–æ–¥–∑–∏
                            if label == 'positive' and not original_text.endswith('!'):
                                sample.iloc[0, sample.columns.get_loc('text')] = original_text + '!'
                            elif label == 'negative' and '.' in original_text:
                                sample.iloc[0, sample.columns.get_loc('text')] = original_text.replace('.', '...')
                        duplicates.append(sample)
                    
                    if duplicates:
                        duplicates_df = pd.concat(duplicates, ignore_index=True)
                        label_df = pd.concat([label_df, duplicates_df], ignore_index=True)
                
                balanced_dfs.append(label_df)
            
            balanced_df = pd.concat(balanced_dfs, ignore_index=True)
            balanced_df = balanced_df.sample(frac=1, random_state=42).reset_index(drop=True)
            
            print(f"üìä –î–∞—Ç–∞—Å–µ—Ç —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω:")
            print(balanced_df['label'].value_counts())
            
            return balanced_df
            
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –≤ –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–µ: {e}")
            return df
    
    def preprocess_text(self, text: str) -> str:
        """–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞"""
        if pd.isna(text):
            return ""
        
        text = str(text)
        
        # –ü—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É
        if TEXT_PREPROCESSING["lowercase"]:
            text = text.lower()
        
        # –£–¥–∞–ª–µ–Ω–∏–µ URL
        if TEXT_PREPROCESSING["remove_urls"]:
            text = re.sub(r'http\S+|www\S+|https\S+', '', text, flags=re.MULTILINE)
        
        # –£–¥–∞–ª–µ–Ω–∏–µ email
        if TEXT_PREPROCESSING["remove_emails"]:
            text = re.sub(r'\S+@\S+', '', text)
        
        # –£–¥–∞–ª–µ–Ω–∏–µ –ª–∏—à–Ω–∏—Ö –ø—Ä–æ–±–µ–ª–æ–≤
        text = re.sub(r'\s+', ' ', text)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤–∞–∂–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã (—ç–º–æ–¥–∑–∏, –ø—É–Ω–∫—Ç—É–∞—Ü–∏—è)
        text = re.sub(r'[^\w\s\U0001F600-\U0001F64F\U0001F300-\U0001F5FF\U0001F680-\U0001F6FF\U0001F1E0-\U0001F1FF!?.,]', ' ', text)
        
        # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
        try:
            tokens = nltk.word_tokenize(text, language='russian')
        except:
            tokens = text.split()
        
        # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Ç–æ–∫–µ–Ω–æ–≤
        tokens = [token for token in tokens if len(token) > 1]
        
        return ' '.join(tokens).strip()
    
    def prepare_final_dataset(self) -> Tuple[pd.DataFrame, pd.DataFrame]:
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞"""
        print("üöÄ –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º RuSentiment...")
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö
        df = self.load_all_available_datasets()
        
        # –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞
        print("üîÑ –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤...")
        tqdm.pandas(desc="–û–±—Ä–∞–±–æ—Ç–∫–∞")
        df['processed_text'] = df['text'].progress_apply(self.preprocess_text)
        
        # –û—á–∏—Å—Ç–∫–∞
        df = df[df['processed_text'].str.len() > 0]
        
        # –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –º–µ—Ç–æ–∫
        df['label_encoded'] = self.label_encoder.fit_transform(df['label'])
        
        # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
        train_df, test_df = train_test_split(
            df[['text', 'processed_text', 'label_encoded', 'label']], 
            test_size=0.2, 
            random_state=42,
            stratify=df['label_encoded']
        )
        
        print(f"\n‚úÖ –§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
        print(f"   –û–±—É—á–∞—é—â–∞—è: {len(train_df)} –ø—Ä–∏–º–µ—Ä–æ–≤")
        print(f"   –¢–µ—Å—Ç–æ–≤–∞—è: {len(test_df)} –ø—Ä–∏–º–µ—Ä–æ–≤")
        print(f"   –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ:")
        print(f"   {train_df['label'].value_counts()}")
        
        return train_df, test_df
    
    def save_processed_data(self, train_df: pd.DataFrame, test_df: pd.DataFrame):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö"""
        train_df.to_csv(PROCESSED_DATA_DIR / "train_data.csv", index=False, encoding='utf-8')
        test_df.to_csv(PROCESSED_DATA_DIR / "test_data.csv", index=False, encoding='utf-8')
        
        import joblib
        joblib.dump(self.label_encoder, PROCESSED_DATA_DIR / "label_encoder.pkl")
        joblib.dump(self.label_encoder, LABEL_ENCODER_PATH)
        
        print(f"\nüíæ –î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {PROCESSED_DATA_DIR}")


if __name__ == "__main__":
    tqdm.pandas()
    
    loader = EnhancedDataLoader()
    train_df, test_df = loader.prepare_final_dataset()
    loader.save_processed_data(train_df, test_df)
    
    print("\nüéâ –î–∞–Ω–Ω—ã–µ —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º RuSentiment –≥–æ—Ç–æ–≤—ã!")
    print("üöÄ –ó–∞–ø—É—Å—Ç–∏—Ç–µ: python production_train.py")