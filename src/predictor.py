"""
–ö–ª–∞—Å—Å –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–π
"""
import numpy as np
import joblib
from tensorflow.keras.models import load_model
from tensorflow.keras.preprocessing.sequence import pad_sequences
from typing import List, Dict, Union

from config import MODEL_PATH, TOKENIZER_PATH, LABEL_ENCODER_PATH, MODEL_CONFIG
from data_loader import DataLoader


class SentimentPredictor:
    """–ö–ª–∞—Å—Å –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–π —Ç–µ–∫—Å—Ç–æ–≤"""
    
    def __init__(self):
        self.model = None
        self.tokenizer = None
        self.label_encoder = None
        self.data_loader = DataLoader()
        self._load_artifacts()
    
    def _load_artifacts(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤"""
        try:
            # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
            model_files = list(MODEL_PATH.glob("*.h5"))
            if model_files:
                # –ë–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é –º–æ–¥–µ–ª—å
                latest_model = sorted(model_files)[-1]
                self.model = load_model(latest_model)
                print(f"–ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {latest_model}")
            else:
                raise FileNotFoundError("–ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
            
            # –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞
            self.tokenizer = joblib.load(TOKENIZER_PATH)
            print(f"–¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∑–∞–≥—Ä—É–∂–µ–Ω")
            
            # –ó–∞–≥—Ä—É–∑–∫–∞ label encoder
            self.label_encoder = joblib.load(LABEL_ENCODER_PATH)
            print(f"Label encoder –∑–∞–≥—Ä—É–∂–µ–Ω")
            
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤: {e}")
            print("–ù–µ–æ–±—Ö–æ–¥–∏–º–æ —Å–Ω–∞—á–∞–ª–∞ –æ–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å (–∑–∞–ø—É—Å—Ç–∏—Ç–µ train.py)")
            raise
    
    def predict(self, text: Union[str, List[str]]) -> Union[Dict, List[Dict]]:
        """
        –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏—è –¥–ª—è —Ç–µ–∫—Å—Ç–∞ –∏–ª–∏ —Å–ø–∏—Å–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤
        
        Args:
            text: –°—Ç—Ä–æ–∫–∞ –∏–ª–∏ —Å–ø–∏—Å–æ–∫ —Å—Ç—Ä–æ–∫ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å –∏–ª–∏ —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è–º–∏
        """
        # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ —Å–ø–∏—Å–æ–∫ –¥–ª—è –µ–¥–∏–Ω–æ–æ–±—Ä–∞–∑–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
        if isinstance(text, str):
            texts = [text]
            single_text = True
        else:
            texts = text
            single_text = False
        
        # –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤
        processed_texts = [self.data_loader.preprocess_text(t) for t in texts]
        
        # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        sequences = self.tokenizer.texts_to_sequences(processed_texts)
        X = pad_sequences(
            sequences, 
            maxlen=MODEL_CONFIG["max_sequence_length"],
            padding='post',
            truncating='post'
        )
        
        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
        predictions = self.model.predict(X, verbose=0)
        
        # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        results = []
        for i, (orig_text, probs) in enumerate(zip(texts, predictions)):
            # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–∞
            predicted_class = np.argmax(probs)
            sentiment = self.label_encoder.inverse_transform([predicted_class])[0]
            
            # –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
            confidence = float(probs[predicted_class])
            
            # –î–µ—Ç–∞–ª—å–Ω—ã–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏
            probabilities = {}
            for idx, label in enumerate(self.label_encoder.classes_):
                probabilities[label] = float(probs[idx])
            
            # –≠–º–æ–¥–∑–∏ –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
            emoji_map = {
                'negative': 'üò¢',
                'neutral': 'üòê',
                'positive': 'üòä'
            }
            
            result = {
                'text': orig_text,
                'sentiment': sentiment,
                'sentiment_ru': self._translate_sentiment(sentiment),
                'emoji': emoji_map.get(sentiment, ''),
                'confidence': confidence,
                'probabilities': probabilities,
                'processed_text': processed_texts[i]
            }
            
            results.append(result)
        
        return results[0] if single_text else results
    
    def _translate_sentiment(self, sentiment: str) -> str:
        """–ü–µ—Ä–µ–≤–æ–¥ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏—è –Ω–∞ —Ä—É—Å—Å–∫–∏–π"""
        translations = {
            'negative': '–ù–µ–≥–∞—Ç–∏–≤–Ω–æ–µ',
            'neutral': '–ù–µ–π—Ç—Ä–∞–ª—å–Ω–æ–µ',
            'positive': '–ü–æ–∑–∏—Ç–∏–≤–Ω–æ–µ'
        }
        return translations.get(sentiment, sentiment)
    
    def predict_batch(self, texts: List[str], batch_size: int = 32) -> List[Dict]:
        """
        –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–ª—è –±–æ–ª—å—à–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ç–µ–∫—Å—Ç–æ–≤ —Å –±–∞—Ç—á–∏–Ω–≥–æ–º
        
        Args:
            texts: –°–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤
            batch_size: –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞
            
        Returns:
            –°–ø–∏—Å–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
        """
        results = []
        
        for i in range(0, len(texts), batch_size):
            batch = texts[i:i + batch_size]
            batch_results = self.predict(batch)
            results.extend(batch_results)
        
        return results
    
    def analyze_sentiment_distribution(self, texts: List[str]) -> Dict:
        """
        –ê–Ω–∞–ª–∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–π –≤ —Å–ø–∏—Å–∫–µ —Ç–µ–∫—Å—Ç–æ–≤
        
        Args:
            texts: –°–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            
        Returns:
            –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏—è–º
        """
        predictions = self.predict_batch(texts)
        
        # –ü–æ–¥—Å—á–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        sentiment_counts = {'negative': 0, 'neutral': 0, 'positive': 0}
        total_confidence = {'negative': 0, 'neutral': 0, 'positive': 0}
        
        for pred in predictions:
            sentiment = pred['sentiment']
            sentiment_counts[sentiment] += 1
            total_confidence[sentiment] += pred['confidence']
        
        # –†–∞—Å—á–µ—Ç –ø—Ä–æ—Ü–µ–Ω—Ç–æ–≤ –∏ —Å—Ä–µ–¥–Ω–µ–π —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
        total = len(texts)
        stats = {}
        
        for sentiment in sentiment_counts:
            count = sentiment_counts[sentiment]
            percentage = (count / total * 100) if total > 0 else 0
            avg_confidence = (total_confidence[sentiment] / count * 100) if count > 0 else 0
            
            stats[sentiment] = {
                'count': count,
                'percentage': percentage,
                'average_confidence': avg_confidence
            }
        
        return {
            'total_texts': total,
            'sentiment_stats': stats,
            'dominant_sentiment': max(sentiment_counts, key=sentiment_counts.get)
        }
    
    def get_confidence_level(self, confidence: float) -> str:
        """
        –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —É—Ä–æ–≤–Ω—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        
        Args:
            confidence: –ó–Ω–∞—á–µ–Ω–∏–µ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ (0-1)
            
        Returns:
            –¢–µ–∫—Å—Ç–æ–≤–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ —É—Ä–æ–≤–Ω—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
        """
        if confidence >= 0.9:
            return "–û—á–µ–Ω—å –≤—ã—Å–æ–∫–∞—è"
        elif confidence >= 0.7:
            return "–í—ã—Å–æ–∫–∞—è"
        elif confidence >= 0.5:
            return "–°—Ä–µ–¥–Ω—è—è"
        else:
            return "–ù–∏–∑–∫–∞—è"


# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
if __name__ == "__main__":
    predictor = SentimentPredictor()
    
    # –¢–µ—Å—Ç–æ–≤—ã–µ –ø—Ä–∏–º–µ—Ä—ã
    test_texts = [
        "–û—Ç–ª–∏—á–Ω—ã–π –ø—Ä–æ–¥—É–∫—Ç, –≤—Å–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥—É—é!",
        "–£–∂–∞—Å–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ, –ø–æ–ª–Ω–æ–µ —Ä–∞–∑–æ—á–∞—Ä–æ–≤–∞–Ω–∏–µ",
        "–ù–æ—Ä–º–∞–ª—å–Ω–æ, –Ω–æ –µ—Å—Ç—å –Ω–µ–¥–æ—Å—Ç–∞—Ç–∫–∏",
        "–ü—Ä–æ—Å—Ç–æ –≤–æ—Å—Ö–∏—Ç–∏—Ç–µ–ª—å–Ω–æ! –õ—É—á—à–µ –Ω–µ –±—ã–≤–∞–µ—Ç!",
        "–ù–µ –≤–ø–µ—á–∞—Ç–ª–∏–ª–æ, –æ–∂–∏–¥–∞–ª –±–æ–ª—å—à–µ–≥–æ"
    ]
    
    print("\n=== –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π ===\n")
    
    # –û–¥–∏–Ω–æ—á–Ω–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
    result = predictor.predict(test_texts[0])
    print(f"–¢–µ–∫—Å—Ç: {result['text']}")
    print(f"–ù–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ: {result['sentiment_ru']} {result['emoji']}")
    print(f"–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {result['confidence']:.2%}")
    print(f"–£—Ä–æ–≤–µ–Ω—å —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏: {predictor.get_confidence_level(result['confidence'])}")
    print()
    
    # –ë–∞—Ç—á –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
    print("\n=== –ê–Ω–∞–ª–∏–∑ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤ ===\n")
    results = predictor.predict(test_texts)
    
    for result in results:
        print(f"{result['emoji']} {result['text']}")
        print(f"   ‚Üí {result['sentiment_ru']} (—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {result['confidence']:.2%})")
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    print("\n=== –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–π ===\n")
    stats = predictor.analyze_sentiment_distribution(test_texts)
    
    print(f"–í—Å–µ–≥–æ —Ç–µ–∫—Å—Ç–æ–≤: {stats['total_texts']}")
    print(f"–î–æ–º–∏–Ω–∏—Ä—É—é—â–µ–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ: {predictor._translate_sentiment(stats['dominant_sentiment'])}")
    print("\n–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ:")
    
    for sentiment, data in stats['sentiment_stats'].items():
        print(f"  {predictor._translate_sentiment(sentiment)}: "
              f"{data['count']} ({data['percentage']:.1f}%), "
              f"—Å—Ä–µ–¥–Ω—è—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {data['average_confidence']:.1f}%")