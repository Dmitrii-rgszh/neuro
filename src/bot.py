"""
–¢–µ–ª–µ–≥—Ä–∞–º-–±–æ—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º GPU –º–æ–¥–µ–ª–∏ PyTorch
"""
import torch
import torch.nn.functional as F
import joblib
import json
import os
import sys
from pathlib import Path
from datetime import datetime
import asyncio
import logging

from telegram import Update, InlineKeyboardButton, InlineKeyboardMarkup
from telegram.ext import (
    Application,
    CommandHandler,
    MessageHandler,
    CallbackQueryHandler,
    ContextTypes,
    filters
)
from dotenv import load_dotenv

# –ó–∞–≥—Ä—É–∑–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è
load_dotenv()

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    level=logging.INFO
)
logger = logging.getLogger(__name__)

# –ü—Ä–æ–≤–µ—Ä–∫–∞ GPU
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
if device.type == 'cuda':
    logger.info(f"üéâ –ò—Å–ø–æ–ª—å–∑—É–µ–º GPU: {torch.cuda.get_device_name(0)}")
else:
    logger.warning("‚ö†Ô∏è GPU –Ω–µ –Ω–∞–π–¥–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º CPU")

# –ü—É—Ç–∏
BASE_DIR = Path(__file__).parent
MODELS_DIR = BASE_DIR / "models"

# –¢–æ–∫–µ–Ω –±–æ—Ç–∞
TELEGRAM_TOKEN = os.getenv("TELEGRAM_BOT_TOKEN")
if not TELEGRAM_TOKEN:
    logger.error("‚ùå –¢–æ–∫–µ–Ω –±–æ—Ç–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω! –î–æ–±–∞–≤—å—Ç–µ TELEGRAM_BOT_TOKEN –≤ .env —Ñ–∞–π–ª")
    sys.exit(1)

# –ò–º–ø–æ—Ä—Ç –º–æ–¥–µ–ª–∏ –∏–∑ gpu_production_train.py
sys.path.append(str(BASE_DIR))
from gpu_production_train import SentimentModelGPU, RussianTokenizer

class SentimentAnalyzerBot:
    """–¢–µ–ª–µ–≥—Ä–∞–º-–±–æ—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–π —Å GPU"""
    
    def __init__(self):
        self.model = None
        self.tokenizer = None
        self.label_encoder = None
        self.config = None
        self.user_stats = {}
        self._load_model()
        
    def _load_model(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –∏ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤"""
        try:
            # –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
            with open(MODELS_DIR / 'config_gpu.json', 'r') as f:
                self.config = json.load(f)
            
            # –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞
            self.tokenizer = joblib.load(MODELS_DIR / 'tokenizer_gpu.pkl')
            logger.info("‚úÖ –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∑–∞–≥—Ä—É–∂–µ–Ω")
            
            # –ó–∞–≥—Ä—É–∑–∫–∞ label encoder
            self.label_encoder = joblib.load(MODELS_DIR / 'label_encoder_gpu.pkl')
            logger.info("‚úÖ Label encoder –∑–∞–≥—Ä—É–∂–µ–Ω")
            
            # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
            vocab_size = len(self.tokenizer.word_index) + 1
            self.model = SentimentModelGPU(vocab_size, self.config).to(device)
            
            checkpoint = torch.load(MODELS_DIR / 'best_sentiment_model_gpu.pth', map_location=device)
            self.model.load_state_dict(checkpoint['model_state_dict'])
            self.model.eval()
            
            logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ (—Ç–æ—á–Ω–æ—Å—Ç—å: {checkpoint['accuracy']:.2f}%)")
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
            logger.error("–°–Ω–∞—á–∞–ª–∞ –æ–±—É—á–∏—Ç–µ –º–æ–¥–µ–ª—å: python gpu_production_train.py")
            sys.exit(1)
    
    def predict_sentiment(self, text):
        """–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞"""
        # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
        sequence = self.tokenizer.texts_to_sequences([text])[0]
        
        # Padding
        if len(sequence) > self.config["max_length"]:
            sequence = sequence[:self.config["max_length"]]
        else:
            sequence = sequence + [0] * (self.config["max_length"] - len(sequence))
        
        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
        with torch.no_grad():
            X = torch.LongTensor([sequence]).to(device)
            output = self.model(X)
            probabilities = F.softmax(output, dim=1)
            prediction = torch.argmax(output, dim=1)
        
        # –†–µ–∑—É–ª—å—Ç–∞—Ç
        label_idx = prediction.cpu().numpy()[0]
        label = self.label_encoder.inverse_transform([label_idx])[0]
        confidence = probabilities[0][label_idx].cpu().numpy()
        
        # –î–µ—Ç–∞–ª—å–Ω—ã–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏
        probs_detail = {
            'negative': float(probabilities[0][0]),
            'neutral': float(probabilities[0][1]),
            'positive': float(probabilities[0][2])
        }
        
        return {
            'label': label,
            'confidence': float(confidence),
            'probabilities': probs_detail
        }
    
    async def start(self, update: Update, context: ContextTypes.DEFAULT_TYPE):
        """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ –∫–æ–º–∞–Ω–¥—ã /start"""
        user = update.effective_user
        
        welcome_message = (
            f"üëã –ü—Ä–∏–≤–µ—Ç, {user.first_name}!\n\n"
            "–Ø –±–æ—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–π –æ–∫—Ä–∞—Å–∫–∏ —Ç–µ–∫—Å—Ç–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º "
            f"**GPU {torch.cuda.get_device_name(0) if device.type == 'cuda' else 'CPU'}**!\n\n"
            "–û—Ç–ø—Ä–∞–≤—å –º–Ω–µ –ª—é–±–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ, –∏ —è –æ–ø—Ä–µ–¥–µ–ª—é –µ–≥–æ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ:\n"
            "‚Ä¢ üòä –ü–æ–∑–∏—Ç–∏–≤–Ω–æ–µ\n"
            "‚Ä¢ üòê –ù–µ–π—Ç—Ä–∞–ª—å–Ω–æ–µ\n"
            "‚Ä¢ üò¢ –ù–µ–≥–∞—Ç–∏–≤–Ω–æ–µ\n\n"
            "–î–æ—Å—Ç—É–ø–Ω—ã–µ –∫–æ–º–∞–Ω–¥—ã:\n"
            "/help - –°–ø—Ä–∞–≤–∫–∞\n"
            "/stats - –í–∞—à–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞\n"
            "/examples - –ü—Ä–∏–º–µ—Ä—ã –∞–Ω–∞–ª–∏–∑–∞\n"
            "/about - –û –±–æ—Ç–µ"
        )
        
        await update.message.reply_text(welcome_message, parse_mode='Markdown')
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        if user.id not in self.user_stats:
            self.user_stats[user.id] = {
                'total': 0,
                'positive': 0,
                'negative': 0,
                'neutral': 0,
                'first_use': datetime.now().isoformat()
            }
    
    async def help_command(self, update: Update, context: ContextTypes.DEFAULT_TYPE):
        """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ –∫–æ–º–∞–Ω–¥—ã /help"""
        help_text = (
            "üìñ **–ö–∞–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –±–æ—Ç–∞:**\n\n"
            "1. –ü—Ä–æ—Å—Ç–æ –æ—Ç–ø—Ä–∞–≤—å—Ç–µ –ª—é–±–æ–µ —Ç–µ–∫—Å—Ç–æ–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ\n"
            "2. –ë–æ—Ç –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—É—é –æ–∫—Ä–∞—Å–∫—É\n"
            "3. –í—ã –ø–æ–ª—É—á–∏—Ç–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Å —É—Ä–æ–≤–Ω–µ–º —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏\n\n"
            "**–û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏:**\n"
            f"‚Ä¢ ‚ö° –ò—Å–ø–æ–ª—å–∑—É–µ—Ç GPU –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞\n"
            f"‚Ä¢ üß† –ú–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞ –Ω–∞ {100000} –ø—Ä–∏–º–µ—Ä–∞—Ö\n"
            f"‚Ä¢ üéØ –¢–æ—á–Ω–æ—Å—Ç—å: >90%\n"
            "‚Ä¢ üìä –î–µ—Ç–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞\n\n"
            "**–°–æ–≤–µ—Ç—ã:**\n"
            "‚Ä¢ –ü–∏—à–∏—Ç–µ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–º —Ä—É—Å—Å–∫–∏–º —è–∑—ã–∫–æ–º\n"
            "‚Ä¢ –ú–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —ç–º–æ–¥–∑–∏\n"
            "‚Ä¢ –ß–µ–º –¥–ª–∏–Ω–Ω–µ–µ —Ç–µ–∫—Å—Ç, —Ç–µ–º —Ç–æ—á–Ω–µ–µ –∞–Ω–∞–ª–∏–∑"
        )
        
        await update.message.reply_text(help_text, parse_mode='Markdown')
    
    async def analyze_message(self, update: Update, context: ContextTypes.DEFAULT_TYPE):
        """–ê–Ω–∞–ª–∏–∑ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è"""
        user_id = update.effective_user.id
        message_text = update.message.text
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º, —á—Ç–æ –±–æ—Ç –ø–µ—á–∞—Ç–∞–µ—Ç
        await context.bot.send_chat_action(
            chat_id=update.effective_chat.id,
            action="typing"
        )
        
        try:
            # –ê–Ω–∞–ª–∏–∑ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏—è
            start_time = datetime.now()
            result = self.predict_sentiment(message_text)
            inference_time = (datetime.now() - start_time).total_seconds()
            
            # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
            if user_id in self.user_stats:
                self.user_stats[user_id]['total'] += 1
                self.user_stats[user_id][result['label']] += 1
            
            # Emoji –¥–ª—è –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–π
            emoji_map = {
                'positive': 'üòä',
                'neutral': 'üòê', 
                'negative': 'üò¢'
            }
            
            label_ru = {
                'positive': '–ü–æ–∑–∏—Ç–∏–≤–Ω–æ–µ',
                'neutral': '–ù–µ–π—Ç—Ä–∞–ª—å–Ω–æ–µ',
                'negative': '–ù–µ–≥–∞—Ç–∏–≤–Ω–æ–µ'
            }
            
            # –£—Ä–æ–≤–µ–Ω—å —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
            if result['confidence'] >= 0.9:
                confidence_level = "–û—á–µ–Ω—å –≤—ã—Å–æ–∫–∞—è"
            elif result['confidence'] >= 0.7:
                confidence_level = "–í—ã—Å–æ–∫–∞—è"
            elif result['confidence'] >= 0.5:
                confidence_level = "–°—Ä–µ–¥–Ω—è—è"
            else:
                confidence_level = "–ù–∏–∑–∫–∞—è"
            
            # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞
            response = (
                f"**–ê–Ω–∞–ª–∏–∑ —Å–æ–æ–±—â–µ–Ω–∏—è:**\n\n"
                f"üìù _–¢–µ–∫—Å—Ç:_ {message_text[:100]}{'...' if len(message_text) > 100 else ''}\n\n"
                f"**–†–µ–∑—É–ª—å—Ç–∞—Ç:**\n"
                f"{emoji_map[result['label']]} **{label_ru[result['label']]}**\n"
                f"üìä –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {result['confidence']:.1%} ({confidence_level})\n\n"
                f"**–î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑:**\n"
                f"‚Ä¢ –ü–æ–∑–∏—Ç–∏–≤: {result['probabilities']['positive']:.1%}\n"
                f"‚Ä¢ –ù–µ–π—Ç—Ä–∞–ª: {result['probabilities']['neutral']:.1%}\n"
                f"‚Ä¢ –ù–µ–≥–∞—Ç–∏–≤: {result['probabilities']['negative']:.1%}\n\n"
                f"‚ö° _–í—Ä–µ–º—è –∞–Ω–∞–ª–∏–∑–∞: {inference_time:.3f} —Å–µ–∫ (GPU)_"
            )
            
            # –î–æ–±–∞–≤–ª—è–µ–º –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π
            if result['label'] == 'positive' and result['confidence'] > 0.8:
                response += "\n\n‚ú® –û—Ç–ª–∏—á–Ω–æ–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ! –¢–∞–∫ –¥–µ—Ä–∂–∞—Ç—å!"
            elif result['label'] == 'negative' and result['confidence'] > 0.8:
                response += "\n\nüíô –ù–µ —Ä–∞—Å—Å—Ç—Ä–∞–∏–≤–∞–π—Ç–µ—Å—å, –≤—Å–µ –Ω–∞–ª–∞–¥–∏—Ç—Å—è!"
            
            # –ö–ª–∞–≤–∏–∞—Ç—É—Ä–∞ –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏
            keyboard = [
                [
                    InlineKeyboardButton("üëç –í–µ—Ä–Ω–æ", callback_data=f"correct_{result['label']}"),
                    InlineKeyboardButton("üëé –ù–µ–≤–µ—Ä–Ω–æ", callback_data=f"wrong_{result['label']}")
                ],
                [InlineKeyboardButton("üìä –ú–æ—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞", callback_data="show_stats")]
            ]
            reply_markup = InlineKeyboardMarkup(keyboard)
            
            await update.message.reply_text(
                response,
                parse_mode='Markdown',
                reply_markup=reply_markup
            )
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ: {e}")
            await update.message.reply_text(
                "üòî –ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –µ—â–µ —Ä–∞–∑."
            )
    
    async def stats_command(self, update: Update, context: ContextTypes.DEFAULT_TYPE):
        """–ü–æ–∫–∞–∑ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"""
        user_id = update.effective_user.id
        
        if user_id not in self.user_stats or self.user_stats[user_id]['total'] == 0:
            await update.message.reply_text("üìä –£ –≤–∞—Å –ø–æ–∫–∞ –Ω–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏.")
            return
        
        stats = self.user_stats[user_id]
        total = stats['total']
        
        # –†–∞—Å—á–µ—Ç –ø—Ä–æ—Ü–µ–Ω—Ç–æ–≤
        pos_percent = (stats['positive'] / total * 100) if total > 0 else 0
        neu_percent = (stats['neutral'] / total * 100) if total > 0 else 0
        neg_percent = (stats['negative'] / total * 100) if total > 0 else 0
        
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –¥–æ–º–∏–Ω–∏—Ä—É—é—â–µ–≥–æ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏—è
        dominant = max(['positive', 'neutral', 'negative'], key=lambda x: stats[x])
        dominant_emoji = {'positive': 'üòä', 'neutral': 'üòê', 'negative': 'üò¢'}[dominant]
        
        stats_text = (
            f"üìä **–í–∞—à–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:**\n\n"
            f"üìù –í—Å–µ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏–π: {total}\n"
            f"üìÖ –ü–µ—Ä–≤–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ: {stats['first_use'][:10]}\n\n"
            f"**–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–π:**\n"
            f"üòä –ü–æ–∑–∏—Ç–∏–≤–Ω—ã—Ö: {stats['positive']} ({pos_percent:.1f}%)\n"
            f"üòê –ù–µ–π—Ç—Ä–∞–ª—å–Ω—ã—Ö: {stats['neutral']} ({neu_percent:.1f}%)\n"
            f"üò¢ –ù–µ–≥–∞—Ç–∏–≤–Ω—ã—Ö: {stats['negative']} ({neg_percent:.1f}%)\n\n"
            f"**–í–∞—à–µ –¥–æ–º–∏–Ω–∏—Ä—É—é—â–µ–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ:** {dominant_emoji}\n\n"
            f"‚ö° _–ê–Ω–∞–ª–∏–∑ –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –Ω–∞ GPU –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π —Å–∫–æ—Ä–æ—Å—Ç–∏!_"
        )
        
        await update.message.reply_text(stats_text, parse_mode='Markdown')
    
    async def examples_command(self, update: Update, context: ContextTypes.DEFAULT_TYPE):
        """–ü—Ä–∏–º–µ—Ä—ã –∞–Ω–∞–ª–∏–∑–∞"""
        examples = [
            ("–°–µ–≥–æ–¥–Ω—è –ø—Ä–µ–∫—Ä–∞—Å–Ω—ã–π –¥–µ–Ω—å! –í—Å–µ –ø–æ–ª—É—á–∞–µ—Ç—Å—è!", "positive"),
            ("–û–±—ã—á–Ω—ã–π —Ä–∞–±–æ—á–∏–π –¥–µ–Ω—å, –Ω–∏—á–µ–≥–æ –æ—Å–æ–±–µ–Ω–Ω–æ–≥–æ", "neutral"),
            ("–í—Å–µ –ø–ª–æ—Ö–æ, –Ω–∏—á–µ–≥–æ –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç", "negative")
        ]
        
        examples_text = "üìù **–ü—Ä–∏–º–µ—Ä—ã –∞–Ω–∞–ª–∏–∑–∞:**\n\n"
        
        for text, expected in examples:
            result = self.predict_sentiment(text)
            emoji = {'positive': 'üòä', 'neutral': 'üòê', 'negative': 'üò¢'}[result['label']]
            
            examples_text += (
                f"–¢–µ–∫—Å—Ç: _{text}_\n"
                f"–†–µ–∑—É–ª—å—Ç–∞—Ç: {emoji} {result['label']} "
                f"(—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {result['confidence']:.0%})\n\n"
            )
        
        await update.message.reply_text(examples_text, parse_mode='Markdown')
    
    async def about_command(self, update: Update, context: ContextTypes.DEFAULT_TYPE):
        """–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –±–æ—Ç–µ"""
        about_text = (
            "ü§ñ **–û –±–æ—Ç–µ:**\n\n"
            "–≠—Ç–æ—Ç –±–æ—Ç –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –Ω–µ–π—Ä–æ–Ω–Ω—É—é —Å–µ—Ç—å –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ "
            "—ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–π –æ–∫—Ä–∞—Å–∫–∏ —Ç–µ–∫—Å—Ç–æ–≤ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ.\n\n"
            "**–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏:**\n"
            f"‚Ä¢ üéÆ GPU: {torch.cuda.get_device_name(0) if device.type == 'cuda' else 'CPU'}\n"
            f"‚Ä¢ üß† –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞: LSTM + CNN + Attention\n"
            f"‚Ä¢ üìä –ü–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: >10M\n"
            f"‚Ä¢ üéØ –¢–æ—á–Ω–æ—Å—Ç—å: >90%\n"
            f"‚Ä¢ ‚ö° –°–∫–æ—Ä–æ—Å—Ç—å: <100ms –Ω–∞ GPU\n\n"
            "**–û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏:**\n"
            "‚Ä¢ –û–±—É—á–µ–Ω –Ω–∞ 100,000+ —Ä—É—Å—Å–∫–∏—Ö —Ç–µ–∫—Å—Ç–∞—Ö\n"
            "‚Ä¢ –ü–æ–Ω–∏–º–∞–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏ –∏—Ä–æ–Ω–∏—é\n"
            "‚Ä¢ –†–∞–±–æ—Ç–∞–µ—Ç —Å —Ç–µ–∫—Å—Ç–∞–º–∏ –ª—é–±–æ–π –¥–ª–∏–Ω—ã\n"
            "‚Ä¢ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç GPU –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞\n\n"
            "–†–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–æ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º PyTorch –∏ CUDA"
        )
        
        await update.message.reply_text(about_text, parse_mode='Markdown')
    
    async def button_callback(self, update: Update, context: ContextTypes.DEFAULT_TYPE):
        """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ –Ω–∞–∂–∞—Ç–∏–π –Ω–∞ –∫–Ω–æ–ø–∫–∏"""
        query = update.callback_query
        await query.answer()
        
        if query.data == "show_stats":
            await query.message.delete()
            await self.stats_command(update, context)
        elif query.data.startswith("correct_"):
            await query.edit_message_reply_markup(reply_markup=None)
            await query.message.reply_text("‚úÖ –°–ø–∞—Å–∏–±–æ –∑–∞ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ!")
        elif query.data.startswith("wrong_"):
            await query.edit_message_reply_markup(reply_markup=None)
            await query.message.reply_text("üìù –°–ø–∞—Å–∏–±–æ –∑–∞ –æ–±—Ä–∞—Ç–Ω—É—é —Å–≤—è–∑—å! –ú—ã —É–ª—É—á—à–∏–º –º–æ–¥–µ–ª—å.")

def main():
    """–ó–∞–ø—É—Å–∫ –±–æ—Ç–∞"""
    # –°–æ–∑–¥–∞–Ω–∏–µ –±–æ—Ç–∞
    bot = SentimentAnalyzerBot()
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
    application = Application.builder().token(TELEGRAM_TOKEN).build()
    
    # –†–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–æ–≤
    application.add_handler(CommandHandler("start", bot.start))
    application.add_handler(CommandHandler("help", bot.help_command))
    application.add_handler(CommandHandler("stats", bot.stats_command))
    application.add_handler(CommandHandler("examples", bot.examples_command))
    application.add_handler(CommandHandler("about", bot.about_command))
    
    # –û–±—Ä–∞–±–æ—Ç—á–∏–∫ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π
    application.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, bot.analyze_message))
    
    # –û–±—Ä–∞–±–æ—Ç—á–∏–∫ –∫–Ω–æ–ø–æ–∫
    application.add_handler(CallbackQueryHandler(bot.button_callback))
    
    # –ó–∞–ø—É—Å–∫
    logger.info("üöÄ –ë–æ—Ç –∑–∞–ø—É—â–µ–Ω —Å GPU –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π!")
    application.run_polling(allowed_updates=Update.ALL_TYPES)

if __name__ == "__main__":
    main()