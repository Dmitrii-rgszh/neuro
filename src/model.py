"""
–£–ª—É—á—à–µ–Ω–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –º–æ–¥–µ–ª–∏ —Å –∞–Ω—Å–∞–º–±–ª–µ–º –¥–ª—è –≤—ã—Å–æ–∫–æ—Ç–æ—á–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–π
"""
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers, regularizers
from tensorflow.keras.callbacks import (
    ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, 
    TensorBoard, LearningRateScheduler
)
import numpy as np
from datetime import datetime
from typing import List, Dict, Tuple
import joblib

from config import MODEL_CONFIG, MODEL_PATHS, VALIDATION_CONFIG


class AttentionLayer(layers.Layer):
    """–ö–∞—Å—Ç–æ–º–Ω—ã–π —Å–ª–æ–π –≤–Ω–∏–º–∞–Ω–∏—è"""
    
    def __init__(self, units, **kwargs):
        super(AttentionLayer, self).__init__(**kwargs)
        self.units = units
        self.W = layers.Dense(units)
        self.U = layers.Dense(units)
        self.V = layers.Dense(1)
        
    def call(self, inputs):
        # inputs shape: (batch_size, timesteps, features)
        score = tf.nn.tanh(self.W(inputs))
        attention_weights = tf.nn.softmax(self.V(score), axis=1)
        context_vector = attention_weights * inputs
        context_vector = tf.reduce_sum(context_vector, axis=1)
        return context_vector
    
    def get_config(self):
        config = super().get_config()
        config.update({"units": self.units})
        return config


class SentimentModelEnsemble:
    """–ê–Ω—Å–∞–º–±–ª—å –º–æ–¥–µ–ª–µ–π –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–π"""
    
    def __init__(self, vocab_size: int = None):
        self.vocab_size = vocab_size or MODEL_CONFIG["max_features"]
        self.models = {}
        self.ensemble_model = None
        
    def build_lstm_model(self, name="lstm") -> keras.Model:
        """–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ LSTM –º–æ–¥–µ–ª–∏ —Å –≤–Ω–∏–º–∞–Ω–∏–µ–º"""
        inputs = layers.Input(shape=(MODEL_CONFIG["max_sequence_length"],), name="input")
        
        # Embedding —Å–ª–æ–π
        embedding = layers.Embedding(
            input_dim=self.vocab_size,
            output_dim=MODEL_CONFIG["embedding_dim"],
            input_length=MODEL_CONFIG["max_sequence_length"],
            mask_zero=True,
            name="embedding"
        )(inputs)
        
        # Spatial Dropout
        embedding = layers.SpatialDropout1D(MODEL_CONFIG["spatial_dropout"])(embedding)
        
        # Bidirectional LSTM —Å–ª–æ–∏
        lstm1 = layers.Bidirectional(
            layers.LSTM(
                MODEL_CONFIG["lstm_units"],
                return_sequences=True,
                dropout=MODEL_CONFIG["dropout_rate"],
                recurrent_dropout=MODEL_CONFIG["recurrent_dropout"],
                kernel_regularizer=regularizers.l2(MODEL_CONFIG["l2_reg"])
            ),
            name="bilstm_1"
        )(embedding)
        
        lstm2 = layers.Bidirectional(
            layers.LSTM(
                MODEL_CONFIG["lstm_units"] // 2,
                return_sequences=True,
                dropout=MODEL_CONFIG["dropout_rate"],
                recurrent_dropout=MODEL_CONFIG["recurrent_dropout"],
                kernel_regularizer=regularizers.l2(MODEL_CONFIG["l2_reg"])
            ),
            name="bilstm_2"
        )(lstm1)
        
        # Attention mechanism
        attention = AttentionLayer(MODEL_CONFIG["lstm_units"], name="attention")(lstm2)
        
        # Global pooling –¥–ª—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        max_pool = layers.GlobalMaxPooling1D(name="global_max_pool")(lstm2)
        avg_pool = layers.GlobalAveragePooling1D(name="global_avg_pool")(lstm2)
        
        # –ö–æ–Ω–∫–∞—Ç–µ–Ω–∞—Ü–∏—è –≤—Å–µ—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        concatenated = layers.concatenate([attention, max_pool, avg_pool], name="feature_concat")
        
        # –ü–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã–µ —Å–ª–æ–∏
        dense = concatenated
        for i, units in enumerate(MODEL_CONFIG["dense_units"]):
            dense = layers.Dense(
                units, 
                activation='relu',
                kernel_regularizer=regularizers.l1_l2(
                    l1=MODEL_CONFIG["l1_reg"], 
                    l2=MODEL_CONFIG["l2_reg"]
                ),
                name=f"dense_{i+1}"
            )(dense)
            dense = layers.BatchNormalization(name=f"batch_norm_{i+1}")(dense)
            dense = layers.Dropout(MODEL_CONFIG["dropout_rate"], name=f"dropout_{i+1}")(dense)
        
        # –í—ã—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π
        outputs = layers.Dense(
            MODEL_CONFIG["num_classes"],
            activation='softmax',
            name="output"
        )(dense)
        
        model = keras.Model(inputs=inputs, outputs=outputs, name=name)
        return model
    
    def build_cnn_lstm_model(self, name="cnn_lstm") -> keras.Model:
        """–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ CNN+LSTM –º–æ–¥–µ–ª–∏"""
        inputs = layers.Input(shape=(MODEL_CONFIG["max_sequence_length"],), name="input")
        
        # Embedding
        embedding = layers.Embedding(
            input_dim=self.vocab_size,
            output_dim=MODEL_CONFIG["embedding_dim"],
            input_length=MODEL_CONFIG["max_sequence_length"],
            name="embedding"
        )(inputs)
        
        # CNN —Å–ª–æ–∏ –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –ª–æ–∫–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        conv_blocks = []
        filter_sizes = [2, 3, 4, 5]  # –†–∞–∑–Ω—ã–µ —Ä–∞–∑–º–µ—Ä—ã —Ñ–∏–ª—å—Ç—Ä–æ–≤
        
        for filter_size in filter_sizes:
            conv = layers.Conv1D(
                filters=128,
                kernel_size=filter_size,
                activation='relu',
                padding='same',
                name=f"conv_{filter_size}"
            )(embedding)
            conv = layers.MaxPooling1D(2, name=f"maxpool_{filter_size}")(conv)
            conv_blocks.append(conv)
        
        # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ CNN –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        if len(conv_blocks) > 1:
            cnn_features = layers.concatenate(conv_blocks, name="cnn_concat")
        else:
            cnn_features = conv_blocks[0]
        
        # LSTM –¥–ª—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
        lstm = layers.Bidirectional(
            layers.LSTM(
                MODEL_CONFIG["lstm_units"] // 2,
                return_sequences=True,
                dropout=MODEL_CONFIG["dropout_rate"]
            ),
            name="bilstm"
        )(cnn_features)
        
        # Attention
        attention = AttentionLayer(MODEL_CONFIG["lstm_units"] // 2, name="attention")(lstm)
        
        # Global pooling
        max_pool = layers.GlobalMaxPooling1D(name="global_max_pool")(lstm)
        avg_pool = layers.GlobalAveragePooling1D(name="global_avg_pool")(lstm)
        
        # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        features = layers.concatenate([attention, max_pool, avg_pool], name="feature_concat")
        
        # –ü–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã–µ —Å–ª–æ–∏
        dense = features
        for i, units in enumerate([256, 128]):
            dense = layers.Dense(units, activation='relu', name=f"dense_{i+1}")(dense)
            dense = layers.BatchNormalization(name=f"batch_norm_{i+1}")(dense)
            dense = layers.Dropout(MODEL_CONFIG["dropout_rate"], name=f"dropout_{i+1}")(dense)
        
        # –í—ã—Ö–æ–¥
        outputs = layers.Dense(MODEL_CONFIG["num_classes"], activation='softmax', name="output")(dense)
        
        model = keras.Model(inputs=inputs, outputs=outputs, name=name)
        return model
    
    def build_transformer_model(self, name="transformer") -> keras.Model:
        """–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ Transformer-–ø–æ–¥–æ–±–Ω–æ–π –º–æ–¥–µ–ª–∏"""
        inputs = layers.Input(shape=(MODEL_CONFIG["max_sequence_length"],), name="input")
        
        # Embedding
        embedding = layers.Embedding(
            input_dim=self.vocab_size,
            output_dim=MODEL_CONFIG["embedding_dim"],
            input_length=MODEL_CONFIG["max_sequence_length"],
            name="embedding"
        )(inputs)
        
        # Positional encoding (—É–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è)
        positions = tf.range(start=0, limit=MODEL_CONFIG["max_sequence_length"], delta=1)
        positions = layers.Embedding(
            input_dim=MODEL_CONFIG["max_sequence_length"],
            output_dim=MODEL_CONFIG["embedding_dim"],
            name="positional_embedding"
        )(positions)
        
        # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–≥–æ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è
        x = embedding + positions
        
        # Multi-head self-attention
        attention = layers.MultiHeadAttention(
            num_heads=MODEL_CONFIG["attention_heads"],
            key_dim=MODEL_CONFIG["attention_key_dim"],
            name="multihead_attention"
        )(x, x)
        
        # Add & Norm
        x = layers.Add(name="add_1")([x, attention])
        x = layers.LayerNormalization(name="layer_norm_1")(x)
        
        # Feed Forward Network
        ffn = layers.Dense(MODEL_CONFIG["embedding_dim"] * 2, activation='relu', name="ffn_1")(x)
        ffn = layers.Dense(MODEL_CONFIG["embedding_dim"], name="ffn_2")(ffn)
        
        # Add & Norm
        x = layers.Add(name="add_2")([x, ffn])
        x = layers.LayerNormalization(name="layer_norm_2")(x)
        
        # Global pooling
        pooled = layers.GlobalAveragePooling1D(name="global_avg_pool")(x)
        
        # Classification head
        dense = layers.Dense(256, activation='relu', name="dense_1")(pooled)
        dense = layers.Dropout(MODEL_CONFIG["dropout_rate"], name="dropout_1")(dense)
        dense = layers.Dense(128, activation='relu', name="dense_2")(dense)
        dense = layers.Dropout(MODEL_CONFIG["dropout_rate"], name="dropout_2")(dense)
        
        outputs = layers.Dense(MODEL_CONFIG["num_classes"], activation='softmax', name="output")(dense)
        
        model = keras.Model(inputs=inputs, outputs=outputs, name=name)
        return model
    
    def build_all_models(self) -> Dict[str, keras.Model]:
        """–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π –∞–Ω—Å–∞–º–±–ª—è"""
        print("–°—Ç—Ä–æ–∏–º –º–æ–¥–µ–ª–∏ –∞–Ω—Å–∞–º–±–ª—è...")
        
        # LSTM –º–æ–¥–µ–ª—å
        self.models["lstm"] = self.build_lstm_model("lstm")
        self.compile_model(self.models["lstm"])
        print(f"‚úÖ LSTM –º–æ–¥–µ–ª—å: {self.models['lstm'].count_params():,} –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤")
        
        # CNN+LSTM –º–æ–¥–µ–ª—å
        self.models["cnn_lstm"] = self.build_cnn_lstm_model("cnn_lstm")
        self.compile_model(self.models["cnn_lstm"])
        print(f"‚úÖ CNN+LSTM –º–æ–¥–µ–ª—å: {self.models['cnn_lstm'].count_params():,} –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤")
        
        # Transformer –º–æ–¥–µ–ª—å
        self.models["transformer"] = self.build_transformer_model("transformer")
        self.compile_model(self.models["transformer"])
        print(f"‚úÖ Transformer –º–æ–¥–µ–ª—å: {self.models['transformer'].count_params():,} –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤")
        
        return self.models
    
    def build_ensemble_model(self) -> keras.Model:
        """–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –∞–Ω—Å–∞–º–±–ª—è –∏–∑ –æ–±—É—á–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""
        if not self.models:
            raise ValueError("–°–Ω–∞—á–∞–ª–∞ –ø–æ—Å—Ç—Ä–æ–π—Ç–µ –∏ –æ–±—É—á–∏—Ç–µ –±–∞–∑–æ–≤—ã–µ –º–æ–¥–µ–ª–∏")
        
        # –í—Ö–æ–¥—ã
        inputs = layers.Input(shape=(MODEL_CONFIG["max_sequence_length"],), name="ensemble_input")
        
        # –ü–æ–ª—É—á–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –æ—Ç –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏
        model_outputs = []
        for name, model in self.models.items():
            # –ó–∞–º–æ—Ä–æ–∑–∫–∞ –≤–µ—Å–æ–≤ –±–∞–∑–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π
            model.trainable = False
            output = model(inputs)
            model_outputs.append(output)
        
        # –°—Ç–µ–∫–∏–Ω–≥ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
        if len(model_outputs) > 1:
            stacked = layers.concatenate(model_outputs, name="stack_predictions")
        else:
            stacked = model_outputs[0]
        
        # Meta-learner
        meta = layers.Dense(64, activation='relu', name="meta_dense_1")(stacked)
        meta = layers.Dropout(0.3, name="meta_dropout_1")(meta)
        meta = layers.Dense(32, activation='relu', name="meta_dense_2")(meta)
        meta = layers.Dropout(0.3, name="meta_dropout_2")(meta)
        
        # –§–∏–Ω–∞–ª—å–Ω—ã–π –≤—ã—Ö–æ–¥
        ensemble_output = layers.Dense(
            MODEL_CONFIG["num_classes"], 
            activation='softmax', 
            name="ensemble_output"
        )(meta)
        
        self.ensemble_model = keras.Model(inputs=inputs, outputs=ensemble_output, name="ensemble")
        self.compile_model(self.ensemble_model)
        
        print(f"‚úÖ –ê–Ω—Å–∞–º–±–ª—å –º–æ–¥–µ–ª—å: {self.ensemble_model.count_params():,} –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤")
        return self.ensemble_model
    
    def compile_model(self, model: keras.Model):
        """–ö–æ–º–ø–∏–ª—è—Ü–∏—è –º–æ–¥–µ–ª–∏ —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–æ–º"""
        # Scheduler –¥–ª—è learning rate
        if MODEL_CONFIG["learning_rate_schedule"]:
            lr_schedule = keras.optimizers.schedules.ExponentialDecay(
                initial_learning_rate=MODEL_CONFIG["learning_rate"],
                decay_steps=1000,
                decay_rate=0.9
            )
            optimizer = keras.optimizers.Adam(learning_rate=lr_schedule)
        else:
            optimizer = keras.optimizers.Adam(learning_rate=MODEL_CONFIG["learning_rate"])
        
        model.compile(
            optimizer=optimizer,
            loss='sparse_categorical_crossentropy',
            metrics=['accuracy']
        )
    
    def get_callbacks(self, model_name: str) -> List:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ callbacks –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        callbacks = [
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏
            ModelCheckpoint(
                filepath=str(MODEL_PATHS[f"{model_name}_model"]),
                monitor='val_accuracy',
                save_best_only=True,
                save_weights_only=False,
                mode='max',
                verbose=1
            ),
            
            # –†–∞–Ω–Ω—è—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞
            EarlyStopping(
                monitor='val_loss',
                patience=MODEL_CONFIG["early_stopping_patience"],
                restore_best_weights=True,
                verbose=1
            ),
            
            # –£–º–µ–Ω—å—à–µ–Ω–∏–µ learning rate
            ReduceLROnPlateau(
                monitor='val_loss',
                factor=MODEL_CONFIG["reduce_lr_factor"],
                patience=MODEL_CONFIG["reduce_lr_patience"],
                min_lr=MODEL_CONFIG["min_lr"],
                verbose=1
            ),
            
            # TensorBoard
            TensorBoard(
                log_dir=f"logs/{model_name}_{timestamp}",
                histogram_freq=1,
                write_graph=True,
                update_freq='epoch'
            )
        ]
        
        return callbacks
    
    def train_model(self, model: keras.Model, model_name: str, 
                   X_train, y_train, X_val, y_val):
        """–û–±—É—á–µ–Ω–∏–µ –æ–¥–Ω–æ–π –º–æ–¥–µ–ª–∏"""
        print(f"\nüöÄ –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ {model_name}...")
        
        # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –≤–µ—Å–æ–≤ –∫–ª–∞—Å—Å–æ–≤
        class_weights = None
        if MODEL_CONFIG["class_weight"]:
            class_weights = self._compute_class_weights(y_train)
        
        # Callbacks
        callbacks = self.get_callbacks(model_name)
        
        # –û–±—É—á–µ–Ω–∏–µ
        history = model.fit(
            X_train, y_train,
            batch_size=MODEL_CONFIG["batch_size"],
            epochs=MODEL_CONFIG["epochs"],
            validation_data=(X_val, y_val),
            callbacks=callbacks,
            class_weight=class_weights,
            verbose=1
        )
        
        return history
    
    def train_ensemble(self, X_train, y_train, X_val, y_val):
        """–û–±—É—á–µ–Ω–∏–µ –≤—Å–µ–≥–æ –∞–Ω—Å–∞–º–±–ª—è"""
        histories = {}
        
        # –û–±—É—á–µ–Ω–∏–µ –±–∞–∑–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π
        for name, model in self.models.items():
            histories[name] = self.train_model(model, name, X_train, y_train, X_val, y_val)
        
        # –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –∏ –æ–±—É—á–µ–Ω–∏–µ –∞–Ω—Å–∞–º–±–ª—è
        if MODEL_CONFIG["ensemble"]["enabled"]:
            self.build_ensemble_model()
            histories["ensemble"] = self.train_model(
                self.ensemble_model, "ensemble", X_train, y_train, X_val, y_val
            )
        
        return histories
    
    def _compute_class_weights(self, y_train):
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –≤–µ—Å–æ–≤ –∫–ª–∞—Å—Å–æ–≤"""
        from sklearn.utils.class_weight import compute_class_weight
        
        classes = np.unique(y_train)
        class_weights = compute_class_weight(
            'balanced',
            classes=classes,
            y=y_train
        )
        
        return dict(zip(classes, class_weights))
    
    def evaluate_ensemble(self, X_test, y_test):
        """–ö–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –∞–Ω—Å–∞–º–±–ª—è"""
        results = {}
        
        for name, model in self.models.items():
            print(f"\nüìä –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏ {name}:")
            metrics = model.evaluate(X_test, y_test, verbose=0)
            results[name] = dict(zip(model.metrics_names, metrics))
            
            # –î–µ—Ç–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞
            predictions = model.predict(X_test, verbose=0)
            predicted_classes = np.argmax(predictions, axis=1)
            
            from sklearn.metrics import classification_report, confusion_matrix
            
            print(f"–¢–æ—á–Ω–æ—Å—Ç—å: {results[name]['accuracy']:.4f}")
            print("\nClassification Report:")
            print(classification_report(
                y_test, predicted_classes,
                target_names=MODEL_CONFIG["class_names"]
            ))
        
        # –û—Ü–µ–Ω–∫–∞ –∞–Ω—Å–∞–º–±–ª—è
        if self.ensemble_model:
            print(f"\nüéØ –û—Ü–µ–Ω–∫–∞ –∞–Ω—Å–∞–º–±–ª—è:")
            metrics = self.ensemble_model.evaluate(X_test, y_test, verbose=0)
            results["ensemble"] = dict(zip(self.ensemble_model.metrics_names, metrics))
            print(f"–¢–æ—á–Ω–æ—Å—Ç—å –∞–Ω—Å–∞–º–±–ª—è: {results['ensemble']['accuracy']:.4f}")
        
        return results
    
    def save_all_models(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π"""
        for name, model in self.models.items():
            model_path = MODEL_PATHS[f"{name}_model"]
            model.save(str(model_path))
            print(f"üíæ {name} –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {model_path}")
        
        if self.ensemble_model:
            ensemble_path = MODEL_PATHS["ensemble_model"]
            self.ensemble_model.save(str(ensemble_path))
            print(f"üíæ –ê–Ω—Å–∞–º–±–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {ensemble_path}")
    
    def load_models(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –≤—Å–µ—Ö —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""
        for name in ["lstm", "cnn_lstm", "transformer"]:
            model_path = MODEL_PATHS[f"{name}_model"]
            if model_path.exists():
                self.models[name] = keras.models.load_model(
                    str(model_path),
                    custom_objects={"AttentionLayer": AttentionLayer}
                )
                print(f"üìÇ {name} –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –∞–Ω—Å–∞–º–±–ª—è
        ensemble_path = MODEL_PATHS["ensemble_model"]
        if ensemble_path.exists():
            self.ensemble_model = keras.models.load_model(
                str(ensemble_path),
                custom_objects={"AttentionLayer": AttentionLayer}
            )
            print(f"üìÇ –ê–Ω—Å–∞–º–±–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω")
    
    def predict_ensemble(self, X, use_ensemble=True):
        """–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∞–Ω—Å–∞–º–±–ª—è"""
        if use_ensemble and self.ensemble_model:
            return self.ensemble_model.predict(X, verbose=0)
        
        # –ú—è–≥–∫–æ–µ –≥–æ–ª–æ—Å–æ–≤–∞–Ω–∏–µ –±–∞–∑–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π
        predictions = []
        for name, model in self.models.items():
            pred = model.predict(X, verbose=0)
            predictions.append(pred)
        
        if predictions:
            # –£—Å—Ä–µ–¥–Ω–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
            ensemble_pred = np.mean(predictions, axis=0)
            return ensemble_pred
        
        return None


class AdvancedSentimentTrainer:
    """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π —Ç—Ä–µ–Ω–µ—Ä –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∞–Ω—Å–∞–º–±–ª—è –º–æ–¥–µ–ª–µ–π"""
    
    def __init__(self):
        self.model_ensemble = None
        self.tokenizer = None
        self.label_encoder = None
        self.training_history = {}
        self.validation_results = {}
    
    def prepare_advanced_features(self, texts):
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
        features = []
        
        for text in texts:
            text_features = {
                'length': len(text.split()),
                'exclamation_count': text.count('!'),
                'question_count': text.count('?'),
                'caps_ratio': sum(1 for c in text if c.isupper()) / len(text) if text else 0,
                'emoji_count': len([c for c in text if ord(c) > 127]),
            }
            features.append(list(text_features.values()))
        
        return np.array(features)
    
    def augment_data(self, texts, labels, augmentation_factor=0.2):
        """–ê—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —É–≤–µ–ª–∏—á–µ–Ω–∏—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è"""
        if not MODEL_CONFIG["data_augmentation"]["enabled"]:
            return texts, labels
        
        augmented_texts = []
        augmented_labels = []
        
        import random
        import nltk
        from nltk.corpus import wordnet
        
        def get_synonyms(word):
            """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–∏–Ω–æ–Ω–∏–º–æ–≤ —Å–ª–æ–≤–∞"""
            synonyms = set()
            try:
                for syn in wordnet.synsets(word, lang='rus'):
                    for lemma in syn.lemmas(lang='rus'):
                        synonyms.add(lemma.name().replace('_', ' '))
            except:
                pass
            return list(synonyms)
        
        def synonym_replacement(text, n=1):
            """–ó–∞–º–µ–Ω–∞ —Å–ª–æ–≤ —Å–∏–Ω–æ–Ω–∏–º–∞–º–∏"""
            words = text.split()
            new_words = words.copy()
            random_word_list = list(set([word for word in words if len(word) > 3]))
            random.shuffle(random_word_list)
            
            num_replaced = 0
            for random_word in random_word_list:
                synonyms = get_synonyms(random_word)
                if len(synonyms) >= 1:
                    synonym = random.choice(synonyms)
                    new_words = [synonym if word == random_word else word for word in new_words]
                    num_replaced += 1
                if num_replaced >= n:
                    break
            
            return ' '.join(new_words)
        
        def random_insertion(text, n=1):
            """–°–ª—É—á–∞–π–Ω–∞—è –≤—Å—Ç–∞–≤–∫–∞ —Å–ª–æ–≤"""
            words = text.split()
            for _ in range(n):
                if words:
                    new_word = random.choice(words)
                    random_idx = random.randint(0, len(words))
                    words.insert(random_idx, new_word)
            return ' '.join(words)
        
        def random_swap(text, n=1):
            """–°–ª—É—á–∞–π–Ω–∞—è –ø–µ—Ä–µ—Å—Ç–∞–Ω–æ–≤–∫–∞ —Å–ª–æ–≤"""
            words = text.split()
            for _ in range(n):
                if len(words) >= 2:
                    idx1, idx2 = random.sample(range(len(words)), 2)
                    words[idx1], words[idx2] = words[idx2], words[idx1]
            return ' '.join(words)
        
        def random_deletion(text, p=0.1):
            """–°–ª—É—á–∞–π–Ω–æ–µ —É–¥–∞–ª–µ–Ω–∏–µ —Å–ª–æ–≤"""
            words = text.split()
            if len(words) == 1:
                return text
            
            new_words = []
            for word in words:
                if random.uniform(0, 1) > p:
                    new_words.append(word)
            
            if len(new_words) == 0:
                return random.choice(words)
            
            return ' '.join(new_words)
        
        # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏
        num_aug = int(len(texts) * augmentation_factor)
        indices = random.sample(range(len(texts)), min(num_aug, len(texts)))
        
        for idx in indices:
            original_text = texts[idx]
            label = labels[idx]
            
            # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ç–µ—Ö–Ω–∏–∫ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏
            if random.random() < MODEL_CONFIG["data_augmentation"]["synonym_replacement"]:
                aug_text = synonym_replacement(original_text, n=2)
                augmented_texts.append(aug_text)
                augmented_labels.append(label)
            
            if random.random() < MODEL_CONFIG["data_augmentation"]["random_insertion"]:
                aug_text = random_insertion(original_text, n=1)
                augmented_texts.append(aug_text)
                augmented_labels.append(label)
            
            if random.random() < MODEL_CONFIG["data_augmentation"]["random_swap"]:
                aug_text = random_swap(original_text, n=1)
                augmented_texts.append(aug_text)
                augmented_labels.append(label)
            
            if random.random() < MODEL_CONFIG["data_augmentation"]["random_deletion"]:
                aug_text = random_deletion(original_text, p=0.1)
                augmented_texts.append(aug_text)
                augmented_labels.append(label)
        
        # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã—Ö –∏ –∞—É–≥–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        all_texts = list(texts) + augmented_texts
        all_labels = list(labels) + augmented_labels
        
        print(f"üìà –ê—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è: –¥–æ–±–∞–≤–ª–µ–Ω–æ {len(augmented_texts)} –ø—Ä–∏–º–µ—Ä–æ–≤")
        return all_texts, all_labels
    
    def cross_validate_models(self, X, y, cv_folds=5):
        """–ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π"""
        from sklearn.model_selection import StratifiedKFold
        from sklearn.metrics import accuracy_score, f1_score
        
        skf = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=42)
        cv_results = {model_name: [] for model_name in ["lstm", "cnn_lstm", "transformer"]}
        
        for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):
            print(f"\nüîÑ –ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è, —Ñ–æ–ª–¥ {fold + 1}/{cv_folds}")
            
            X_train_fold, X_val_fold = X[train_idx], X[val_idx]
            y_train_fold, y_val_fold = y[train_idx], y[val_idx]
            
            # –°–æ–∑–¥–∞–Ω–∏–µ –∏ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –¥–ª—è —ç—Ç–æ–≥–æ —Ñ–æ–ª–¥–∞
            ensemble = SentimentModelEnsemble(vocab_size=self.model_ensemble.vocab_size)
            ensemble.build_all_models()
            
            for model_name, model in ensemble.models.items():
                # –ë—ã—Å—Ç—Ä–æ–µ –æ–±—É—á–µ–Ω–∏–µ –¥–ª—è –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏–∏
                model.fit(
                    X_train_fold, y_train_fold,
                    batch_size=MODEL_CONFIG["batch_size"],
                    epochs=10,  # –ú–µ–Ω—å—à–µ —ç–ø–æ—Ö –¥–ª—è CV
                    validation_data=(X_val_fold, y_val_fold),
                    verbose=0
                )
                
                # –û—Ü–µ–Ω–∫–∞
                predictions = model.predict(X_val_fold, verbose=0)
                predicted_classes = np.argmax(predictions, axis=1)
                
                accuracy = accuracy_score(y_val_fold, predicted_classes)
                f1 = f1_score(y_val_fold, predicted_classes, average='weighted')
                
                cv_results[model_name].append({
                    'accuracy': accuracy,
                    'f1_score': f1
                })
        
        # –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏–∏
        print("\nüìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏–∏:")
        for model_name, results in cv_results.items():
            mean_acc = np.mean([r['accuracy'] for r in results])
            std_acc = np.std([r['accuracy'] for r in results])
            mean_f1 = np.mean([r['f1_score'] for r in results])
            
            print(f"{model_name}:")
            print(f"  –¢–æ—á–Ω–æ—Å—Ç—å: {mean_acc:.4f} ¬± {std_acc:.4f}")
            print(f"  F1-score: {mean_f1:.4f}")
        
        return cv_results
    
    def validate_model_quality(self, model, X_test, y_test, model_name):
        """–í–∞–ª–∏–¥–∞—Ü–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –º–æ–¥–µ–ª–∏ –ø–æ —Ü–µ–ª–µ–≤—ã–º –ø–æ–∫–∞–∑–∞—Ç–µ–ª—è–º"""
        predictions = model.predict(X_test, verbose=0)
        predicted_classes = np.argmax(predictions, axis=1)
        
        from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score
        
        accuracy = accuracy_score(y_test, predicted_classes)
        f1 = f1_score(y_test, predicted_classes, average='weighted')
        precision = precision_score(y_test, predicted_classes, average='weighted', zero_division=0)
        recall = recall_score(y_test, predicted_classes, average='weighted', zero_division=0)
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è —Ü–µ–ª–µ–≤—ã–º –ø–æ–∫–∞–∑–∞—Ç–µ–ª—è–º
        quality_checks = {
            'accuracy': accuracy >= VALIDATION_CONFIG["target_accuracy"],
            'f1_score': f1 >= VALIDATION_CONFIG["target_f1_score"],
            'precision': precision >= VALIDATION_CONFIG["min_class_precision"]
        }
        
        all_passed = all(quality_checks.values())
        
        print(f"\nüéØ –í–∞–ª–∏–¥–∞—Ü–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –º–æ–¥–µ–ª–∏ {model_name}:")
        print(f"  –¢–æ—á–Ω–æ—Å—Ç—å: {accuracy:.4f} {'‚úÖ' if quality_checks['accuracy'] else '‚ùå'}")
        print(f"  F1-score: {f1:.4f} {'‚úÖ' if quality_checks['f1_score'] else '‚ùå'}")
        print(f"  Precision: {precision:.4f} {'‚úÖ' if quality_checks['precision'] else '‚ùå'}")
        print(f"  –û–±—â–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç: {'‚úÖ –ü–†–û–ô–î–ï–ù–û' if all_passed else '‚ùå –ù–ï –ü–†–û–ô–î–ï–ù–û'}")
        
        return {
            'accuracy': accuracy,
            'f1_score': f1,
            'precision': precision,
            'recall': recall,
            'quality_passed': all_passed
        }
    
    def train_production_model(self, X_train, y_train, X_val, y_val, X_test, y_test):
        """–ü–æ–ª–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –ø—Ä–æ–¥–∞–∫—à–Ω –º–æ–¥–µ–ª–∏"""
        print("üöÄ –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ –ø—Ä–æ–¥–∞–∫—à–Ω –º–æ–¥–µ–ª–∏...")
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞–Ω—Å–∞–º–±–ª—è
        vocab_size = len(self.tokenizer.word_index) + 1
        self.model_ensemble = SentimentModelEnsemble(vocab_size=vocab_size)
        
        # –ê—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
        if MODEL_CONFIG["data_augmentation"]["enabled"]:
            X_train_texts = [self.tokenizer.sequences_to_texts([seq])[0] for seq in X_train]
            X_train_aug, y_train_aug = self.augment_data(X_train_texts, y_train)
            
            # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –æ–±—Ä–∞—Ç–Ω–æ –≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
            X_train_aug_seq = self.tokenizer.texts_to_sequences(X_train_aug)
            from tensorflow.keras.preprocessing.sequence import pad_sequences
            X_train_aug_pad = pad_sequences(
                X_train_aug_seq, 
                maxlen=MODEL_CONFIG["max_sequence_length"]
            )
            
            # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Å –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
            X_train = np.vstack([X_train, X_train_aug_pad])
            y_train = np.hstack([y_train, y_train_aug])
        
        # –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π
        self.model_ensemble.build_all_models()
        
        # –ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        if VALIDATION_CONFIG["cross_validation_folds"] > 0:
            print("üîÑ –í—ã–ø–æ–ª–Ω—è–µ–º –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—é...")
            cv_results = self.cross_validate_models(X_train, y_train)
        
        # –û–±—É—á–µ–Ω–∏–µ –∞–Ω—Å–∞–º–±–ª—è
        print("üéØ –û–±—É—á–µ–Ω–∏–µ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –∞–Ω—Å–∞–º–±–ª—è...")
        self.training_history = self.model_ensemble.train_ensemble(
            X_train, y_train, X_val, y_val
        )
        
        # –û—Ü–µ–Ω–∫–∞ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        print("üìä –§–∏–Ω–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞...")
        test_results = self.model_ensemble.evaluate_ensemble(X_test, y_test)
        
        # –í–∞–ª–∏–¥–∞—Ü–∏—è –∫–∞—á–µ—Å—Ç–≤–∞
        for model_name, model in self.model_ensemble.models.items():
            validation_result = self.validate_model_quality(
                model, X_test, y_test, model_name
            )
            self.validation_results[model_name] = validation_result
        
        # –í–∞–ª–∏–¥–∞—Ü–∏—è –∞–Ω—Å–∞–º–±–ª—è
        if self.model_ensemble.ensemble_model:
            validation_result = self.validate_model_quality(
                self.model_ensemble.ensemble_model, X_test, y_test, "ensemble"
            )
            self.validation_results["ensemble"] = validation_result
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π
        self.model_ensemble.save_all_models()
        
        return self.training_history, test_results, self.validation_results
    
    def generate_training_report(self):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–æ–¥—Ä–æ–±–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞ –æ–± –æ–±—É—á–µ–Ω–∏–∏"""
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        
        report = {
            "training_timestamp": timestamp,
            "model_config": MODEL_CONFIG,
            "training_history": str(self.training_history),
            "validation_results": self.validation_results,
            "best_model": max(
                self.validation_results.items(),
                key=lambda x: x[1]["accuracy"]
            )[0] if self.validation_results else None
        }
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ç—á–µ—Ç–∞
        report_path = MODEL_PATHS["main_model"].parent / f"training_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        import json
        with open(report_path, 'w', encoding='utf-8') as f:
            # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è numpy —Ç–∏–ø–æ–≤ –¥–ª—è JSON —Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏–∏
            def convert_numpy(obj):
                if isinstance(obj, np.integer):
                    return int(obj)
                elif isinstance(obj, np.floating):
                    return float(obj)
                elif isinstance(obj, np.ndarray):
                    return obj.tolist()
                return obj
            
            json.dump(report, f, ensure_ascii=False, indent=2, default=convert_numpy)
        
        print(f"üìÑ –û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {report_path}")
        return report


# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
if __name__ == "__main__":
    # –°–æ–∑–¥–∞–Ω–∏–µ –∏ –æ–±—É—á–µ–Ω–∏–µ –∞–Ω—Å–∞–º–±–ª—è
    trainer = AdvancedSentimentTrainer()
    
    # –ó–¥–µ—Å—å –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –∑–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    # X_train, y_train, X_val, y_val, X_test, y_test = load_data()
    
    # –û–±—É—á–µ–Ω–∏–µ –ø—Ä–æ–¥–∞–∫—à–Ω –º–æ–¥–µ–ª–∏
    # history, results, validation = trainer.train_production_model(
    #     X_train, y_train, X_val, y_val, X_test, y_test
    # )
    
    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞
    # report = trainer.generate_training_report()
    
    print("‚úÖ –£–ª—É—á—à–µ–Ω–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –º–æ–¥–µ–ª–∏ –≥–æ—Ç–æ–≤–∞ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é!")