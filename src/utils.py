"""
–í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è –ø—Ä–æ–µ–∫—Ç–∞
"""
import re
import json
import os
from typing import List, Dict, Any
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from datetime import datetime
import pandas as pd
from pathlib import Path

from config import MODELS_DIR


def clean_text(text: str) -> str:
    """
    –û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –æ—Ç —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤
    
    Args:
        text: –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç
        
    Returns:
        –û—á–∏—â–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
    """
    # –£–¥–∞–ª–µ–Ω–∏–µ HTML —Ç–µ–≥–æ–≤
    text = re.sub(r'<[^>]+>', '', text)
    
    # –£–¥–∞–ª–µ–Ω–∏–µ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –ø—Ä–æ–±–µ–ª–æ–≤
    text = re.sub(r'\s+', ' ', text)
    
    # –£–¥–∞–ª–µ–Ω–∏–µ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤ (—Å–æ—Ö—Ä–∞–Ω—è–µ–º —ç–º–æ–¥–∑–∏)
    text = re.sub(r'[^\w\s\U0001F600-\U0001F64F\U0001F300-\U0001F5FF\U0001F680-\U0001F6FF\U0001F1E0-\U0001F1FF.,!?-]', '', text)
    
    return text.strip()


def extract_emojis(text: str) -> List[str]:
    """
    –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —ç–º–æ–¥–∑–∏ –∏–∑ —Ç–µ–∫—Å—Ç–∞
    
    Args:
        text: –¢–µ–∫—Å—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        
    Returns:
        –°–ø–∏—Å–æ–∫ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —ç–º–æ–¥–∑–∏
    """
    emoji_pattern = re.compile(
        "["
        "\U0001F600-\U0001F64F"  # emoticons
        "\U0001F300-\U0001F5FF"  # symbols & pictographs
        "\U0001F680-\U0001F6FF"  # transport & map symbols
        "\U0001F1E0-\U0001F1FF"  # flags (iOS)
        "\U00002702-\U000027B0"
        "\U000024C2-\U0001F251"
        "]+",
        flags=re.UNICODE
    )
    
    return emoji_pattern.findall(text)


def save_training_report(history: Dict, metrics: Dict, save_path: Path = None):
    """
    –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ç—á–µ—Ç–∞ –æ–± –æ–±—É—á–µ–Ω–∏–∏ –º–æ–¥–µ–ª–∏
    
    Args:
        history: –ò—Å—Ç–æ—Ä–∏—è –æ–±—É—á–µ–Ω–∏—è
        metrics: –ú–µ—Ç—Ä–∏–∫–∏ –º–æ–¥–µ–ª–∏
        save_path: –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
    """
    if save_path is None:
        save_path = MODELS_DIR / f"training_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    
    report = {
        "training_date": datetime.now().isoformat(),
        "final_metrics": metrics,
        "training_history": {
            "loss": history.history.get('loss', []),
            "accuracy": history.history.get('accuracy', []),
            "val_loss": history.history.get('val_loss', []),
            "val_accuracy": history.history.get('val_accuracy', [])
        },
        "epochs_trained": len(history.history.get('loss', [])),
        "best_val_accuracy": max(history.history.get('val_accuracy', [0])),
        "best_val_loss": min(history.history.get('val_loss', [float('inf')]))
    }
    
    with open(save_path, 'w', encoding='utf-8') as f:
        json.dump(report, f, ensure_ascii=False, indent=2)
    
    print(f"–û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {save_path}")


def plot_confusion_matrix(y_true: np.ndarray, y_pred: np.ndarray, 
                         class_names: List[str], save_path: Path = None):
    """
    –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –º–∞—Ç—Ä–∏—Ü—ã –æ—à–∏–±–æ–∫
    
    Args:
        y_true: –ò—Å—Ç–∏–Ω–Ω—ã–µ –º–µ—Ç–∫–∏
        y_pred: –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –º–µ—Ç–∫–∏
        class_names: –ù–∞–∑–≤–∞–Ω–∏—è –∫–ª–∞—Å—Å–æ–≤
        save_path: –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
    """
    from sklearn.metrics import confusion_matrix
    
    cm = confusion_matrix(y_true, y_pred)
    
    plt.figure(figsize=(10, 8))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                xticklabels=class_names, yticklabels=class_names)
    plt.title('–ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫')
    plt.ylabel('–ò—Å—Ç–∏–Ω–Ω—ã–µ –º–µ—Ç–∫–∏')
    plt.xlabel('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –º–µ—Ç–∫–∏')
    
    if save_path:
        plt.savefig(save_path)
    plt.show()


def analyze_dataset_balance(df: pd.DataFrame, label_column: str = 'sentiment'):
    """
    –ê–Ω–∞–ª–∏–∑ –±–∞–ª–∞–Ω—Å–∞ –∫–ª–∞—Å—Å–æ–≤ –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ
    
    Args:
        df: DataFrame —Å –¥–∞–Ω–Ω—ã–º–∏
        label_column: –ù–∞–∑–≤–∞–Ω–∏–µ —Å—Ç–æ–ª–±—Ü–∞ —Å –º–µ—Ç–∫–∞–º–∏
    """
    # –ü–æ–¥—Å—á–µ—Ç –∫–ª–∞—Å—Å–æ–≤
    class_counts = df[label_column].value_counts()
    
    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
    plt.figure(figsize=(10, 6))
    
    # –ì—Ä–∞—Ñ–∏–∫ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
    ax1 = plt.subplot(1, 2, 1)
    class_counts.plot(kind='bar', color=['red', 'gray', 'green'])
    plt.title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤')
    plt.xlabel('–ö–ª–∞—Å—Å')
    plt.ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ')
    plt.xticks(rotation=45)
    
    # –ö—Ä—É–≥–æ–≤–∞—è –¥–∏–∞–≥—Ä–∞–º–º–∞
    ax2 = plt.subplot(1, 2, 2)
    class_counts.plot(kind='pie', autopct='%1.1f%%', 
                     colors=['red', 'gray', 'green'])
    plt.title('–ü—Ä–æ—Ü–µ–Ω—Ç–Ω–æ–µ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ')
    plt.ylabel('')
    
    plt.tight_layout()
    plt.show()
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    print("–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫–ª–∞—Å—Å–∞–º:")
    print(f"–í—Å–µ–≥–æ –ø—Ä–∏–º–µ—Ä–æ–≤: {len(df)}")
    print("\n–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ:")
    for class_name, count in class_counts.items():
        percentage = (count / len(df)) * 100
        print(f"  {class_name}: {count} ({percentage:.1f}%)")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –±–∞–ª–∞–Ω—Å–∞
    max_count = class_counts.max()
    min_count = class_counts.min()
    imbalance_ratio = max_count / min_count
    
    print(f"\n–°–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ –¥–∏—Å–±–∞–ª–∞–Ω—Å–∞: {imbalance_ratio:.2f}")
    if imbalance_ratio > 2:
        print("‚ö†Ô∏è  –î–∞—Ç–∞—Å–µ—Ç –Ω–µ—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω! –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞.")


def export_model_to_onnx(model, save_path: Path = None):
    """
    –≠–∫—Å–ø–æ—Ä—Ç –º–æ–¥–µ–ª–∏ –≤ —Ñ–æ—Ä–º–∞—Ç ONNX –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ –¥—Ä—É–≥–∏—Ö –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è—Ö
    
    Args:
        model: –ú–æ–¥–µ–ª—å Keras
        save_path: –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
    """
    try:
        import tf2onnx
        
        if save_path is None:
            save_path = MODELS_DIR / "sentiment_model.onnx"
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è
        onnx_model, _ = tf2onnx.convert.from_keras(model)
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
        with open(save_path, "wb") as f:
            f.write(onnx_model.SerializeToString())
        
        print(f"–ú–æ–¥–µ–ª—å —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–∞ –≤ ONNX: {save_path}")
        
    except ImportError:
        print("–î–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞ –≤ ONNX —É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ tf2onnx: pip install tf2onnx")


def create_sample_dataset(num_samples: int = 1000) -> pd.DataFrame:
    """
    –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–∏–º–µ—Ä–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
    
    Args:
        num_samples: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–º–µ—Ä–æ–≤
        
    Returns:
        DataFrame —Å –ø—Ä–∏–º–µ—Ä–∞–º–∏
    """
    positive_phrases = [
        "–û—Ç–ª–∏—á–Ω—ã–π", "–ü—Ä–µ–∫—Ä–∞—Å–Ω—ã–π", "–ó–∞–º–µ—á–∞—Ç–µ–ª—å–Ω—ã–π", "–°—É–ø–µ—Ä", "–í–µ–ª–∏–∫–æ–ª–µ–ø–Ω–æ",
        "–†–∞–¥", "–°—á–∞—Å—Ç–ª–∏–≤", "–î–æ–≤–æ–ª–µ–Ω", "–í–æ—Å—Ö–∏—Ç–∏—Ç–µ–ª—å–Ω–æ", "–ö—Ä—É—Ç–æ"
    ]
    
    negative_phrases = [
        "–£–∂–∞—Å–Ω—ã–π", "–ü–ª–æ—Ö–æ–π", "–û—Ç–≤—Ä–∞—Ç–∏—Ç–µ–ª—å–Ω—ã–π", "–†–∞–∑–æ—á–∞—Ä–æ–≤–∞–Ω", "–ì—Ä—É—Å—Ç–Ω–æ",
        "–ü–µ—á–∞–ª—å–Ω–æ", "–ó–ª–æ–π", "–†–∞–∑–¥—Ä–∞–∂–µ–Ω", "–ù–µ–¥–æ–≤–æ–ª–µ–Ω", "–ö–æ—à–º–∞—Ä"
    ]
    
    neutral_phrases = [
        "–ù–æ—Ä–º–∞–ª—å–Ω–æ", "–û–±—ã—á–Ω–æ", "–°—Ä–µ–¥–Ω–µ", "–¢–∞–∫ —Å–µ–±–µ", "–ü–æ–π–¥–µ—Ç",
        "–ù–∏—á–µ–≥–æ –æ—Å–æ–±–µ–Ω–Ω–æ–≥–æ", "–ö–∞–∫ –≤—Å–µ–≥–¥–∞", "–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ", "–¢–∏–ø–∏—á–Ω–æ", "–û–±—ã–¥–µ–Ω–Ω–æ"
    ]
    
    data = []
    
    for _ in range(num_samples // 3):
        # –ü–æ–∑–∏—Ç–∏–≤–Ω—ã–µ
        text = f"{np.random.choice(positive_phrases)} –¥–µ–Ω—å! –í—Å–µ {np.random.choice(positive_phrases).lower()}!"
        data.append({"text": text, "sentiment": "positive"})
        
        # –ù–µ–≥–∞—Ç–∏–≤–Ω—ã–µ
        text = f"{np.random.choice(negative_phrases)} –æ–ø—ã—Ç. –û—á–µ–Ω—å {np.random.choice(negative_phrases).lower()}."
        data.append({"text": text, "sentiment": "negative"})
        
        # –ù–µ–π—Ç—Ä–∞–ª—å–Ω—ã–µ
        text = f"{np.random.choice(neutral_phrases)}. –í—Å–µ {np.random.choice(neutral_phrases).lower()}."
        data.append({"text": text, "sentiment": "neutral"})
    
    return pd.DataFrame(data)


def calculate_model_size(model_path: Path) -> Dict[str, Any]:
    """
    –†–∞—Å—á–µ—Ç —Ä–∞–∑–º–µ—Ä–∞ –º–æ–¥–µ–ª–∏ –∏ –µ—ë —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫
    
    Args:
        model_path: –ü—É—Ç—å –∫ –º–æ–¥–µ–ª–∏
        
    Returns:
        –°–ª–æ–≤–∞—Ä—å —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –º–æ–¥–µ–ª–∏
    """
    from tensorflow import keras
    
    model = keras.models.load_model(model_path)
    
    # –†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞
    file_size_mb = os.path.getsize(model_path) / (1024 * 1024)
    
    # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
    total_params = model.count_params()
    trainable_params = sum([np.prod(var.shape) for var in model.trainable_variables])
    non_trainable_params = total_params - trainable_params
    
    # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Å–ª–æ—è—Ö
    layer_info = []
    for layer in model.layers:
        layer_info.append({
            "name": layer.name,
            "type": layer.__class__.__name__,
            "params": layer.count_params(),
            "output_shape": str(layer.output_shape)
        })
    
    return {
        "file_size_mb": round(file_size_mb, 2),
        "total_params": total_params,
        "trainable_params": trainable_params,
        "non_trainable_params": non_trainable_params,
        "num_layers": len(model.layers),
        "layer_info": layer_info
    }


def benchmark_prediction_speed(model, tokenizer, test_texts: List[str], num_runs: int = 100):
    """
    –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–∫–æ—Ä–æ—Å—Ç–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
    
    Args:
        model: –ú–æ–¥–µ–ª—å –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
        tokenizer: –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
        test_texts: –¢–µ–∫—Å—Ç—ã –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
        num_runs: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø—É—Å–∫–æ–≤
        
    Returns:
        –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –≤—Ä–µ–º–µ–Ω–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
    """
    import time
    from tensorflow.keras.preprocessing.sequence import pad_sequences
    
    times = []
    
    for _ in range(num_runs):
        start_time = time.time()
        
        # –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞
        sequences = tokenizer.texts_to_sequences(test_texts)
        X = pad_sequences(sequences, maxlen=128)
        
        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
        predictions = model.predict(X, verbose=0)
        
        end_time = time.time()
        times.append(end_time - start_time)
    
    times = np.array(times)
    
    return {
        "mean_time_ms": np.mean(times) * 1000,
        "std_time_ms": np.std(times) * 1000,
        "min_time_ms": np.min(times) * 1000,
        "max_time_ms": np.max(times) * 1000,
        "texts_per_second": len(test_texts) / np.mean(times)
    }


# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
if __name__ == "__main__":
    # –¢–µ—Å—Ç —Ñ—É–Ω–∫—Ü–∏–π
    test_text = "–ü—Ä–∏–≤–µ—Ç! üòä –°–µ–≥–æ–¥–Ω—è –æ—Ç–ª–∏—á–Ω—ã–π –¥–µ–Ω—å!"
    print(f"–ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç: {test_text}")
    print(f"–û—á–∏—â–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç: {clean_text(test_text)}")
    print(f"–ù–∞–π–¥–µ–Ω–Ω—ã–µ —ç–º–æ–¥–∑–∏: {extract_emojis(test_text)}")
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–∏–º–µ—Ä–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞
    sample_df = create_sample_dataset(100)
    print(f"\n–°–æ–∑–¥–∞–Ω –ø—Ä–∏–º–µ—Ä–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç —Å {len(sample_df)} –ø—Ä–∏–º–µ—Ä–∞–º–∏")
    
    # –ê–Ω–∞–ª–∏–∑ –±–∞–ª–∞–Ω—Å–∞
    analyze_dataset_balance(sample_df)